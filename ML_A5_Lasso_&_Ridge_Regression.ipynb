{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07eb33c"
      },
      "source": [
        "<div>\n",
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=220 height=220 align=left class=\"saturate\">\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center> \n",
        "<!-- <font color=0F5298 size=7> -->\n",
        "<font color=0F5298 size=6>\n",
        "    Introduction to Machine Learning <br> <br>\n",
        "<!-- <font color=2565AE size=5> -->\n",
        "<font size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023 <br> <br>\n",
        "<font color=606060 size=5>\n",
        "    Homework 5: Practical - Lasso & Ridge Regression <br> <br>\n",
        "<font color=686880 size=4>\n",
        "    TAs: Alireza Dehghanpour - Arman Malekzadeh - Ali Salesi\n",
        "    \n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a79508"
      },
      "source": [
        "### Full Name : Parsa Sharifi\n",
        "### Student Number : 99101762\n",
        "### Colab Link: https://colab.research.google.com/drive/1gdnwt_LrBDumZh8khU3jBkeALwUWf1Gn?usp=sharing\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries. The datasets are made available to public for the purpose of health data analysis. The dataset related to life expectancy, health factors for 193 countries has been collected from the same WHO data repository website and its corresponding economic data was collected from United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative.\n",
        "\n",
        "In this assignment you have to predict **life expectancy**."
      ],
      "metadata": {
        "id": "5sy39_vXU_PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:27.082403Z",
          "iopub.execute_input": "2023-04-30T08:33:27.082868Z",
          "iopub.status.idle": "2023-04-30T08:33:27.089987Z",
          "shell.execute_reply.started": "2023-04-30T08:33:27.082829Z",
          "shell.execute_reply": "2023-04-30T08:33:27.088672Z"
        },
        "trusted": true,
        "id": "bBUrdXrVU_PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration (20 points)"
      ],
      "metadata": {
        "id": "5sEpKmvvU_PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/dataset.csv \"https://www.dropbox.com/s/2kz21qjt40pjy43/train.csv?dl=1\""
      ],
      "metadata": {
        "id": "GJCBJo5XjWUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f28ce4e-dbc5-49ed-9919-ebc4f71d9986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-11 17:56:37--  https://www.dropbox.com/s/2kz21qjt40pjy43/train.csv?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/2kz21qjt40pjy43/train.csv [following]\n",
            "--2023-05-11 17:56:37--  https://www.dropbox.com/s/dl/2kz21qjt40pjy43/train.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdf80119c90be53ff2d2996c494.dl.dropboxusercontent.com/cd/0/get/B73SrE7RWO6VABchGrnMsoLXDlLgus1xjFvlxF37BmQToqkEQba5HQKNeln7L2vvlZpXyLWm8_Oz8P1aBWElgrPuI3mnJ61-f4d1AVPNy0ZP06aZzOA-wJCTriilmHQjzZ-ES9fs5VsKftraCFfhZKu12F9GcwyZFahNfSsFSfT8EgLOD5KPY8y0M-XQXWeUX_s/file?dl=1# [following]\n",
            "--2023-05-11 17:56:37--  https://ucdf80119c90be53ff2d2996c494.dl.dropboxusercontent.com/cd/0/get/B73SrE7RWO6VABchGrnMsoLXDlLgus1xjFvlxF37BmQToqkEQba5HQKNeln7L2vvlZpXyLWm8_Oz8P1aBWElgrPuI3mnJ61-f4d1AVPNy0ZP06aZzOA-wJCTriilmHQjzZ-ES9fs5VsKftraCFfhZKu12F9GcwyZFahNfSsFSfT8EgLOD5KPY8y0M-XQXWeUX_s/file?dl=1\n",
            "Resolving ucdf80119c90be53ff2d2996c494.dl.dropboxusercontent.com (ucdf80119c90be53ff2d2996c494.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to ucdf80119c90be53ff2d2996c494.dl.dropboxusercontent.com (ucdf80119c90be53ff2d2996c494.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308420 (301K) [application/binary]\n",
            "Saving to: ‘/content/dataset.csv’\n",
            "\n",
            "/content/dataset.cs 100%[===================>] 301.19K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-05-11 17:56:38 (8.88 MB/s) - ‘/content/dataset.csv’ saved [308420/308420]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset as a dataframe"
      ],
      "metadata": {
        "id": "XU9i17XgU_PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/dataset.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:30.197254Z",
          "iopub.execute_input": "2023-04-30T08:33:30.197642Z",
          "iopub.status.idle": "2023-04-30T08:33:30.238114Z",
          "shell.execute_reply.started": "2023-04-30T08:33:30.197608Z",
          "shell.execute_reply": "2023-04-30T08:33:30.236875Z"
        },
        "trusted": true,
        "id": "GARdmLGkU_PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot \"year\" against \"average life expectancy\""
      ],
      "metadata": {
        "id": "HBsBXJ5dU_PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(df[\"Year\"], df[\"Life expectancy \"])\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Life expectancy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:33.477341Z",
          "iopub.execute_input": "2023-04-30T08:33:33.478151Z",
          "iopub.status.idle": "2023-04-30T08:33:33.711698Z",
          "shell.execute_reply.started": "2023-04-30T08:33:33.478109Z",
          "shell.execute_reply": "2023-04-30T08:33:33.710510Z"
        },
        "trusted": true,
        "id": "-DfKHpd8U_PR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "bc3a63d0-2e85-45c6-fc53-235a5bd1e327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby(\"Year\")[\"Life expectancy \"].mean()\n",
        "grouped.plot(kind=\"line\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "sZkh28y7zccP",
        "outputId": "3332d69d-07d4-4bb6-cd48-85900e21c6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAG2CAYAAADfpuRqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKmElEQVR4nO3dd3hUVf4G8Hcmk5n0SirpQEhC6E1AigiEsiqK6CooFlAhSlNUdH/rukWKuGIBKYtZXZoiRVRC771DQkkhPSSB1EmdJDPn90fISKSlzMzNJO/neeZ5zJ07d75nE2bePeeec2RCCAEiIiIiE5FLXQARERG1LgwfREREZFIMH0RERGRSDB9ERERkUgwfREREZFIMH0RERGRSDB9ERERkUgwfREREZFIMH0RERGRSDB9ERERkUg0KHwEBAZDJZHc8IiMjkZ+fj7feegsdO3aEtbU1/Pz8MH36dBQVFRmrdiIiIjJDioacfOrUKWi1Wv3PsbGxGD58OMaPH4/r16/j+vXrWLRoEcLCwpCamoo33ngD169fx08//WTwwomIiMg8yZqysdzMmTPx66+/IiEhATKZ7I7nN2zYgIkTJ6K0tBQKRYNyDhEREbVQjU4ElZWVWL16NWbPnn3X4AEARUVFcHBwuG/w0Gg00Gg0+p91Oh3y8/Ph6up6z+sSERFR8yKEQHFxMby9vSGXP+CuDtFIP/zwg7CwsBCZmZl3ff7mzZvCz89PfPDBB/e9zkcffSQA8MEHH3zwwQcfLeCRnp7+wAzR6GGXiIgIKJVK/PLLL3c8p1arMXz4cLi4uGDr1q2wtLS853X+2PNRVFQEPz8/pKenw8HBoTGlERERkYmp1Wr4+vqisLAQjo6O9z23UcMuqamp2L17NzZt2nTHc8XFxRg5ciTs7e2xefPm+wYPAFCpVFCpVHccd3BwYPggIiIyM/W5ZaJR63xERUXB3d0dY8aMqXNcrVZjxIgRUCqV2Lp1K6ysrBpzeSIiImrBGtzzodPpEBUVhUmTJtW5kbQ2eJSVlWH16tVQq9VQq9UAADc3N1hYWBiuaiIiIjJbDQ4fu3fvRlpaGl555ZU6x8+ePYsTJ04AANq3b1/nueTkZAQEBDS+SiIiImoxmrTOhzGo1Wo4Ojrqp+kSERFR89eQ72/u7UJEREQmxfBBREREJsXwQURERCbF8EFEREQmxfBBREREJsXwQURERCbF8EFEREQmxfBBREREJsXwQURE1IrklmhwMaNQ0hoatastERERmZcSTTX+cygJKw8mwdVOhd2zB0OpkKYPguGDiIioBaus1mHdyTR8uScBeaWVAIB2Npa4WaJBWydrSWpi+CAiImqBdDqBXy5ex2c745GWXwYACHC1wZyIEIzu7AmZTCZZbQwfRERELcyhhJuYH30Vl66rAQBt7FSYOawDnu3tC0sL6W/3ZPggIiJqIS5mFGLB9qs4kpgHALBTKfDG4CC88nAgbJTN5yu/+VRCREREjZKcW4pFO+Pw28UsAIDSQo4X+vkj8pH2cLFVSlzdnRg+iIiIzNSN4gp8uScB60+mo1onIJMBT3Zvi1nDguHrYiN1effE8EFERGRmiiuqsOJgEv5zKBnlVVoAwNAQd8yJ6IhQLweJq3swhg8iIiIzoanWYs3xNHy9LxH5t6bNdvN1wvujQvBQkKvE1dUfwwcREVEzp9MJ/HwhE5/tjEdGQTkAIMjNFu9GhCCik4ek02Ybg+GDiIiomRJCYH/8TSyIvoqr2cUAAA8HFWYNC8bTPX2gaAbTZhuD4YOIiKgZOpdWgPnRV3EiOR8AYG+lwLQh7fFS/wBYKy0krq5pGD6IiIiakWs3S7BoRxyiY7MBAEqFHC/3D8DUIe3gZNP8ps02BsMHERFRM5CjrsDi3Qn48XQ6tDoBuQwY18MHs4YHw1uiPViMheGDiIhIQkXlVVh+4Bq+PZKMiiodAGBYqAfeHdkRwR72EldnHAwfREREEqio0uJ/x1KxZH8iCsuqAAC9/J3x/qgQ9Apwkbg642L4ICIiMrEcdQWeWX4MqXk1u812cLfDeyND8Giou9lNm20Mhg8iIiITqtLq8Obas0jNK4OngxXeHhGMp3r4wELe8kNHLYYPIiIiE/p0RxxOpRTAXqXAutceQmAbW6lLMjnzXJ2EiIjIDG2PzcaKg0kAgE/Hd2mVwQNg+CAiIjKJlNxSzNlwAQAw+eFAjAz3krgi6TB8EBERGVlFlRZT15xFsaYavfyd8d6oEKlLkhTDBxERkZF99PMlXMlSw9VWia+f7wFLM92TxVBad+uJiIiM7MfT6fjhdDrkMuDL57rD09FK6pIkx/BBRERkJJevq/F/W2IBALOHB2NA+zYSV9Q8MHwQEREZgbqiCtPWnIGmWodHOrph2pD2UpfUbDB8EBERGZgQAu9uuIiUvDK0dbLG5892g7wVLSL2IAwfREREBrbqcDK2X8qGpYUMSyf0gJONUuqSmhWGDyIiIgM6lZKPedFXAQB//VMYuvo6SVtQM8TwQUREZCA3izWIXHMWWp3AE928MfEhf6lLapYYPoiIiAxAqxOYsf4cbhRr0N7dDp882blV7FDbGAwfREREBvD5rngcvZYHG6UFlk3sAVsV9269F4YPIiKiJtp7NQdf70sEAMwf1wXt3e0lrqh5Y/ggIiJqgoyCMsz6oWbDuBf7+ePxrt4SV9T8MXwQERE1kqZai2lrzqKovApdfZ3w4ZhQqUsyCwwfREREjfTPX6/gYkYRnGwsseT57lApLKQuySwwfBARETXCz+cz8b/jqQCAz5/tBh9nG4krMh8MH0RERA2UkFOM9zfGAADeGtoej3R0l7gi88LwQURE1AClmmq8sfoMyqu0GNDeFTOHBUtdktlh+CAiIqonIQTe3xSDazdL4eGgwhd/7g4LbhjXYAwfRERE9fS/46n45cJ1KOQyLHm+B9rYqaQuySwxfBAREdXD+fRC/OPXywCA90eFoFeAi8QVmS+GDyIiogcoKK1E5JqzqNIKjAr3xKsPB0pdkllj+CAiIroPnU5g5g/nkVlYjsA2tlj4dBduGNdEDB9ERET3sWRfIg7E34RKIcfSCT1gb2UpdUlmj+GDiIjoHg4n5OLfu+MBAP8cG45QLweJK2oZGD6IiIjuIquoHNPXn4MQwJ97+2J8L1+pS2oxGD6IiIj+oEqrw5trzyG/tBJhXg742+OdpC6pRWH4ICIi+oP50VdxJrUA9lYKfDOxB6wsuWGcITF8EBER3SY6JgurDicDAD4b3xX+rrYSV9TyNCh8BAQEQCaT3fGIjIwEAKxYsQJDhgyBg4MDZDIZCgsLjVEzERGRUSTdLMGcny4CAF4fFIQRnTwlrqhlalD4OHXqFLKysvSPXbt2AQDGjx8PACgrK8PIkSPxwQcfGL5SIiIiIyqv1GLamrMo0VSjT4AL5kR0lLqkFkvRkJPd3Nzq/Dx//ny0a9cOgwcPBgDMnDkTALB//36DFEdERGQKQgj8ZUssrmYXo42dCl8/3x0KC96ZYCwNCh+3q6ysxOrVqzF79uwmrfSm0Wig0Wj0P6vV6kZfi4iIqDF+OJWOjWczIJcBXz7XDe4OVlKX1KI1OtZt2bIFhYWFeOmll5pUwLx58+Do6Kh/+PpyHjUREZlObGYR/rr1EgDg7REd0b9dG4kravkaHT5WrVqFUaNGwdvbu0kFzJ07F0VFRfpHenp6k65HRERUX9dulmDamrOorNbh0RB3TB3cTuqSWoVGDbukpqZi9+7d2LRpU5MLUKlUUKlUTb4OERFRfel0AlFHU7Bw+1VoqnXwcbbGZ890hVzODeNMoVHhIyoqCu7u7hgzZoyh6yEiIjKq1LxSzNlwESdT8gEAAzu0wYJxXeBko5S4stajweFDp9MhKioKkyZNgkJR9+XZ2dnIzs5GYmIiACAmJgb29vbw8/ODi4uLYSomIiJqBCEEVp9Iw7xtV1BWqYWN0gIfjA7FhL5+TZo4QQ3X4PCxe/dupKWl4ZVXXrnjuWXLluHjjz/W/zxo0CAANT0lTb0xlYiIqLEyC8vx3k8XcTgxFwDQN9AFnz7dFX6uNhJX1jrJhBBC6iJup1ar4ejoiKKiIjg4cOtiIiJqPCEENpzOwD9+vYxiTTWsLOV4NyIEL/UP4P0dBtaQ7+9Gr/NBRETUnOWoKzB3Uwz2Xr0BAOju54TPxndFkJudxJURwwcREbUoQgj8fP46Ptp6CUXlVVBayDF7RDCmDAyCBXs7mgWGDyIiajFySzT4cHMMdlzKAQB0buuIz57pimAPe4kro9sxfBARUYsQHZOFD7fEIr+0Egq5DNMf7YCpQ9rBknu0NDsMH0REZNYKSivx0dZL2HrhOgAgxNMei8Z3RXhbR4kro3th+CAiIrO150oO3t8Ug5vFGljIZZg6uB2mP9oBSgV7O5ozhg8iIjI76ooq/P2Xy/jpTAYAoL27HT4b3xVdfZ2kLYzqheGDiIjMysH4m3hv40VkFVVAJgOmDAzC7OHBsLK0kLo0qieGDyIiMgslmmp8su0K1p5IAwD4u9pg0fiu6B3A7TvMDcMHERE1e8eu5WHOTxeQUVAOAJjUzx/vjQqBjZJfY+aIvzUiImq2yiu1WLjjKqKOpAAA2jpZ49PxXdC/XRtpC6MmYfggIqJm6UxqAd7ZcAHJuaUAgOf6+OLDMWGwU/Gry9zxN0hERM1KRZUWn++Ox8qDSdAJwNPBCvPHdcaQju5Sl0YGwvBBRETNRkxGEWb/eB4JN0oAAE/1aIuPHusER2tLiSsjQ2L4ICIiyel0Al/sScDX+xKh1Qm0sVNh3lOdMTzMQ+rSyAgYPoiISHLrTqXhiz0JAIA/dfHC358Ih4utUuKqyFgYPoiISFLFFVX49854AMCciI6IfKS9xBWRsXHxeyIiktTS/deQV1qJoDa2eG1QkNTlkAkwfBARkWQyCsqw6nAyAOCD0aGwtODXUmvA3zIREUlm4fY4VFbr0C/IFY+Gcipta8HwQUREkjiXVoCtF65DJgM+HBMKmUwmdUlkIgwfRERkckII/PO3KwCAcT18EN7WUeKKyJQYPoiIyOS2xWTjTGoBrC0t8M6IjlKXQybG8EFERCalqdZi/vaaXo/XBwfB09FK4orI1Bg+iIjIpL47moL0/HJ4OKg4tbaVYvggIiKTyS+txFd7EwEA74zoCBsl17psjRg+iIjIZL7YHY/iimp08nbAuB4+UpdDEmH4ICIik0i8UYLVJ9IA1Eytlcs5tba1YvggIiKTmLftCrQ6gWGhHujfro3U5ZCEGD6IiMjojiTmYs/VG1DIZZg7OkTqckhiDB9ERGRUWt3vC4pNfMgf7dzsJK6IpMbwQURERrXxTAauZKnhYKXAjEc7SF0ONQMMH0REZDSlmmos2hkHAHhraAc42yolroiaA4YPIiIymuUHk3CjWAM/Fxu82N9f6nKomWD4ICIio8gqKseKg9cAAHNHhUClsJC4ImouGD6IiMgoFu2IR0WVDr0DnDEy3FPqcqgZYfggIiKDi80swsazGQCAD8eEQSbjgmL0O4YPIiIyKCEE/vnbZQDAE9280c3XSdqCqNlh+CAiIoPadTkHx5PyoVLI8e5ILihGd2L4ICIig6ms1mFe9FUAwOSBgWjrZC1xRdQcMXwQEZHBrDmRiuTcUrSxU2LqkPZSl0PNFMMHEREZRGFZJRbvTgAAzB7eEXYqhcQVUXPF8EFERAbx1d5EFJVXoaOHPZ7p5SN1OdSMMXwQEVGTpeSW4vtjKQCAD8aEQmHBrxe6N/51EBFRk82PvooqrcDgYDcMDnaTuhxq5hg+iIioSU4k5WH7pWzIZcCHY0KlLofMAMMHERE1mk4n8K9tVwAAf+7jh2APe4krInPA8EFERI3284VMXMwogp1KgVnDgqUuh8wEwwcRETVKeaUWC7fHAQCmDmkHN3uVxBWRuWD4ICKiRll1OAlZRRVo62SNVx8OlLocMiMMH0RE1GA3iiuwdP81AMC7IzvCytJC4orInDB8EBFRg32+Kx5llVp09XXC4129pS6HzAzDBxERNciVLDV+OJUOAPi/MaGQyWQSV0TmhuGDiIjqTQiBT7ZdgU4AYzp7oVeAi9QlkRli+CAionrbH38ThxJyobSQ472RIVKXQ2aK4YOIiOqlWqvDv36rWVDspQEB8HO1kbgiMlcMH0REVC/rTqUj8UYJnG0sEflIe6nLITPG8EFERA+krqjC4l3xAICZw4LhaG0pcUVkzhg+iIjogZbuu4a80koEudni+b5+UpdDZq5B4SMgIAAymeyOR2RkJACgoqICkZGRcHV1hZ2dHcaNG4ecnByjFE5ERKaRnl+Gbw8nAwA+GBUKSwv+/1Zqmgb9BZ06dQpZWVn6x65duwAA48ePBwDMmjULv/zyCzZs2IADBw7g+vXreOqppwxfNRERmczCHXGo1OrQv50rHg11l7ocagEUDTnZzc2tzs/z589Hu3btMHjwYBQVFWHVqlVYu3Ythg4dCgCIiopCaGgojh8/joceeshwVRMRkUmcSS3ALxeuQyYDPuSCYmQgje47q6ysxOrVq/HKK69AJpPhzJkzqKqqwrBhw/TnhISEwM/PD8eOHTNIsUREZDpCCPzzt8sAgKd7+KCTt6PEFVFL0aCej9tt2bIFhYWFeOmllwAA2dnZUCqVcHJyqnOeh4cHsrOz73kdjUYDjUaj/1mtVje2JCIiMqDfYrJwLq0Q1pYWeCeio9TlUAvS6J6PVatWYdSoUfD2btqGQvPmzYOjo6P+4evr26TrERFR01VUaTE/+ioA4PXBQfBwsJK4ImpJGhU+UlNTsXv3bkyePFl/zNPTE5WVlSgsLKxzbk5ODjw9Pe95rblz56KoqEj/SE9Pb0xJRERkQP89moKMgnJ4OKjw2qAgqcuhFqZR4SMqKgru7u4YM2aM/ljPnj1haWmJPXv26I/FxcUhLS0N/fr1u+e1VCoVHBwc6jyIiEg6eSUaLNmbCACYExECG2WjR+iJ7qrBf1E6nQ5RUVGYNGkSFIrfX+7o6IhXX30Vs2fPhouLCxwcHPDWW2+hX79+nOlCRGRGFu9OQLGmGp28HfBU97ZSl0MtUIPDx+7du5GWloZXXnnljuc+//xzyOVyjBs3DhqNBhEREVi6dKlBCiUiIuMqrqjC+pPpWHsyDUDN1Fq5nFNryfBkQgghdRG3U6vVcHR0RFFREYdgiIhMILuoAlFHk7H2RBqKK6oBAGM6e2HJhB4SV0bmpCHf3xzIIyJqpeKyi7HiYBK2XshElbbm/4e2c7PFlIFBeKqHj8TVUUvG8EFE1IoIIXAsKQ8rDiZhf9xN/fE+AS54bVAQhoa4c6iFjI7hg4ioFajW6rAtNhsrDyYhJrMIACCXASPDPTFlYBC6+zlLXCG1JgwfREQtWKmmGj+eTseqw8nIKCgHAFhZyjG+py8mDwyEv6utxBVSa8TwQUTUAt0orsB3R1Ow+ngaisqrAAAutkpM6heAF/r5w8VWKXGF1JoxfBARtSCJN0qw8mASNp/LRKVWBwAIcLXB5IFBeLqnD6wsLSSukIjhg4jI7AkhcCqlACsOXsPuKzf0x3v4OeG1Qe0wPMwDFryJlJoRhg8iIjOl1QnsuJSNFQeTcD69EAAgkwHDQz3w2qAg9ApwkbZAontg+CAiMjPllVr8dCYd/zmcjNS8MgCAUiHHuB4+mDwwEO3c7CSukOj+GD6IiMxEXokG3x1Lxf+OpaCgrOYmUicbS7z4kD9e7B+ANnYqiSskqh+GDyKiZi45txT/OZSEn85kQFNdcxOpr4s1Jj8chPG9fLjrLJkd/sUSEd1HfE7NEuRlldVQyOVQyGWwkMugsPj9vy0tZLC49ZzCQnbruPzWcdmt43L9f9e8Rl7nubtdq6i8Ct8fS8HOyzmo3YWrq48jXhvUDiPDPXkTKZkthg8ionvYHpuN2T+eR1mlVupS8GiIO14bFIQ+gS6QyRg6yLwxfBAR/YFOJ7B4TwK+3JMAAOjfzhUjwz1RrRWo1ulQrRPQagWqdALaWz9XawW0ulvPa0XNOTqBKq3u1vE7f66u8981r605p+ZcABgU3AZTBgahg4e9lP+TEBkUwwcR0W2KK6ow64cL2H0lBwDw6sOBmDsqBAoLucSVEbUcDB9ERLck3SzBlO9P49rNUigVcsx7sjPG9eTW8kSGxvBBRARg39UbmL7+HIorquHlaIXlL/REFx8nqcsiapEYPoioVRNC4JsD1/DpjjgIAfTyd8Y3E3vCzZ5rZhAZC8MHEbVaZZXVmPPTRfx2MQsAMKGvHz56rBOUCt7fQWRMDB9E1Cql55dhyvencTW7GJYWMnz8eDie7+sndVlErQLDBxG1OkcTcxG59iwKyqrQxk6FZRN7cBM2IhNi+CCiVkMIgW+PpOCTbVeg1Ql09XHEshd6wsvRWurSiFoVhg8iahUqqrT4YHMMNp3NBACM6+GDfz0ZDitLC4krI2p9GD6IqMXLKirH6/87g4sZRbCQy/Dh6FC8PCCAy5QTSYThg4hatNMp+Xhj9VnklmjgbGOJJc/3QP/2baQui6hVY/ggohZrzYlU/G3rJVRpBUI87bHyxV7wdbGRuiyiVo/hg4hanMpqHf72yyWsPZEGABjTxQufPt0FNkp+5BE1B/yXSEQtyo3iCkxbfRanUwsgkwHvRoTgjcFBvL+DqBlh+CCiFuNCeiFe/98ZZKsrYG+lwJfPdccjHd2lLouI/oDhg4hahI1nMjB3cwwqq3Vo726HFS/0RJCbndRlEdFdMHwQkVmr1urwybar+PZIMgBgWKgHPn+2K+ytLCWujIjuheGDiMxWfmkl3lx7Fkev5QEAZjzaATMe7QC5nPd3EDVn3LqRyEz9eCodAxfuxbm0AqlLkcTl62o8/vVhHL2WB1ulBZZN7IlZw4MZPIjMAMMHkRkSQmDJ/kSk55fjo62XIISQuiST+vXidYz75igyCsrh72qDzZEDMDLcU+qyiKieGD6IzNDlLDVS88oAABczirAtJlviikxDqxNYuP0q3lx7DuVVWgzs0AZbIx9GsIe91KURUQMwfBCZoehbYUOlqPknvGhnHKq0OilLMrriiiq8+t0pLN1/DQDw+qAg/PflPnC04Y2lROaG4YPIzAghsC0mCwDw18fC4GqrRHJuKX48nS5xZcb10dZL2B93E1aWcnzx526YOzoUFry/g8gsMXwQmZn4nBIk5ZZCaSHH41298ebQ9gCAL3YnoLxSK3F1xnE+vRCbzmYCAL5/pS+e6NZW4oqIqCkYPojMTG2vx6DgNrC3ssTzff3g42yNG8Ua/VoXLYkQAn//5RIAYFwPH/QJdJG4IiJqKoYPIjMTHVsTPkaFewEAVAoLvDOiIwBg2f5rKCitlKw2Y9h64TrOphXCRmmBd0d2lLocIjIAhg8iM5J4oxjxOSWwtJBhWKiH/vjjXb0R6uWAYk01lu5PlLBCwyqv1GJB9FUAwLQh7eDhYCVxRURkCAwfRGakdpbLgPZt6szykMtl+l6B746lIrOwXJL6DG3FwSRcL6pAWydrTB4YJHU5RGQgDB9EZmRbbE34GH1ryOV2Q4Ld0DfQBZXVOizeFW/q0gwuq6gcyw7UTKudOzoEVpYWEldERIbC8EFkJpJzS3ElSw0LuQzDwzzueF4mk+G9USEAgI1nMxCfU2zqEg1q4fY4lFdp0TvAGWM63xm2iMh8MXwQmYnaG037t3OFs63yruf08HPGyE6e0ImaL29zdS6tAJvPZUImA/76p06QybieB1FLwvBBZCZq7/cYdZchl9u9E9ERchmw+0oOTqfkm6I0gxJC4O+/XgZQM7W2s4+jxBURkaExfBCZgfT8MsRkFkEuA0Z0unPI5Xbt3e3wTC9fAMCC7VfNbtO5rReu41zt1NoITq0laokYPojMwPZbN5r2DXRFGzvVA8+fOSwYKoUcp1IKsPfqDWOXZzBlldWYf2tqbeQj7eHOqbVELRLDB5EZ2Hbrfo/Rneu3bbynoxVeGhAAoKb3Q6szj96PFQeTkHVrau2rDwdKXQ4RGQnDB1Ezd72wHOfSCiGTARGd6hc+AGDa4PZwsFIgPqcEm89lGrFCw7he+PvU2g9Gh3JqLVELxvBB1MzVDrn09ndp0DCEo40lpj1Ss+nc57viUVHVvDedW7j9KiqqdOgT4FLvHh4iMk8MH0TNXO0U25HhDf9Cfql/ADwdrJBZWI7Vx1MNXZrBnE0rwJbz1yGTAf/3pzBOrSVq4Rg+iJqxHHUFTqcWAGhc+LCytMDMYR0AAEv2JUJdUWXQ+gxBpxP4+y81U2uf5tRaolaB4YOoGdtxKRtCAN39nODtZN2oazzd0wft3GxRUFaFFQeSDFxh0/18IRPn0wthq7TAHE6tJWoVGD6ImrFtMbdmuTxgYbH7UVjIMSeiZtn1VYeTcUNdYZDaDKGsshoLomtWYp3GqbVErQbDB1EzdbNYg5PJNSuUNmbI5XYRnTzQ3c8J5VVafLk3wRDlGcSyA0nIVlfAx5lTa4laE4YPomZq5+Vs6ATQxccRvi42TbqWTCbDeyNrej/Wn0xHcm6pIUpskszCcizn1FqiVonhg6iZqu9eLvX1UJArhnR0Q7VOYNFO6TedWxB9FZpqHfoEumBUE3t2iMi8MHwQNUP5pZU4lpQHAAb9Yn43IgQyGfDbxSzEZBQZ7LoNdSY1H1svXL+1ay2n1hK1Ng0OH5mZmZg4cSJcXV1hbW2Nzp074/Tp0/rnc3Jy8NJLL8Hb2xs2NjYYOXIkEhKazxgzkTnYdTkbWp1AmJcDAtrYGuy6Yd4OGNutLYCaZdelcPvU2md6+iK8LafWErU2DQofBQUFGDBgACwtLREdHY3Lly/js88+g7OzM4CarbDHjh2LpKQk/Pzzzzh37hz8/f0xbNgwlJZKP8ZMZC623RpyMcZKn7OHB8PSQobDibk4nJBr8Os/yJbzmbiQUQQ7lQJvRwSb/P2JSHqKhpy8YMEC+Pr6IioqSn8sMPD3O9QTEhJw/PhxxMbGolOnTgCAb775Bp6enli3bh0mT55soLKJWq6isiocSawJBaM6G+Z+j9v5uthgQl9//PdoChZsv4r+7QZALjfNsEepplrf4xL5SHu423NqLVFr1KCej61bt6JXr14YP3483N3d0b17d6xcuVL/vEajAQBYWf3+gSKXy6FSqXD48GEDlUzUsu26koNqnUBHD3u0c7Mzynu8ObQ9bJUWiMkswm+31hIxheUHriFHrYGvizVevrXrLhG1Pg0KH0lJSfjmm2/QoUMH7NixA1OnTsX06dPx3XffAQBCQkLg5+eHuXPnoqCgAJWVlViwYAEyMjKQlXX3DziNRgO1Wl3nQdSaRd8KA6OMuLlaGzsVXhvUDgDw2c44VGl1RnuvWhkFZVh+sGaF1Q85tZaoVWtQ+NDpdOjRowc++eQTdO/eHa+99hqmTJmCZcuWAQAsLS2xadMmxMfHw8XFBTY2Nti3bx9GjRoFufzubzVv3jw4OjrqH76+vk1vFZGZUldU4dCt+zBGG2HI5XaTBwaijZ0SKXllWH8q3ajvBQALtsdBU61D30AXRHTi1Fqi1qxB4cPLywthYWF1joWGhiItLU3/c8+ePXH+/HkUFhYiKysL27dvR15eHoKCgu56zblz56KoqEj/SE83/ocgUXO198oNVGp1aOdmiw7uxhlyqWWrUuCtoTWbzn25JwFlldVGe6/TKfn4pXZq7WOcWkvU2jUofAwYMABxcXUXJ4qPj4e/v/8d5zo6OsLNzQ0JCQk4ffo0nnjiibteU6VSwcHBoc6DqLWKjr21l0tnL5N8QT/Xxw9+Lja4WazBt4eTjfIeOp3A33+tmVr7bC9fdPLm1Fqi1q5B4WPWrFk4fvw4PvnkEyQmJmLt2rVYsWIFIiMj9eds2LAB+/fv10+3HT58OMaOHYsRI0YYvHiilqRUU439cTcBGG5V0wdRKuR4e0TNdNdlB5KQX1pp8PfYdC4TF2un1o7grrVE1MDw0bt3b2zevBnr1q1DeHg4/vGPf2Dx4sWYMGGC/pysrCy88MILCAkJwfTp0/HCCy9g3bp1Bi+cqKXZF3cDmmodAlxtEOplb7L3fayLNzp5O6BEU40l+xINeu1STTUW3ppa++bQ9nCzVxn0+kRknmRCCCF1EbdTq9VwdHREUVERh2CoVYlccxa/xWRh6pB2+k3gTOVA/E1M+vYklBZy7H1nMHycm7aRXa1FO+Lw9b5E+LnYYNfsQVApOMOFqKVqyPc393YhagbKK7XYe/UGAGC0iYZcbjeoQxv0C3JFpVaHz3cZZjuEjIIyrDhUM7X2g9GhDB5EpMfwQdQMHIi/gfIqLXycrRHe1vQ9fjKZDO+Nqult2XQuA1ezm77ezrzoq6is1qFfkCsiOnk0+XpE1HIwfBA1A7/v5WKaWS53083XCaM7e0II4NPtcQ9+wX2cSsnHbxezIJMB/8dda4noDxg+iCRWUaXFnis5AIBR4dIuvvXOiI6wkMuw5+oNnErJb9Q1bt+19s+9fRHmzXu3iKguhg8iiR1KyEVppRbejlbo5uskaS1BbnZ4plfNKsPzo6+iMfejbzybgZhMTq0lontj+CCSWO1eLiPDpRtyud3MYR1gZSnHmdQC7L5yo0GvLdFUY+GOmiGbt4a2Rxs7Tq0lojsxfBBJSFOtxa7aIRcjbiTXEB4OVnh5QCAAYOH2q9Dq6t/78c3+RNws1sDf1QYvcddaIroHhg8iCR1NzENxRTXc7VXo6ecsdTl6bwxuB0drSyTcKMHGsxn1ek16fhlWHqpZop1Ta4nofhg+iCS0TT/k4gm5XPohl1qO1paIfKQdAGDxrnhUVGkf+Jr5t6bW9m/nihFhnFpLRPfG8EEkkSqtDjsv185yMf3CYg/yYr8AeDla4XpRBf53LPW+555IysNvMVmQc2otEdUDwweRRI5dy0NReRVcbZXoE+gidTl3sLK0wKxhNZvOLdmfiKLyqruep71t19o/9/FDqBen1hLR/TF8EEkkOrZmyCUi3BMWzWjI5XZP9WiLDu52KCyrwvID1+56zsazGbh0XQ17lQKzhwebuEIiMkcMH0QSqNbqsONSzZCLFHu51JfCQo45ETVrdXx7JBk56oo6z5doqvHpram10x/twKm1RFQvDB9EEjiZnI/80ko421iib1DzG3K53fAwD/T0d0ZFlQ5f7Km76dzSfTVTawNcbTCpf4A0BRKR2WH4IJLAtltDLiPCPGFp0bz/GcpkMrw3smbTuR9OpSPpZgmAmqm1/zlcM7X2wzFhUCqadzuIqPngpwWRiWl1Qj/k0lwWFnuQPoEuGBriDq1OYNHOmmGWedFXUFmtw4D2rhgW6i5xhURkThg+iEzsTGoBbhZr4GClQP92baQup97eHdkRMlnNDrwrDyZhW0w2p9YSUaMwfBCZWO3CYsPDPM1qqCLE0wFPdm8LAPjXtisAgOf6+CHEk1NriahhzOeTj6gF0OkEtsdmAwBGm8mQy+1mDw+G8tY9KvZWnFpLRI3D8EFkQufSC5GtroCdSoGHO5jPkEstH2cbTBlUs+ncuxEd4cqptUTUCAqpCyBqTaJvDbkMC3U3243X3hnRERP6+sPbyVrqUojITLHng8hEhBCIvjXkMqpz811Y7EFkMhmDBxE1CcMHkYlczChCZmE5bJQWGBzsJnU5RESSYfggMpHahcWGhrjDytI8h1yIiAyB4YPIBIQQiI6pneVivkMuRESGwPBBZAKXrquRll8GK0s5hnTkkAsRtW4MH0QmEH1ryOWRju6wUXKSGRG1bgwfREYmhMC2GPOf5UJEZCgMH0RGFpdTjOTcUigVcgwN4QZsREQMH0RGVtvrMTjYDXYqDrkQETF8EBlZ7aqm5riXCxGRMTB8EBlRQk4xEm6UwNJChqEhHlKXQ0TULDB8EBlR7XLqD7dvA0drS4mrISJqHhg+iIxo260hF85yISL6HcMHkZEk55bianYxFHIZRoRxyIWIqBbDB5GR1C4s1q+dK5xslBJXQ0TUfDB8EBkJ93IhIro7hg8iI0jPL0NMZhHkMnDIhYjoDxg+iIygdsjloSBXuNqpJK6GiKh5YfggMgLu5UJEdG8MH0QGlllYjvPphZDJgIhOHHIhIvojhg8iA9t+a2Gx3gEucLe3krgaIqLmh+GDyMD0e7mEcy8XIqK7YfggMqDsogqcTi0AAIwM5/0eRER3w/BBZEA7LtUMufT0d4anI4dciIjuhuGDyID0e7lwyIWI6J4YPogM5GaxBidT8gFwii0R0f0wfBAZyI5L2RAC6OrrhLZO1lKXQ0TUbDF8EBlI7aqmnOVCRHR/DB9EBpBXosHxpFtDLpzlQkR0XwwfRAaw63IOtDqB8LYO8HO1kbocIqJmjeGDyAC23VrVlL0eREQPxvBB1ESFZZU4mpgLgFNsiYjqg+GDqIl2Xc5BtU4gxNMeQW52UpdDRNTsMXwQNYGmWov/Hk0BAIzm2h5ERPXC8EHUBH//5TIuXVfDycYSz/TylbocIiKzwPBB1Egbz2RgzYk0yGTA4me7cS8XIqJ6YvggaoQrWWp8uCUGADDj0Q4Y0tFd4oqIiMwHwwdRA6krqjB19RlUVOkwONgN04d2kLokIiKzwvBB1ABCCLzz4wWk5JWhrZM1Fj/bDXK5TOqyiIjMSoPDR2ZmJiZOnAhXV1dYW1ujc+fOOH36tP75kpISvPnmm/Dx8YG1tTXCwsKwbNkygxZNJJUVB5Ow83IOlBZyLJ3QA862SqlLIiIyO4qGnFxQUIABAwbgkUceQXR0NNzc3JCQkABnZ2f9ObNnz8bevXuxevVqBAQEYOfOnZg2bRq8vb3x+OOPG7wBRKZy7FoeFmy/CgD462Nh6OrrJG1BRERmqkHhY8GCBfD19UVUVJT+WGBgYJ1zjh49ikmTJmHIkCEAgNdeew3Lly/HyZMnGT7IbN1QV+CtdeegE8BT3dtiQl8/qUsiIjJbDRp22bp1K3r16oXx48fD3d0d3bt3x8qVK+uc079/f2zduhWZmZkQQmDfvn2Ij4/HiBEj7npNjUYDtVpd50HUnFRpdYhcexa5JRqEeNrjX092hkzG+zyIiBqrQeEjKSkJ33zzDTp06IAdO3Zg6tSpmD59Or777jv9OV999RXCwsLg4+MDpVKJkSNHYsmSJRg0aNBdrzlv3jw4OjrqH76+XKiJmpeF26/iVEoB7FUKfDOxJ6yVFlKXRERk1mRCCFHfk5VKJXr16oWjR4/qj02fPh2nTp3CsWPHAACLFi3CypUrsWjRIvj7++PgwYOYO3cuNm/ejGHDht1xTY1GA41Go/9ZrVbD19cXRUVFcHBwaErbiJpsW0wWpq05CwBYNrEnRnLjOCKiu1Kr1XB0dKzX93eD7vnw8vJCWFhYnWOhoaHYuHEjAKC8vBwffPABNm/ejDFjxgAAunTpgvPnz2PRokV3DR8qlQoqlaohZRCZxLWbJXj3p4sAgNcHBTF4EBEZSIOGXQYMGIC4uLg6x+Lj4+Hv7w8AqKqqQlVVFeTyupe1sLCATqdrYqlEplNWWY2pq8+gRFONPoEumBPRUeqSiIhajAb1fMyaNQv9+/fHJ598gmeeeQYnT57EihUrsGLFCgCAg4MDBg8ejDlz5sDa2hr+/v44cOAAvv/+e/z73/82SgOIDE0IgbmbYhCfUwI3exW+fr47FBZcj4+IyFAadM8HAPz666+YO3cuEhISEBgYiNmzZ2PKlCn657OzszF37lzs3LkT+fn58Pf3x2uvvYZZs2bVa4ZAQ8aMiIzhf8dS8H8/X4KFXIa1k/uib5Cr1CURETV7Dfn+bnD4MDaGD5LSubQCPLP8GKq0Ah+ODsWUQUFSl0REZBYa8v3NvmSiW/JKNJi25iyqtAIjO3li8sDAB7+IiIgajOGDCIBWJzDzh/PIKqpAYBtbfDq+CxcSIyIyEoYPIgBf7EnAoYRcWFtaYNnEnrC3spS6JCKiFovhgwxu9fFUvPG/M7iYUSh1KfWy7+oNfLknAQAw76nO6OhpL3FFREQtW4Om2hI9SGpeKf629RKqdQI7LmdjYl9/vBPREY7WzbMnIT2/DDN/OA8AeOEhf4zt3lbagoiIWgH2fJBBfbEnAdU6gTZ2SggB/O94Kh79bD82n8tAM5tYhYoqLaatOYui8ip09XXCX/4UKnVJREStAsMHGUzijRJsOZcJAFg1qTfWTumLdm62yC2pxKwfLuC5lceReKNY4ip/9/EvlxGTWQRnG0ssndADKgU3jCMiMgWGDzKYxbvjoRPAsFAPdPV1Qv92bRA9YxDmRHSElaUcx5PyMeqLQ1iw/SrKK7WS1rrhdDrWnUyDTAYs/nN3tHWylrQeIqLWhOGDDOJKlhq/XswCAMweHqw/rlTIEflIe+yaNRiPhrijSivwzf5rGPbvA9h9OUeSWi9fV+MvW2IBADMfDcbgYDdJ6iAiaq0YPsggPt8VDwAY08ULYd53rmzn62KDVS/1xooXeqKtkzUyC8sx+fvTmPzdaWQUlJmszqLyKkxdcwaaah2GdHTDW0Pbm+y9iYioBsMHNdnFjELsvJwDuQyYNazDfc8d0ckTu2YPwhuD20Ehl2H3lRwM+/cBLN2fiMpq4+58rNMJvP3jBaTmlaGtkzU+f6Yb5HIuJEZEZGoMH9Rkn+2s6fUY260t2rs/eI0MG6UC748KwbYZA9En0AUVVTos3B6HMV8ewvGkPKPVufxgEnZfyYHSQo5vJvaAs63SaO9FRET3xvBBTXI6JR8H4m/CQi7DjAf0evxRsIc9fnjtIXw2vitcbZVIuFGCP684jtk/nEduicagdR67lodPd1wFAPzt8U7o4uNk0OsTEVH9MXxQk9T2ejzTywf+rrYNfr1MJsO4nj7Y+/YQTOjrB5kM2HQuE0MX7cf/jqdCq2v62iA56gq8te4sdAIY18MHz/XxbfI1iYio8Rg+qNGOJubiWFIelBZyvDm0Yb0ef+RoY4l/PdkZm6b2RydvB6grqvF/W2Lx1NIjiMkoavR1q7Q6RK45i9ySSoR42uOfY8O5YRwRkcQYPqhRhBD47NYMl+f6+BpsnYzufs7Y+ubD+NtjYbBXKXAhowhPLDmMj36OhbqiqsHXmx99FadTC2CvUuCbiT1hreRCYkREUmP4oEbZH38TZ1ILoLq1jochWchleGlAIPa8PRiPd/WGTgDfHUvF0EUH8PP5zHov0/7bxSysOpwMAFj0TFcEtmn4sBARERkewwc1mBACn+2MAwC82M8f7g5WRnkfdwcrfPlcd6x+tS+C2tgit0SDGevPY8J/TiDxRsl9X5t4owTv/nQBAPD64CBEdPI0So1ERNRwDB/UYDsu5SA2Uw0bpQXeGNzO6O/3cIc2iJ45EG8PD4ZKIcfRa3kY9cVBLNoRd9dl2ks11Zi6+gxKK7XoG+iCOSM6Gr1GIiKqP4YPahCdTuhXM31lQCBc7VQmeV+VwgJvPdoBu2YNxiMd3VClFfh6XyKGf34Ae6/+vky7EAJzN8Ug4UYJ3O1V+Or57lBY8M+ciKg54acyNcivMVmIyymGvZUCUwYGmfz9/Vxt8O1LvbFsYg94OVoho6Acr/z3NF77/jQyC8vx/bFUbL1wHRZyGZZM6AF3e+MMCRERUeMppC6AzEe1VofFt3o9pgwMgqONpSR1yGQyjAz3wsAObvhiTwJWHU7Gzss5OJSQi2pdzRLtc0eFoHeAiyT1ERHR/bHng+pty/nrSMothbONJV4eECB1ObBVKfDB6FD8Nv1h9A5wRnmVFlVagdGdPfHqw4FSl0dERPfAng+ql8pqHb7YU9Pr8cbgdrC3kqbX425CPB3ww2v98POFTFzNLsZbQztwITEiomaM4YPqZcOZdKTnl6ONnQov9guQupw7yOUyPNndR+oyiIioHjjsQg9UUaXF13sTAQCRj7TjKqFERNQkDB/0QOtOpiGrqAJejlZ4ro+f1OUQEZGZY/ig+yqv1GLJvmsAgDeHtoeVJXs9iIioaRg+6L6+P5aC3BINfF2sMb4nt6InIqKmY/igeyquqMKyAzW9HjMeDYZSwT8XIiJqOn6b0D1FHUlBQVkVgtrYYmw3b6nLISKiFoLhg+6qqKwKKw8lAQBmDg/m/ihERGQw/Eahu1p5KAnFFdXo6GGPP3X2krocIiJqQRg+6A55JRp8eyQZADBreDDkcq4WSkREhsPwQXdYfjAJZZVahLd1QEQnD6nLISKiFobhg+q4oa7Ad0dTAABvj+jIPVKIiMjgGD6ojiX7EqGp1qGnvzOGBLtJXQ4REbVADB+kl1lYjnUn0wEAbw8PZq8HEREZBcMH6X29NwGVWh36Bbmif/s2UpdDREQtFMMHAQBSckvx4+kMAMDbI4IlroaIiFoyhg8CAHy5JwFancDgYDf0CnCRuhwiImrBGD4IiTeKseV8JgD2ehARkfExfBA+350AnQBGhHmgi4+T1OUQEVELx/DRyl2+rsZvF7MA1KxmSkREZGwMH63c57vjAQB/6uKFUC8HiashIqLWgOGjFbuQXohdl3MglwEzh7HXg4iITIPhoxX7bFdNr8fY7m3R3t1O4mqIiKi1YPhopU6l5ONg/E0o5DLMeLSD1OUQEVErwvDRCgkhsGhHHABgfC9f+LvaSlwRERG1JgwfrdDRa3k4kZwPpYUcbw1tL3U5RETUyjB8tDJCCHy2s6bX4/m+fvB2spa4IiIiam0YPkwgPqcYZ1ILoNUJqUvB/ribOJtWCCtLOaYNaSd1OURE1AoppC6gpaqo0uLXi1lYcyIV59IKAQBt7JSI6OSJMZ290CfQBQoL02Y/IQQW3er1mNQvAO4OViZ9fyIiIoDhw+ASb5Rg7Yk0/HQmHeqKagCApYUM1pYWyC2pxJoTaVhzIg2utkqMuBVEHgoyTRDZcSkbl66rYau0wOuD2etBRETSYPgwgMpqHXZcysaaE6k4npSvP+7jbI3n+/phfE9fONlY4ui1PGy7mIUdl7ORV1qJdSfTsO5kGlxslYjo5IHRnb3wUJArLI0QRLQ6gX/fWtfjlYcD4WKrNPh7EBER1YdMCCH9jQi3UavVcHR0RFFRERwcmvdy3+n5ZVh7Mg0bTqcjt6QSACCXAY+GemBCXz8M6uAGuVx2x+uqtDocT8rDtpgs7LiUg/zSSv1zzjaWGBHmidFdvNC/neGCyM/nMzFj/Xk4WClw6L2hcLS2NMh1iYiIgIZ9fzN8NFC1Voe9V29gzYk0HEy4idr/9TwcVPhzbz8829u3QTNIqrU6HE/Kx7bYLOyIrekRqeVkY4kRYTU9Iv3btYFS0bggUq3VYfjnB5GcW4q3hwfjLS4qRkREBsbwYQTZRRVYfyoN60+mI1tdoT8+KNgNE/r64dEQ9ybft1Gt1eFkcj5+i8nCjkvZ+t4UAHC0tsTwMA+M6eyFAe0bFkQ2nE7HnJ8uwtnGEofeGwo7FUfbiIjIsBg+DESnEziUmIs1x1Ox5+oN/VRZF1slxvfywfN9/Iy2OqhWJ3AyOR/bYrIQHZuN3BKN/jkHKwWGh3lidGdPPNyhDVQKi3tep7Jah6Gf7UdGQTk+GB2C1wbxRlMiIjI8o4aPzMxMvPfee4iOjkZZWRnat2+PqKgo9OrVq+aCsjvvcQCAhQsXYs6cOQYt3lhySzTYcDoDa0+mIj2/XH+8T6ALJvT1w8hwz/t+4RuaVidwKuX3IHKz+PcgYm+lwPDQmqGZgcF3BpHVx1Pxly2xcLNX4eCcR2CtNF3dRETUejTk+7tB/e8FBQUYMGAAHnnkEURHR8PNzQ0JCQlwdnbWn5OVlVXnNdHR0Xj11Vcxbty4hryVyQkhcCI5H2tOpGF7bBaqtDWZzN5KgXE9fDChrx86eNhLUpuFXIaHglzxUJArPnqsE86kFmBbTBa2xWThRrEGm85lYtO5TNirFBgW5oFR4Z4YFOwGAPh6byIAIHJIOwYPIiJqFhrU8/H+++/jyJEjOHToUL3fYOzYsSguLsaePXvqdb6pez6Kyqqw8WwG1pxIxbWbpfrjXX2dMKGvHx7r4t1sv7R1OoEzaQX47WIWtsdm17kXxU6lQAcPO5xLK4S3oxX2zRli0t4aIiJqXYw27BIWFoaIiAhkZGTgwIEDaNu2LaZNm4YpU6bc9fycnBz4+Pjgu+++w/PPP3/XczQaDTSa34cR1Go1fH19jRo+hBA4n16INSfS8MuF69BU6wAANkoLPNGtLSb09UN4W0ejvLex6HQC59IL8NvFbETHZiGr6Pcg8smTnfF8Xz8JqyMiopbOaOHDyqpmOe7Zs2dj/PjxOHXqFGbMmIFly5Zh0qRJd5y/cOFCzJ8/H9evX9e/9o/+9re/4eOPP77juDHCR4mmGj+fz8Sa42m4nKXWHw/xtMeEh/wxtps37K3Mf/2LmiBSiO2xWZDLZHgnoqNRFi4jIiKqZbTwoVQq0atXLxw9elR/bPr06Th16hSOHTt2x/khISEYPnw4vvrqq3te01Q9Hwk5xRi75AhKK7U1bVHI8acuXpjQ1x89/JzueaMsERERPZjRbjj18vJCWFhYnWOhoaHYuHHjHeceOnQIcXFx+OGHH+57TZVKBZVK1ZAyGqWdmx2cbZVwd5BjQl8/jOvhA2cuMU5ERGRyDQofAwYMQFxcXJ1j8fHx8Pf3v+PcVatWoWfPnujatWvTKjQQuVyGH1/vBy9HK/ZyEBERSahBNwLMmjULx48fxyeffILExESsXbsWK1asQGRkZJ3z1Go1NmzYgMmTJxu02KbydrJm8CAiIpJYg8JH7969sXnzZqxbtw7h4eH4xz/+gcWLF2PChAl1zlu/fj2EEHjuuecMWiwRERGZPy6vTkRERE3WkO9vzr8kIiIik2L4ICIiIpNi+CAiIiKTYvggIiIik2L4ICIiIpNi+CAiIiKTYvggIiIik2L4ICIiIpNi+CAiIiKTYvggIiIik2L4ICIiIpNSSF3AH9VuNaNWqyWuhIiIiOqr9nu7PlvGNbvwUVxcDADw9fWVuBIiIiJqqOLiYjg6Ot73nGa3q61Op8P169dhb28PmUxm0Gur1Wr4+voiPT29VeyYy/a2bK2tvUDrazPb27K1tPYKIVBcXAxvb2/I5fe/q6PZ9XzI5XL4+PgY9T0cHBxaxC+6vtjelq21tRdofW1me1u2ltTeB/V41OINp0RERGRSDB9ERERkUq0qfKhUKnz00UdQqVRSl2ISbG/L1traC7S+NrO9LVtra+/tmt0Np0RERNSytaqeDyIiIpIewwcRERGZFMMHERERmRTDBxEREZmU2YWPefPmoXfv3rC3t4e7uzvGjh2LuLi4OudUVFQgMjISrq6usLOzw7hx45CTk1PnnLS0NIwZMwY2NjZwd3fHnDlzUF1dXeec/fv3o0ePHlCpVGjfvj3++9//Grt5dzBVezdt2oThw4fDzc0NDg4O6NevH3bs2GGSNt7OlL/fWkeOHIFCoUC3bt2M1ax7MmV7NRoNPvzwQ/j7+0OlUiEgIADffvut0dt4O1O2d82aNejatStsbGzg5eWFV155BXl5eUZv4+0M1d7p06ejZ8+eUKlU9/w7vXjxIgYOHAgrKyv4+vpi4cKFxmrWPZmqvfv378cTTzwBLy8v2Nraolu3blizZo0xm3ZXpvz91kpMTIS9vT2cnJwM3BoTE2YmIiJCREVFidjYWHH+/HkxevRo4efnJ0pKSvTnvPHGG8LX11fs2bNHnD59Wjz00EOif//++uerq6tFeHi4GDZsmDh37pzYtm2baNOmjZg7d67+nKSkJGFjYyNmz54tLl++LL766ithYWEhtm/f3iLbO2PGDLFgwQJx8uRJER8fL+bOnSssLS3F2bNnW2R7axUUFIigoCAxYsQI0bVrV1M0sQ5Ttvfxxx8Xffv2Fbt27RLJycni6NGj4vDhwyZrqxCma+/hw4eFXC4XX3zxhUhKShKHDh0SnTp1Ek8++aTZtVcIId566y3x9ddfixdeeOGuf6dFRUXCw8NDTJgwQcTGxop169YJa2trsXz5cmM3sQ5Ttfdf//qX+Mtf/iKOHDkiEhMTxeLFi4VcLhe//PKLsZtYh6naW6uyslL06tVLjBo1Sjg6OhqpVaZhduHjj27cuCEAiAMHDgghhCgsLBSWlpZiw4YN+nOuXLkiAIhjx44JIYTYtm2bkMvlIjs7W3/ON998IxwcHIRGoxFCCPHuu++KTp061XmvZ599VkRERBi7SfdlrPbeTVhYmPj444+N1JL6MXZ7n332WfGXv/xFfPTRR5KEjz8yVnujo6OFo6OjyMvLM2FrHsxY7f30009FUFBQnff68ssvRdu2bY3dpPtqTHtvd6+/06VLlwpnZ+c6f9/vvfee6Nixo+Eb0QDGau/djB49Wrz88ssGqbuxjN3ed999V0ycOFFERUWZffgwu2GXPyoqKgIAuLi4AADOnDmDqqoqDBs2TH9OSEgI/Pz8cOzYMQDAsWPH0LlzZ3h4eOjPiYiIgFqtxqVLl/Tn3H6N2nNqryEVY7X3j3Q6HYqLi/XvIxVjtjcqKgpJSUn46KOPTNGUejFWe7du3YpevXph4cKFaNu2LYKDg/HOO++gvLzcVE27K2O1t1+/fkhPT8e2bdsghEBOTg5++uknjB492lRNu6vGtLc+jh07hkGDBkGpVOqPRUREIC4uDgUFBQaqvuGM1d57vZc5fl7V1969e7FhwwYsWbLEcAVLqNltLNcQOp0OM2fOxIABAxAeHg4AyM7OhlKpvGM8zMPDA9nZ2fpzbv/gqn2+9rn7naNWq1FeXg5ra2tjNOm+jNneP1q0aBFKSkrwzDPPGLgV9WfM9iYkJOD999/HoUOHoFA0j38GxmxvUlISDh8+DCsrK2zevBm5ubmYNm0a8vLyEBUVZeSW3Z0x2ztgwACsWbMGzz77LCoqKlBdXY3HHntM0g/uxra3PrKzsxEYGHjHNWqfc3Z2blrxjWDM9v7Rjz/+iFOnTmH58uVNKblJjNnevLw8vPTSS1i9enWL2YCueXzqNlJkZCRiY2Nx+PBhqUsxCVO1d+3atfj444/x888/w93d3ajvdT/Gaq9Wq8Xzzz+Pjz/+GMHBwQa9dlMY8/er0+kgk8mwZs0a/a6T//73v/H0009j6dKlkoRpY7b38uXLmDFjBv76178iIiICWVlZmDNnDt544w2sWrXK4O9XH/y8Mo59+/bh5ZdfxsqVK9GpUyejvtf9GLO9U6ZMwfPPP49BgwYZ/NpSMdthlzfffBO//vor9u3bBx8fH/1xT09PVFZWorCwsM75OTk58PT01J/zx7uNa39+0DkODg6SfFAbu7211q9fj8mTJ+PHH3+8Y9jJlIzZ3uLiYpw+fRpvvvkmFAoFFAoF/v73v+PChQtQKBTYu3evcRt3F8b+/Xp5eaFt27Z1trsODQ2FEAIZGRnGaNJ9Gbu98+bNw4ABAzBnzhx06dIFERERWLp0Kb799ltkZWUZsWV315T21kdD/o2bgrHbW+vAgQN47LHH8Pnnn+PFF19satmNZuz27t27F4sWLdJ/Xr366qsoKiqCQqEw+Yw1g5H6ppOG0ul0IjIyUnh7e4v4+Pg7nq+9weenn37SH7t69epdb1jLycnRn7N8+XLh4OAgKioqhBA1N/aEh4fXufZzzz1n8htOTdVeIYRYu3atsLKyElu2bDFii+7PFO3VarUiJiamzmPq1KmiY8eOIiYmps6d6sZmqt/v8uXLhbW1tSguLtafs2XLFiGXy0VZWZmxmncHU7X3qaeeEs8880ydax89elQAEJmZmcZo2l0Zor23e9ANp5WVlfpjc+fONfkNp6ZqrxBC7Nu3T9ja2oqvv/7aYPU3lKnae/ny5TqfV//85z+Fvb29iImJEfn5+QZtk6mYXfiYOnWqcHR0FPv37xdZWVn6x+0foG+88Ybw8/MTe/fuFadPnxb9+vUT/fr10z9fO1VvxIgR4vz582L79u3Czc3trlNt58yZI65cuSKWLFkiyVRbU7V3zZo1QqFQiCVLltR5n8LCwhbZ3j+SaraLqdpbXFwsfHx8xNNPPy0uXbokDhw4IDp06CAmT57cItsbFRUlFAqFWLp0qbh27Zo4fPiw6NWrl+jTp4/ZtVcIIRISEsS5c+fE66+/LoKDg8W5c+fEuXPn9LNbCgsLhYeHh3jhhRdEbGysWL9+vbCxsTH5VFtTtXfv3r3CxsZGzJ07t877mHo2l6na+0ctYbaL2YUPAHd9REVF6c8pLy8X06ZNE87OzsLGxkY8+eSTIisrq851UlJSxKhRo4S1tbVo06aNePvtt0VVVVWdc/bt2ye6desmlEqlCAoKqvMepmKq9g4ePPiu7zNp0iQTtbSGKX+/t5MqfJiyvVeuXBHDhg0T1tbWwsfHR8yePdukvR5CmLa9X375pQgLCxPW1tbCy8tLTJgwQWRkZJiimXqGau+9/n0mJyfrz7lw4YJ4+OGHhUqlEm3bthXz5883USt/Z6r2Tpo06a7PDx482HSNFab9/d6uJYQPmRBCNHSohoiIiKixzPaGUyIiIjJPDB9ERERkUgwfREREZFIMH0RERGRSDB9ERERkUgwfREREZFIMH0RERGRSDB9ERERkUgwfRNQoQggMGzYMERERdzy3dOlSODk5SbJpHRE1fwwfRNQoMpkMUVFROHHiBJYvX64/npycjHfffRdfffVVnR0+DaGqqsqg1yMiaTB8EFGj+fr64osvvsA777yD5ORkCCHw6quvYsSIEejevTtGjRoFOzs7eHh44IUXXkBubq7+tdu3b8fDDz8MJycnuLq64k9/+hOuXbumfz4lJQUymQw//PADBg8eDCsrK6xZs0aKZhKRgXFvFyJqsrFjx6KoqAhPPfUU/vGPf+DSpUvo1KkTJk+ejBdffBHl5eV47733UF1djb179wIANm7cCJlMhi5duqCkpAR//etfkZKSgvPnz0MulyMlJQWBgYEICAjAZ599hu7du8PKygpeXl4St5aImorhg4ia7MaNG+jUqRPy8/OxceNGxMbG4tChQ9ixY4f+nIyMDPj6+iIuLg7BwcF3XCM3Nxdubm6IiYlBeHi4PnwsXrwYM2bMMGVziMjIOOxCRE3m7u6O119/HaGhoRg7diwuXLiAffv2wc7OTv8ICQkBAP3QSkJCAp577jkEBQXBwcEBAQEBAIC0tLQ61+7Vq5dJ20JExqeQugAiahkUCgUUipqPlJKSEjz22GNYsGDBHefVDps89thj8Pf3x8qVK+Ht7Q2dTofw8HBUVlbWOd/W1tb4xRORSTF8EJHB9ejRAxs3bkRAQIA+kNwuLy8PcXFxWLlyJQYOHAgAOHz4sKnLJCKJcNiFiAwuMjIS+fn5eO6553Dq1Clcu3YNO3bswMsvvwytVgtnZ2e4urpixYoVSExMxN69ezF79mypyyYiE2H4ICKD8/b2xpEjR6DVajFixAh07twZM2fOhJOTE+RyOeRyOdavX48zZ84gPDwcs2bNwqeffip12URkIpztQkRERCbFng8iIiIyKYYPIiIiMimGDyIiIjIphg8iIiIyKYYPIiIiMimGDyIiIjIphg8iIiIyKYYPIiIiMimGDyIiIjIphg8iIiIyKYYPIiIiMimGDyIiIjKp/we38rBfb9BVdAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw_slbcVyvwM",
        "outputId": "44b38722-50f7-4c82-d89d-a9d521242e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Country', 'Year', 'Status', 'Life expectancy ', 'Adult Mortality',\n",
              "       'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B',\n",
              "       'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure',\n",
              "       'Diphtheria ', ' HIV/AIDS', 'GDP', 'Population',\n",
              "       ' thinness  1-19 years', ' thinness 5-9 years',\n",
              "       'Income composition of resources', 'Schooling'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "S56W_Fal0ADj",
        "outputId": "fa052056-e75c-491b-cb59-f902d295c995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Country  Year      Status  Life expectancy   Adult Mortality  \\\n",
              "0                Finland  2013  Developing              87.0             79.0   \n",
              "1                  Japan  2015   Developed              83.7             55.0   \n",
              "2   Syrian Arab Republic  2014  Developing              64.4            294.0   \n",
              "3                 Latvia  2010   Developed              72.8             18.0   \n",
              "4  Sao Tome and Principe  2013  Developing              67.1            192.0   \n",
              "\n",
              "   infant deaths  Alcohol  percentage expenditure  Hepatitis B  Measles   ...  \\\n",
              "0              0     8.97             6115.496624          NaN         2  ...   \n",
              "1              2      NaN                0.000000          NaN        35  ...   \n",
              "2              7     0.01                0.000000         47.0       594  ...   \n",
              "3              0     9.80             1109.969508         91.0         0  ...   \n",
              "4              0     0.01              200.660099         97.0         0  ...   \n",
              "\n",
              "   Polio  Total expenditure  Diphtheria    HIV/AIDS           GDP  Population  \\\n",
              "0   98.0               9.55         98.0        0.1  49638.771300   5438972.0   \n",
              "1   99.0                NaN         96.0        0.1  34474.137360    127141.0   \n",
              "2   52.0               3.25         43.0        0.1           NaN     19239.0   \n",
              "3   92.0               6.55         92.0        0.1  11326.219470    297555.0   \n",
              "4   97.0               9.76         97.0        0.2   1619.532678     18745.0   \n",
              "\n",
              "    thinness  1-19 years   thinness 5-9 years  \\\n",
              "0                    0.9                  0.8   \n",
              "1                    2.1                  1.8   \n",
              "2                    6.3                  6.1   \n",
              "3                    2.2                  2.3   \n",
              "4                    5.7                  5.5   \n",
              "\n",
              "   Income composition of resources  Schooling  \n",
              "0                            0.887       17.0  \n",
              "1                            0.902       15.3  \n",
              "2                            0.575        9.0  \n",
              "3                            0.815       16.0  \n",
              "4                            0.559       11.0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ce7324e-5a68-4f9f-9447-267e527e2d7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Status</th>\n",
              "      <th>Life expectancy</th>\n",
              "      <th>Adult Mortality</th>\n",
              "      <th>infant deaths</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>percentage expenditure</th>\n",
              "      <th>Hepatitis B</th>\n",
              "      <th>Measles</th>\n",
              "      <th>...</th>\n",
              "      <th>Polio</th>\n",
              "      <th>Total expenditure</th>\n",
              "      <th>Diphtheria</th>\n",
              "      <th>HIV/AIDS</th>\n",
              "      <th>GDP</th>\n",
              "      <th>Population</th>\n",
              "      <th>thinness  1-19 years</th>\n",
              "      <th>thinness 5-9 years</th>\n",
              "      <th>Income composition of resources</th>\n",
              "      <th>Schooling</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Finland</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>87.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.97</td>\n",
              "      <td>6115.496624</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>98.0</td>\n",
              "      <td>9.55</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>49638.771300</td>\n",
              "      <td>5438972.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.887</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2015</td>\n",
              "      <td>Developed</td>\n",
              "      <td>83.7</td>\n",
              "      <td>55.0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>...</td>\n",
              "      <td>99.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>34474.137360</td>\n",
              "      <td>127141.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.902</td>\n",
              "      <td>15.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Syrian Arab Republic</td>\n",
              "      <td>2014</td>\n",
              "      <td>Developing</td>\n",
              "      <td>64.4</td>\n",
              "      <td>294.0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.0</td>\n",
              "      <td>594</td>\n",
              "      <td>...</td>\n",
              "      <td>52.0</td>\n",
              "      <td>3.25</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19239.0</td>\n",
              "      <td>6.3</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0.575</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Latvia</td>\n",
              "      <td>2010</td>\n",
              "      <td>Developed</td>\n",
              "      <td>72.8</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.80</td>\n",
              "      <td>1109.969508</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>92.0</td>\n",
              "      <td>6.55</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>11326.219470</td>\n",
              "      <td>297555.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.815</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sao Tome and Principe</td>\n",
              "      <td>2013</td>\n",
              "      <td>Developing</td>\n",
              "      <td>67.1</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>200.660099</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9.76</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1619.532678</td>\n",
              "      <td>18745.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.559</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ce7324e-5a68-4f9f-9447-267e527e2d7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ce7324e-5a68-4f9f-9447-267e527e2d7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ce7324e-5a68-4f9f-9447-267e527e2d7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the average life expectancy of \"Developed\" and \"Developing\" countries for each year "
      ],
      "metadata": {
        "id": "_euMb0n0U_PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby([\"Year\", \"Status\"])[\"Life expectancy \"].mean()\n",
        "grouped.unstack().plot(kind=\"line\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:37.167421Z",
          "iopub.execute_input": "2023-04-30T08:33:37.167866Z",
          "iopub.status.idle": "2023-04-30T08:33:37.181164Z",
          "shell.execute_reply.started": "2023-04-30T08:33:37.167824Z",
          "shell.execute_reply": "2023-04-30T08:33:37.179864Z"
        },
        "trusted": true,
        "id": "FHvcfXklU_PS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "dde49e18-c111-480d-cf78-f8669d73c86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the correlation between \"Life expectancy\" and \"GDP\""
      ],
      "metadata": {
        "id": "v3PokphQU_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df[\"Life expectancy \"].corr(df[\"GDP\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:39.947445Z",
          "iopub.execute_input": "2023-04-30T08:33:39.947838Z",
          "iopub.status.idle": "2023-04-30T08:33:39.956485Z",
          "shell.execute_reply.started": "2023-04-30T08:33:39.947799Z",
          "shell.execute_reply": "2023-04-30T08:33:39.955285Z"
        },
        "trusted": true,
        "id": "mOfNroPqU_PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_TvDpUj0YU8",
        "outputId": "08908dd6-2f5e-413e-f4c1-ec0e1153ab82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4652553879500644"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find out the feature having the strongest correlation with the \"Life expectancy\"<br>\n",
        "Note: Do not display all of the correlation values. Your code should only print the result (the name of a column)."
      ],
      "metadata": {
        "id": "uL-6KStPU_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df.corr()\n",
        "\n",
        "strongest_corr_feature = corr_matrix[\"Life expectancy \"].abs().idxmax()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:44.957730Z",
          "iopub.execute_input": "2023-04-30T08:33:44.958938Z",
          "iopub.status.idle": "2023-04-30T08:33:44.977069Z",
          "shell.execute_reply.started": "2023-04-30T08:33:44.958881Z",
          "shell.execute_reply": "2023-04-30T08:33:44.975781Z"
        },
        "trusted": true,
        "id": "sHqKM8nKU_PS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009c14ff-459c-44d0-81a1-245064b06298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-3bf380138da4>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  corr_matrix = df.corr()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strongest_corr_feature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Cw6gioA_0YQk",
        "outputId": "1bdc8a70-0eaf-4f28-bd37-54524f82ce49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Life expectancy '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df.corr().abs()\n",
        "\n",
        "second_corr_feature = corr_matrix[\"Life expectancy \"].sort_values(ascending=False)[1:].idxmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_NzUemW3idL",
        "outputId": "1e71d1fa-3c99-429b-f3ea-6024e5ffb920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-38619ce52e2a>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  corr_matrix = df.corr().abs()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_corr_feature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n6Qwf_Zo3kyz",
        "outputId": "cdc1c04c-a98b-4179-b5aa-c78e96555482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Schooling'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing (20 points)"
      ],
      "metadata": {
        "id": "Pbgj4XbWU_PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove the rows containing NaN (or null) values"
      ],
      "metadata": {
        "id": "MeJjivW3U_PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:49.739974Z",
          "iopub.execute_input": "2023-04-30T08:33:49.740394Z",
          "iopub.status.idle": "2023-04-30T08:33:49.749989Z",
          "shell.execute_reply.started": "2023-04-30T08:33:49.740354Z",
          "shell.execute_reply": "2023-04-30T08:33:49.748700Z"
        },
        "trusted": true,
        "id": "ZbBXGM9eU_PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert categorical columns into their one-hot encoded versions"
      ],
      "metadata": {
        "id": "8tbAjlb2U_PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "df = pd.get_dummies(df, columns=cat_cols)"
      ],
      "metadata": {
        "id": "GAL14uy8RP2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f86IPu-RlE_",
        "outputId": "fb91fd15-19a2-478e-9226-01fb82f59fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Year  Life expectancy   Adult Mortality  infant deaths  Alcohol  \\\n",
            "3   2010              72.8             18.0              0     9.80   \n",
            "4   2013              67.1            192.0              0     0.01   \n",
            "7   2005              73.1            144.0             15     4.38   \n",
            "11  2012              63.4              3.0              2     0.01   \n",
            "12  2014              76.6            122.0             31     5.26   \n",
            "\n",
            "    percentage expenditure  Hepatitis B  Measles    BMI   under-five deaths   \\\n",
            "3              1109.969508         91.0         0   58.9                   0   \n",
            "4               200.660099         97.0         0   29.3                   0   \n",
            "7               531.980818         93.0         0    5.5                  18   \n",
            "11               12.834474         95.0         7   36.2                   3   \n",
            "12              168.173753         84.0         3   62.8                  36   \n",
            "\n",
            "    ...  Country_Turkmenistan  Country_Uganda  Country_Ukraine  \\\n",
            "3   ...                     0               0                0   \n",
            "4   ...                     0               0                0   \n",
            "7   ...                     0               0                0   \n",
            "11  ...                     0               0                0   \n",
            "12  ...                     0               0                0   \n",
            "\n",
            "    Country_Uruguay  Country_Uzbekistan  Country_Vanuatu  Country_Zambia  \\\n",
            "3                 0                   0                0               0   \n",
            "4                 0                   0                0               0   \n",
            "7                 0                   0                0               0   \n",
            "11                0                   0                0               0   \n",
            "12                0                   0                0               0   \n",
            "\n",
            "    Country_Zimbabwe  Status_Developed  Status_Developing  \n",
            "3                  0                 1                  0  \n",
            "4                  0                 0                  1  \n",
            "7                  0                 0                  1  \n",
            "11                 0                 0                  1  \n",
            "12                 0                 0                  1  \n",
            "\n",
            "[5 rows x 155 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the dataframe into two numpy arrays (called `x` and `y`).<br>\n",
        "To make the first array, remove the \"Life expectancy\" column, and convert the remaining dataframe to a numpy array.<br>\n",
        "Then, use the removed column to make another numpy array (`y`)."
      ],
      "metadata": {
        "id": "C6gaeeHxU_PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.drop(columns=[\"Life expectancy \"]).to_numpy()\n",
        "\n",
        "y = df[\"Life expectancy \"].to_numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:33:57.363830Z",
          "iopub.execute_input": "2023-04-30T08:33:57.364230Z",
          "iopub.status.idle": "2023-04-30T08:33:57.373246Z",
          "shell.execute_reply.started": "2023-04-30T08:33:57.364195Z",
          "shell.execute_reply": "2023-04-30T08:33:57.372021Z"
        },
        "trusted": true,
        "id": "dbN1zZ-dU_PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7rU5BVqQki-",
        "outputId": "9d577d14-b606-4b68-9c92-37f52f916f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Year', 'Life expectancy ', 'Adult Mortality', 'infant deaths',\n",
              "       'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles ', ' BMI ',\n",
              "       'under-five deaths ',\n",
              "       ...\n",
              "       'Country_Turkmenistan', 'Country_Uganda', 'Country_Ukraine',\n",
              "       'Country_Uruguay', 'Country_Uzbekistan', 'Country_Vanuatu',\n",
              "       'Country_Zambia', 'Country_Zimbabwe', 'Status_Developed',\n",
              "       'Status_Developing'],\n",
              "      dtype='object', length=155)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the data"
      ],
      "metadata": {
        "id": "X0OE0dA1U_PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:34:01.194507Z",
          "iopub.execute_input": "2023-04-30T08:34:01.195165Z",
          "iopub.status.idle": "2023-04-30T08:34:01.204086Z",
          "shell.execute_reply.started": "2023-04-30T08:34:01.195123Z",
          "shell.execute_reply": "2023-04-30T08:34:01.202988Z"
        },
        "trusted": true,
        "id": "SBYXkQCbU_PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRbVUTn3QAiK",
        "outputId": "738c27b4-c5e7-4354-c979-573ee43add4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.52684427 -1.20551632 -0.27267108 ... -0.08851417  2.49387004\n",
            "  -2.49387004]\n",
            " [ 1.26862003  0.19437979 -0.27267108 ... -0.08851417 -0.40098321\n",
            "   0.40098321]\n",
            " [-0.70944867 -0.19179845 -0.15310568 ... -0.08851417 -0.40098321\n",
            "   0.40098321]\n",
            " ...\n",
            " [ 0.03232709  0.54033113 -0.22484492 ... -0.08851417 -0.40098321\n",
            "   0.40098321]\n",
            " [-0.21493149  1.66668432  0.42080824 ... -0.08851417 -0.40098321\n",
            "   0.40098321]\n",
            " [-1.94574161 -0.44925061 -0.24078697 ... -0.08851417 -0.40098321\n",
            "   0.40098321]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "0GwHCHMlU_PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data to training (80%) and testing (20%) parts (5 points)"
      ],
      "metadata": {
        "id": "IksGhWU6U_PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:34:04.739033Z",
          "iopub.execute_input": "2023-04-30T08:34:04.739581Z",
          "iopub.status.idle": "2023-04-30T08:34:04.748932Z",
          "shell.execute_reply.started": "2023-04-30T08:34:04.739529Z",
          "shell.execute_reply": "2023-04-30T08:34:04.747440Z"
        },
        "trusted": true,
        "id": "YAdRX8VVU_PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Regression Class"
      ],
      "metadata": {
        "id": "LMBCL_FAU_PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a regression class and you do not need to change its code. You have to fully understand it and then create new classes for rigde and lasso regression which inherit from this class.\n",
        "For initialization you have to specify learning rate, number of iteration, and a regularization object."
      ],
      "metadata": {
        "id": "lVm0vp2_U_PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Regression:\n",
        "    def __init__(self, learning_rate, iteration, regularization):\n",
        "        \n",
        "        self.N = None # number of samples\n",
        "        self.n = None # number of features\n",
        "        self.w = None # initial weights\n",
        "        self.regularization = regularization # will be the l1/l2 regularization class according to the regression model\n",
        "        self.lr = learning_rate\n",
        "        self.it = iteration\n",
        "\n",
        "    def loss_function(self, y, y_pred):\n",
        "        return (1 / (2*self.N)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n",
        "    \n",
        "    def hypothesis(self, weights, X):\n",
        "        return np.dot(X, weights)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        # Target value should be in the shape of (n, 1) not (n, ).\n",
        "\n",
        "        # Insert constant ones for bias weights.\n",
        "        X = np.insert(X, 0, 1, axis=1)\n",
        "\n",
        "        self.N = X.shape[0]\n",
        "        self.n = X.shape[1]\n",
        "        self.w = np.zeros(self.n)\n",
        "\n",
        "        for it in range(1, self.it+1):\n",
        "            y_pred = self.hypothesis(self.w, X)\n",
        "            cost = self.loss_function(y, y_pred)\n",
        "            dw = (1/self.N) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n",
        "            self.w = self.w - self.lr * dw\n",
        "\n",
        "            if it % 10 == 0:\n",
        "                print(\"The loss function for the iteration {}----->{} :)\".format(it, cost))\n",
        "    \n",
        "    def predict(self, test_X):\n",
        "        # Insert constant ones for bias weights.\n",
        "        test_X = np.insert(test_X, 0, 1, axis=1)\n",
        "        y_pred = self.hypothesis(self.w, test_X)\n",
        "        return y_pred"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:34:15.969955Z",
          "iopub.execute_input": "2023-04-30T08:34:15.970344Z",
          "iopub.status.idle": "2023-04-30T08:34:15.982914Z",
          "shell.execute_reply.started": "2023-04-30T08:34:15.970307Z",
          "shell.execute_reply": "2023-04-30T08:34:15.981717Z"
        },
        "trusted": true,
        "id": "G-YfrGzAU_PU"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization Classes (20 points)"
      ],
      "metadata": {
        "id": "HE6pCA4MU_PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to implement l2/l1 regularization."
      ],
      "metadata": {
        "id": "101Ics8WU_PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class l1_regularization:\n",
        "    \"\"\"Regularization used for Lasso Regression\"\"\"\n",
        "    def __init__(self, lamda):\n",
        "        self.lamda = lamda\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        \"This will be returned when we call this class.\"\n",
        "        return self.lamda * np.sum(np.abs(weights))\n",
        "\n",
        "    def derivation(self, weights):\n",
        "        \"Derivation of the regulariozation function.\"\n",
        "        return self.lamda * np.sign(weights)\n",
        "\n",
        "\n",
        "class l2_regularization:\n",
        "    \"\"\"Regularization used for Ridge Regression\"\"\"\n",
        "    def __init__(self, lamda):\n",
        "        self.lamda = lamda\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        \"This will be retuned when we call this class.\"\n",
        "        return self.lamda * np.sum(np.square(weights))\n",
        "\n",
        "    def derivation(self, weights):\n",
        "        \"Derivation of the regulariozation function.\"\n",
        "        return 2 * self.lamda * weights\n"
      ],
      "metadata": {
        "id": "_93Zue5Ry2qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso Regression from scratch (5 points)"
      ],
      "metadata": {
        "id": "MYUC1u_FU_PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a lasso regression model using your own code and the following class."
      ],
      "metadata": {
        "id": "UxjF4g7rU_PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LassoRegression(Regression):\n",
        "    def __init__(self, lamda, learning_rate, iteration):\n",
        "        self.regularization = l1_regularization(lamda)\n",
        "        super(LassoRegression, self).__init__(learning_rate, iteration, self.regularization)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:34:23.921661Z",
          "iopub.execute_input": "2023-04-30T08:34:23.922713Z",
          "iopub.status.idle": "2023-04-30T08:34:23.928316Z",
          "shell.execute_reply.started": "2023-04-30T08:34:23.922672Z",
          "shell.execute_reply": "2023-04-30T08:34:23.927223Z"
        },
        "trusted": true,
        "id": "Zl180mzhU_PU"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_lasso = LassoRegression(lamda=0.1, learning_rate=0.001, iteration=10000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:34:43.634507Z",
          "iopub.execute_input": "2023-04-30T08:34:43.634909Z",
          "iopub.status.idle": "2023-04-30T08:34:43.669074Z",
          "shell.execute_reply.started": "2023-04-30T08:34:43.634873Z",
          "shell.execute_reply": "2023-04-30T08:34:43.667346Z"
        },
        "trusted": true,
        "id": "Vm0z4ECLU_PV"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_lasso.train(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa7O1llCzt1I",
        "outputId": "1c0e22ea-ff55-4890-946c-6a1b803ec5df"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss function for the iteration 10----->2379.923743741642 :)\n",
            "The loss function for the iteration 20----->2330.3429234902637 :)\n",
            "The loss function for the iteration 30----->2281.972446438536 :)\n",
            "The loss function for the iteration 40----->2234.7645326869806 :)\n",
            "The loss function for the iteration 50----->2188.6752273802254 :)\n",
            "The loss function for the iteration 60----->2143.6639160231125 :)\n",
            "The loss function for the iteration 70----->2099.6929637062344 :)\n",
            "The loss function for the iteration 80----->2056.7274258703947 :)\n",
            "The loss function for the iteration 90----->2014.73490010503 :)\n",
            "The loss function for the iteration 100----->1973.6848857793425 :)\n",
            "The loss function for the iteration 110----->1933.5489423433257 :)\n",
            "The loss function for the iteration 120----->1894.3000243389074 :)\n",
            "The loss function for the iteration 130----->1855.9127925636847 :)\n",
            "The loss function for the iteration 140----->1818.3631136738397 :)\n",
            "The loss function for the iteration 150----->1781.6281292676342 :)\n",
            "The loss function for the iteration 160----->1745.686122767039 :)\n",
            "The loss function for the iteration 170----->1710.5163753272209 :)\n",
            "The loss function for the iteration 180----->1676.0990697803836 :)\n",
            "The loss function for the iteration 190----->1642.4153570611131 :)\n",
            "The loss function for the iteration 200----->1609.4470236182085 :)\n",
            "The loss function for the iteration 210----->1577.17656474802 :)\n",
            "The loss function for the iteration 220----->1545.5871649905484 :)\n",
            "The loss function for the iteration 230----->1514.6627156969994 :)\n",
            "The loss function for the iteration 240----->1484.3877188912065 :)\n",
            "The loss function for the iteration 250----->1454.746856527726 :)\n",
            "The loss function for the iteration 260----->1425.7254971312052 :)\n",
            "The loss function for the iteration 270----->1397.3094648845308 :)\n",
            "The loss function for the iteration 280----->1369.4850235564684 :)\n",
            "The loss function for the iteration 290----->1342.2388319779081 :)\n",
            "The loss function for the iteration 300----->1315.5579344766077 :)\n",
            "The loss function for the iteration 310----->1289.4297821087798 :)\n",
            "The loss function for the iteration 320----->1263.8421046604049 :)\n",
            "The loss function for the iteration 330----->1238.7830569025332 :)\n",
            "The loss function for the iteration 340----->1214.2410926659863 :)\n",
            "The loss function for the iteration 350----->1190.204850277399 :)\n",
            "The loss function for the iteration 360----->1166.6633558832407 :)\n",
            "The loss function for the iteration 370----->1143.6058822932557 :)\n",
            "The loss function for the iteration 380----->1121.0219935092398 :)\n",
            "The loss function for the iteration 390----->1098.9014861214425 :)\n",
            "The loss function for the iteration 400----->1077.234425586027 :)\n",
            "The loss function for the iteration 410----->1056.0112500493149 :)\n",
            "The loss function for the iteration 420----->1035.2224495015703 :)\n",
            "The loss function for the iteration 430----->1014.8588226717882 :)\n",
            "The loss function for the iteration 440----->994.9112568063604 :)\n",
            "The loss function for the iteration 450----->975.3708817814208 :)\n",
            "The loss function for the iteration 460----->956.2291175053448 :)\n",
            "The loss function for the iteration 470----->937.4775125330596 :)\n",
            "The loss function for the iteration 480----->919.1078575126988 :)\n",
            "The loss function for the iteration 490----->901.1126720367605 :)\n",
            "The loss function for the iteration 500----->883.4839623207046 :)\n",
            "The loss function for the iteration 510----->866.2135843602587 :)\n",
            "The loss function for the iteration 520----->849.2940597856555 :)\n",
            "The loss function for the iteration 530----->832.7180218337787 :)\n",
            "The loss function for the iteration 540----->816.4782642434996 :)\n",
            "The loss function for the iteration 550----->800.5677804625179 :)\n",
            "The loss function for the iteration 560----->784.9796804944962 :)\n",
            "The loss function for the iteration 570----->769.7072575375721 :)\n",
            "The loss function for the iteration 580----->754.7439231901093 :)\n",
            "The loss function for the iteration 590----->740.0832802562331 :)\n",
            "The loss function for the iteration 600----->725.718981109411 :)\n",
            "The loss function for the iteration 610----->711.6448892600524 :)\n",
            "The loss function for the iteration 620----->697.8549616214634 :)\n",
            "The loss function for the iteration 630----->684.3433245204403 :)\n",
            "The loss function for the iteration 640----->671.1041763838407 :)\n",
            "The loss function for the iteration 650----->658.1319104122148 :)\n",
            "The loss function for the iteration 660----->645.4209634810339 :)\n",
            "The loss function for the iteration 670----->632.9661041346325 :)\n",
            "The loss function for the iteration 680----->620.7628145183121 :)\n",
            "The loss function for the iteration 690----->608.805337087204 :)\n",
            "The loss function for the iteration 700----->597.0884625004795 :)\n",
            "The loss function for the iteration 710----->585.6070759417578 :)\n",
            "The loss function for the iteration 720----->574.3565372709056 :)\n",
            "The loss function for the iteration 730----->563.3318968927873 :)\n",
            "The loss function for the iteration 740----->552.5284885220885 :)\n",
            "The loss function for the iteration 750----->541.9417786680409 :)\n",
            "The loss function for the iteration 760----->531.5672996827367 :)\n",
            "The loss function for the iteration 770----->521.4007033904242 :)\n",
            "The loss function for the iteration 780----->511.437696533705 :)\n",
            "The loss function for the iteration 790----->501.6742142990569 :)\n",
            "The loss function for the iteration 800----->492.1063425964544 :)\n",
            "The loss function for the iteration 810----->482.72983811167575 :)\n",
            "The loss function for the iteration 820----->473.5407496426776 :)\n",
            "The loss function for the iteration 830----->464.5353051482689 :)\n",
            "The loss function for the iteration 840----->455.7097547765743 :)\n",
            "The loss function for the iteration 850----->447.0603605189568 :)\n",
            "The loss function for the iteration 860----->438.5838329477114 :)\n",
            "The loss function for the iteration 870----->430.2764722103116 :)\n",
            "The loss function for the iteration 880----->422.134833810758 :)\n",
            "The loss function for the iteration 890----->414.1555574636313 :)\n",
            "The loss function for the iteration 900----->406.3353159691921 :)\n",
            "The loss function for the iteration 910----->398.6709001673913 :)\n",
            "The loss function for the iteration 920----->391.1590582378211 :)\n",
            "The loss function for the iteration 930----->383.79663457923834 :)\n",
            "The loss function for the iteration 940----->376.5806228195454 :)\n",
            "The loss function for the iteration 950----->369.5080410319232 :)\n",
            "The loss function for the iteration 960----->362.575961166959 :)\n",
            "The loss function for the iteration 970----->355.7815269549199 :)\n",
            "The loss function for the iteration 980----->349.12198049230324 :)\n",
            "The loss function for the iteration 990----->342.5945881091001 :)\n",
            "The loss function for the iteration 1000----->336.1966279009733 :)\n",
            "The loss function for the iteration 1010----->329.92548096113916 :)\n",
            "The loss function for the iteration 1020----->323.77859506095234 :)\n",
            "The loss function for the iteration 1030----->317.7534129424139 :)\n",
            "The loss function for the iteration 1040----->311.84747471279155 :)\n",
            "The loss function for the iteration 1050----->306.0584467940766 :)\n",
            "The loss function for the iteration 1060----->300.383927535847 :)\n",
            "The loss function for the iteration 1070----->294.82157497539987 :)\n",
            "The loss function for the iteration 1080----->289.36924236089715 :)\n",
            "The loss function for the iteration 1090----->284.0246834894002 :)\n",
            "The loss function for the iteration 1100----->278.7856548456804 :)\n",
            "The loss function for the iteration 1110----->273.6500303499956 :)\n",
            "The loss function for the iteration 1120----->268.61572992628584 :)\n",
            "The loss function for the iteration 1130----->263.6807087728053 :)\n",
            "The loss function for the iteration 1140----->258.8429825600081 :)\n",
            "The loss function for the iteration 1150----->254.1005357148449 :)\n",
            "The loss function for the iteration 1160----->249.45146269548337 :)\n",
            "The loss function for the iteration 1170----->244.89390670626943 :)\n",
            "The loss function for the iteration 1180----->240.42645907279564 :)\n",
            "The loss function for the iteration 1190----->236.04694001965348 :)\n",
            "The loss function for the iteration 1200----->231.75352925548378 :)\n",
            "The loss function for the iteration 1210----->227.5444964718969 :)\n",
            "The loss function for the iteration 1220----->223.41811105428755 :)\n",
            "The loss function for the iteration 1230----->219.37274403846996 :)\n",
            "The loss function for the iteration 1240----->215.40679760980981 :)\n",
            "The loss function for the iteration 1250----->211.51868905659023 :)\n",
            "The loss function for the iteration 1260----->207.706857229471 :)\n",
            "The loss function for the iteration 1270----->203.96973945438862 :)\n",
            "The loss function for the iteration 1280----->200.30580397019148 :)\n",
            "The loss function for the iteration 1290----->196.71363025754428 :)\n",
            "The loss function for the iteration 1300----->193.1917588013153 :)\n",
            "The loss function for the iteration 1310----->189.73879911651053 :)\n",
            "The loss function for the iteration 1320----->186.35336661163927 :)\n",
            "The loss function for the iteration 1330----->183.03407376163568 :)\n",
            "The loss function for the iteration 1340----->179.77964245091374 :)\n",
            "The loss function for the iteration 1350----->176.5887628902269 :)\n",
            "The loss function for the iteration 1360----->173.46017003841553 :)\n",
            "The loss function for the iteration 1370----->170.39259896450432 :)\n",
            "The loss function for the iteration 1380----->167.38483966144997 :)\n",
            "The loss function for the iteration 1390----->164.43587433786843 :)\n",
            "The loss function for the iteration 1400----->161.54436027779212 :)\n",
            "The loss function for the iteration 1410----->158.70916045677177 :)\n",
            "The loss function for the iteration 1420----->155.92911695738593 :)\n",
            "The loss function for the iteration 1430----->153.20318621218965 :)\n",
            "The loss function for the iteration 1440----->150.5302379183193 :)\n",
            "The loss function for the iteration 1450----->147.90925475537358 :)\n",
            "The loss function for the iteration 1460----->145.3391922090788 :)\n",
            "The loss function for the iteration 1470----->142.81902407313635 :)\n",
            "The loss function for the iteration 1480----->140.34777899470592 :)\n",
            "The loss function for the iteration 1490----->137.9244621925472 :)\n",
            "The loss function for the iteration 1500----->135.54813215878312 :)\n",
            "The loss function for the iteration 1510----->133.21785767357284 :)\n",
            "The loss function for the iteration 1520----->130.9327200280869 :)\n",
            "The loss function for the iteration 1530----->128.69183893807647 :)\n",
            "The loss function for the iteration 1540----->126.49430623650645 :)\n",
            "The loss function for the iteration 1550----->124.33928610132396 :)\n",
            "The loss function for the iteration 1560----->122.22594047124846 :)\n",
            "The loss function for the iteration 1570----->120.15340859170931 :)\n",
            "The loss function for the iteration 1580----->118.12092089004189 :)\n",
            "The loss function for the iteration 1590----->116.12768822604438 :)\n",
            "The loss function for the iteration 1600----->114.17288543637551 :)\n",
            "The loss function for the iteration 1610----->112.2558010806595 :)\n",
            "The loss function for the iteration 1620----->110.37581300214212 :)\n",
            "The loss function for the iteration 1630----->108.53219234352143 :)\n",
            "The loss function for the iteration 1640----->106.72409632567192 :)\n",
            "The loss function for the iteration 1650----->104.95079784196159 :)\n",
            "The loss function for the iteration 1660----->103.21163936212498 :)\n",
            "The loss function for the iteration 1670----->101.50600396611217 :)\n",
            "The loss function for the iteration 1680----->99.83316745171746 :)\n",
            "The loss function for the iteration 1690----->98.1924881787215 :)\n",
            "The loss function for the iteration 1700----->96.58332818597285 :)\n",
            "The loss function for the iteration 1710----->95.0050395904087 :)\n",
            "The loss function for the iteration 1720----->93.45717558781946 :)\n",
            "The loss function for the iteration 1730----->91.9394447808331 :)\n",
            "The loss function for the iteration 1740----->90.45081835226142 :)\n",
            "The loss function for the iteration 1750----->88.99074724536386 :)\n",
            "The loss function for the iteration 1760----->87.55862720029039 :)\n",
            "The loss function for the iteration 1770----->86.153958146599 :)\n",
            "The loss function for the iteration 1780----->84.77638043521054 :)\n",
            "The loss function for the iteration 1790----->83.42514486568483 :)\n",
            "The loss function for the iteration 1800----->82.09975119312553 :)\n",
            "The loss function for the iteration 1810----->80.79969755479596 :)\n",
            "The loss function for the iteration 1820----->79.5244820280669 :)\n",
            "The loss function for the iteration 1830----->78.27363510752791 :)\n",
            "The loss function for the iteration 1840----->77.04664468064003 :)\n",
            "The loss function for the iteration 1850----->75.84307785334855 :)\n",
            "The loss function for the iteration 1860----->74.66247631685214 :)\n",
            "The loss function for the iteration 1870----->73.504342235882 :)\n",
            "The loss function for the iteration 1880----->72.36826931643759 :)\n",
            "The loss function for the iteration 1890----->71.25384684690842 :)\n",
            "The loss function for the iteration 1900----->70.16060683724832 :)\n",
            "The loss function for the iteration 1910----->69.08815773574358 :)\n",
            "The loss function for the iteration 1920----->68.03610922624686 :)\n",
            "The loss function for the iteration 1930----->67.00407223826889 :)\n",
            "The loss function for the iteration 1940----->65.99182028447152 :)\n",
            "The loss function for the iteration 1950----->64.99876161901622 :)\n",
            "The loss function for the iteration 1960----->64.02456963694742 :)\n",
            "The loss function for the iteration 1970----->63.06882850609838 :)\n",
            "The loss function for the iteration 1980----->62.1312057597439 :)\n",
            "The loss function for the iteration 1990----->61.21135321322 :)\n",
            "The loss function for the iteration 2000----->60.308912095386574 :)\n",
            "The loss function for the iteration 2010----->59.42354402111138 :)\n",
            "The loss function for the iteration 2020----->58.55494091597332 :)\n",
            "The loss function for the iteration 2030----->57.702742499471995 :)\n",
            "The loss function for the iteration 2040----->56.86664936289759 :)\n",
            "The loss function for the iteration 2050----->56.046352097411344 :)\n",
            "The loss function for the iteration 2060----->55.24153396884645 :)\n",
            "The loss function for the iteration 2070----->54.451868028221924 :)\n",
            "The loss function for the iteration 2080----->53.677139293839076 :)\n",
            "The loss function for the iteration 2090----->52.91698716850428 :)\n",
            "The loss function for the iteration 2100----->52.17114404496932 :)\n",
            "The loss function for the iteration 2110----->51.43933317071681 :)\n",
            "The loss function for the iteration 2120----->50.721363110424576 :)\n",
            "The loss function for the iteration 2130----->50.017080276050535 :)\n",
            "The loss function for the iteration 2140----->49.32606103694332 :)\n",
            "The loss function for the iteration 2150----->48.64800225714109 :)\n",
            "The loss function for the iteration 2160----->47.98271774505021 :)\n",
            "The loss function for the iteration 2170----->47.32987648091972 :)\n",
            "The loss function for the iteration 2180----->46.68931056126758 :)\n",
            "The loss function for the iteration 2190----->46.06074396945332 :)\n",
            "The loss function for the iteration 2200----->45.44398259722169 :)\n",
            "The loss function for the iteration 2210----->44.838834551348924 :)\n",
            "The loss function for the iteration 2220----->44.245125644045125 :)\n",
            "The loss function for the iteration 2230----->43.662775245452046 :)\n",
            "The loss function for the iteration 2240----->43.091354396929525 :)\n",
            "The loss function for the iteration 2250----->42.530588137112126 :)\n",
            "The loss function for the iteration 2260----->41.98034296343061 :)\n",
            "The loss function for the iteration 2270----->41.44035310607012 :)\n",
            "The loss function for the iteration 2280----->40.910498340245 :)\n",
            "The loss function for the iteration 2290----->40.39079665385533 :)\n",
            "The loss function for the iteration 2300----->39.88079552777286 :)\n",
            "The loss function for the iteration 2310----->39.38030187748029 :)\n",
            "The loss function for the iteration 2320----->38.88918806899555 :)\n",
            "The loss function for the iteration 2330----->38.40717245093005 :)\n",
            "The loss function for the iteration 2340----->37.934163447376676 :)\n",
            "The loss function for the iteration 2350----->37.4699373096758 :)\n",
            "The loss function for the iteration 2360----->37.01437265544002 :)\n",
            "The loss function for the iteration 2370----->36.56727927900718 :)\n",
            "The loss function for the iteration 2380----->36.12846704882406 :)\n",
            "The loss function for the iteration 2390----->35.69782675835997 :)\n",
            "The loss function for the iteration 2400----->35.27531477035201 :)\n",
            "The loss function for the iteration 2410----->34.86066249835449 :)\n",
            "The loss function for the iteration 2420----->34.45376736181616 :)\n",
            "The loss function for the iteration 2430----->34.0543549237226 :)\n",
            "The loss function for the iteration 2440----->33.66240024840527 :)\n",
            "The loss function for the iteration 2450----->33.27770511790389 :)\n",
            "The loss function for the iteration 2460----->32.900188882755984 :)\n",
            "The loss function for the iteration 2470----->32.52970455022111 :)\n",
            "The loss function for the iteration 2480----->32.16604549569283 :)\n",
            "The loss function for the iteration 2490----->31.809091684355053 :)\n",
            "The loss function for the iteration 2500----->31.458753581411017 :)\n",
            "The loss function for the iteration 2510----->31.114887842942444 :)\n",
            "The loss function for the iteration 2520----->30.77735559157121 :)\n",
            "The loss function for the iteration 2530----->30.44602544669285 :)\n",
            "The loss function for the iteration 2540----->30.120806936596555 :)\n",
            "The loss function for the iteration 2550----->29.801593060499393 :)\n",
            "The loss function for the iteration 2560----->29.488201099540007 :)\n",
            "The loss function for the iteration 2570----->29.180623006400076 :)\n",
            "The loss function for the iteration 2580----->28.87866267576517 :)\n",
            "The loss function for the iteration 2590----->28.58226766603078 :)\n",
            "The loss function for the iteration 2600----->28.291288323111615 :)\n",
            "The loss function for the iteration 2610----->28.005682256780428 :)\n",
            "The loss function for the iteration 2620----->27.72530829919521 :)\n",
            "The loss function for the iteration 2630----->27.45001461970948 :)\n",
            "The loss function for the iteration 2640----->27.179879848598002 :)\n",
            "The loss function for the iteration 2650----->26.91468511597772 :)\n",
            "The loss function for the iteration 2660----->26.654308559704916 :)\n",
            "The loss function for the iteration 2670----->26.39875031423329 :)\n",
            "The loss function for the iteration 2680----->26.147775940973748 :)\n",
            "The loss function for the iteration 2690----->25.901462326993148 :)\n",
            "The loss function for the iteration 2700----->25.659577750565916 :)\n",
            "The loss function for the iteration 2710----->25.422138351424838 :)\n",
            "The loss function for the iteration 2720----->25.189078168761824 :)\n",
            "The loss function for the iteration 2730----->24.960356752208 :)\n",
            "The loss function for the iteration 2740----->24.735755682336304 :)\n",
            "The loss function for the iteration 2750----->24.515276826014734 :)\n",
            "The loss function for the iteration 2760----->24.29877893103609 :)\n",
            "The loss function for the iteration 2770----->24.08622726624396 :)\n",
            "The loss function for the iteration 2780----->23.8775522103432 :)\n",
            "The loss function for the iteration 2790----->23.672628474098516 :)\n",
            "The loss function for the iteration 2800----->23.47144325531098 :)\n",
            "The loss function for the iteration 2810----->23.273911579772303 :)\n",
            "The loss function for the iteration 2820----->23.080052794581672 :)\n",
            "The loss function for the iteration 2830----->22.88971812537614 :)\n",
            "The loss function for the iteration 2840----->22.70282086519343 :)\n",
            "The loss function for the iteration 2850----->22.519280981783577 :)\n",
            "The loss function for the iteration 2860----->22.33909482357233 :)\n",
            "The loss function for the iteration 2870----->22.16212809306195 :)\n",
            "The loss function for the iteration 2880----->21.988355295492216 :)\n",
            "The loss function for the iteration 2890----->21.817768411249972 :)\n",
            "The loss function for the iteration 2900----->21.6503187481309 :)\n",
            "The loss function for the iteration 2910----->21.485907679487845 :)\n",
            "The loss function for the iteration 2920----->21.32454590225394 :)\n",
            "The loss function for the iteration 2930----->21.166120263411806 :)\n",
            "The loss function for the iteration 2940----->21.010495636638954 :)\n",
            "The loss function for the iteration 2950----->20.857673481351704 :)\n",
            "The loss function for the iteration 2960----->20.70768879494526 :)\n",
            "The loss function for the iteration 2970----->20.560446733449695 :)\n",
            "The loss function for the iteration 2980----->20.41590206456234 :)\n",
            "The loss function for the iteration 2990----->20.27389741978893 :)\n",
            "The loss function for the iteration 3000----->20.134583211096775 :)\n",
            "The loss function for the iteration 3010----->19.99774919734147 :)\n",
            "The loss function for the iteration 3020----->19.863472030401432 :)\n",
            "The loss function for the iteration 3030----->19.73181904801547 :)\n",
            "The loss function for the iteration 3040----->19.602513157305275 :)\n",
            "The loss function for the iteration 3050----->19.475534707210894 :)\n",
            "The loss function for the iteration 3060----->19.35081627725564 :)\n",
            "The loss function for the iteration 3070----->19.228578996081477 :)\n",
            "The loss function for the iteration 3080----->19.10884268455328 :)\n",
            "The loss function for the iteration 3090----->18.99125977942736 :)\n",
            "The loss function for the iteration 3100----->18.875797924158938 :)\n",
            "The loss function for the iteration 3110----->18.762462173237697 :)\n",
            "The loss function for the iteration 3120----->18.65113740532051 :)\n",
            "The loss function for the iteration 3130----->18.54184732247054 :)\n",
            "The loss function for the iteration 3140----->18.434487037312394 :)\n",
            "The loss function for the iteration 3150----->18.329032700290263 :)\n",
            "The loss function for the iteration 3160----->18.22546738265966 :)\n",
            "The loss function for the iteration 3170----->18.123756630308506 :)\n",
            "The loss function for the iteration 3180----->18.02386326043647 :)\n",
            "The loss function for the iteration 3190----->17.925743209894836 :)\n",
            "The loss function for the iteration 3200----->17.82937513157305 :)\n",
            "The loss function for the iteration 3210----->17.73470724208938 :)\n",
            "The loss function for the iteration 3220----->17.641746228915867 :)\n",
            "The loss function for the iteration 3230----->17.550458646461607 :)\n",
            "The loss function for the iteration 3240----->17.460781648459392 :)\n",
            "The loss function for the iteration 3250----->17.372689793492366 :)\n",
            "The loss function for the iteration 3260----->17.286163926844928 :)\n",
            "The loss function for the iteration 3270----->17.20114717529434 :)\n",
            "The loss function for the iteration 3280----->17.11767340378323 :)\n",
            "The loss function for the iteration 3290----->17.035678265603377 :)\n",
            "The loss function for the iteration 3300----->16.95511753737567 :)\n",
            "The loss function for the iteration 3310----->16.875951689468458 :)\n",
            "The loss function for the iteration 3320----->16.79823645692825 :)\n",
            "The loss function for the iteration 3330----->16.72184271269025 :)\n",
            "The loss function for the iteration 3340----->16.646807609300552 :)\n",
            "The loss function for the iteration 3350----->16.573107767383092 :)\n",
            "The loss function for the iteration 3360----->16.500667171602515 :)\n",
            "The loss function for the iteration 3370----->16.42953282398117 :)\n",
            "The loss function for the iteration 3380----->16.359621664452078 :)\n",
            "The loss function for the iteration 3390----->16.290937329842432 :)\n",
            "The loss function for the iteration 3400----->16.223479913474502 :)\n",
            "The loss function for the iteration 3410----->16.157184085370538 :)\n",
            "The loss function for the iteration 3420----->16.092030187101244 :)\n",
            "The loss function for the iteration 3430----->16.028042644209954 :)\n",
            "The loss function for the iteration 3440----->15.96516028538942 :)\n",
            "The loss function for the iteration 3450----->15.903362058816551 :)\n",
            "The loss function for the iteration 3460----->15.84263951857663 :)\n",
            "The loss function for the iteration 3470----->15.783011401216282 :)\n",
            "The loss function for the iteration 3480----->15.724384031849434 :)\n",
            "The loss function for the iteration 3490----->15.66675815041341 :)\n",
            "The loss function for the iteration 3500----->15.610181560752617 :)\n",
            "The loss function for the iteration 3510----->15.55457385408284 :)\n",
            "The loss function for the iteration 3520----->15.499892730045257 :)\n",
            "The loss function for the iteration 3530----->15.446176611701475 :)\n",
            "The loss function for the iteration 3540----->15.393370999037563 :)\n",
            "The loss function for the iteration 3550----->15.341512610982406 :)\n",
            "The loss function for the iteration 3560----->15.290507842777648 :)\n",
            "The loss function for the iteration 3570----->15.240422643912748 :)\n",
            "The loss function for the iteration 3580----->15.19117036338378 :)\n",
            "The loss function for the iteration 3590----->15.142883564690884 :)\n",
            "The loss function for the iteration 3600----->15.096073125337288 :)\n",
            "The loss function for the iteration 3610----->15.050063233443725 :)\n",
            "The loss function for the iteration 3620----->15.004881270159121 :)\n",
            "The loss function for the iteration 3630----->14.960432013478306 :)\n",
            "The loss function for the iteration 3640----->14.916781991283298 :)\n",
            "The loss function for the iteration 3650----->14.87386872432731 :)\n",
            "The loss function for the iteration 3660----->14.83171684381334 :)\n",
            "The loss function for the iteration 3670----->14.790311632308503 :)\n",
            "The loss function for the iteration 3680----->14.74962710580731 :)\n",
            "The loss function for the iteration 3690----->14.709652009776034 :)\n",
            "The loss function for the iteration 3700----->14.670316983648721 :)\n",
            "The loss function for the iteration 3710----->14.631710406032987 :)\n",
            "The loss function for the iteration 3720----->14.593752324967245 :)\n",
            "The loss function for the iteration 3730----->14.556418720377286 :)\n",
            "The loss function for the iteration 3740----->14.519756025105686 :)\n",
            "The loss function for the iteration 3750----->14.483709899812677 :)\n",
            "The loss function for the iteration 3760----->14.44839968387668 :)\n",
            "The loss function for the iteration 3770----->14.41373980703769 :)\n",
            "The loss function for the iteration 3780----->14.379656837329694 :)\n",
            "The loss function for the iteration 3790----->14.346173087663573 :)\n",
            "The loss function for the iteration 3800----->14.313190410235004 :)\n",
            "The loss function for the iteration 3810----->14.28085129117692 :)\n",
            "The loss function for the iteration 3820----->14.249002343072373 :)\n",
            "The loss function for the iteration 3830----->14.217739636138575 :)\n",
            "The loss function for the iteration 3840----->14.187008732485449 :)\n",
            "The loss function for the iteration 3850----->14.15679014478506 :)\n",
            "The loss function for the iteration 3860----->14.127075163682147 :)\n",
            "The loss function for the iteration 3870----->14.09790029146074 :)\n",
            "The loss function for the iteration 3880----->14.069211006883139 :)\n",
            "The loss function for the iteration 3890----->14.041000745489672 :)\n",
            "The loss function for the iteration 3900----->14.01327998188652 :)\n",
            "The loss function for the iteration 3910----->13.986009622252787 :)\n",
            "The loss function for the iteration 3920----->13.959200908307468 :)\n",
            "The loss function for the iteration 3930----->13.932809949097267 :)\n",
            "The loss function for the iteration 3940----->13.906958395316414 :)\n",
            "The loss function for the iteration 3950----->13.881580348272582 :)\n",
            "The loss function for the iteration 3960----->13.857095794299424 :)\n",
            "The loss function for the iteration 3970----->13.833009804352873 :)\n",
            "The loss function for the iteration 3980----->13.809320444145047 :)\n",
            "The loss function for the iteration 3990----->13.786065989418116 :)\n",
            "The loss function for the iteration 4000----->13.763177081095357 :)\n",
            "The loss function for the iteration 4010----->13.740688492725209 :)\n",
            "The loss function for the iteration 4020----->13.718580943146268 :)\n",
            "The loss function for the iteration 4030----->13.696842168743466 :)\n",
            "The loss function for the iteration 4040----->13.675468240456745 :)\n",
            "The loss function for the iteration 4050----->13.654456907235062 :)\n",
            "The loss function for the iteration 4060----->13.633773571298981 :)\n",
            "The loss function for the iteration 4070----->13.613478046463364 :)\n",
            "The loss function for the iteration 4080----->13.593497759065345 :)\n",
            "The loss function for the iteration 4090----->13.573861840786405 :)\n",
            "The loss function for the iteration 4100----->13.554566583895294 :)\n",
            "The loss function for the iteration 4110----->13.535693332192846 :)\n",
            "The loss function for the iteration 4120----->13.517102101997853 :)\n",
            "The loss function for the iteration 4130----->13.49886506194964 :)\n",
            "The loss function for the iteration 4140----->13.480905926097934 :)\n",
            "The loss function for the iteration 4150----->13.46325944152439 :)\n",
            "The loss function for the iteration 4160----->13.445887742706532 :)\n",
            "The loss function for the iteration 4170----->13.428815374909062 :)\n",
            "The loss function for the iteration 4180----->13.412038964628993 :)\n",
            "The loss function for the iteration 4190----->13.395513208428468 :)\n",
            "The loss function for the iteration 4200----->13.379294121262308 :)\n",
            "The loss function for the iteration 4210----->13.36329516115357 :)\n",
            "The loss function for the iteration 4220----->13.347588089302981 :)\n",
            "The loss function for the iteration 4230----->13.332173033676584 :)\n",
            "The loss function for the iteration 4240----->13.316972348149134 :)\n",
            "The loss function for the iteration 4250----->13.302014151592326 :)\n",
            "The loss function for the iteration 4260----->13.287281595858822 :)\n",
            "The loss function for the iteration 4270----->13.272846755444803 :)\n",
            "The loss function for the iteration 4280----->13.258644166600469 :)\n",
            "The loss function for the iteration 4290----->13.244715353682118 :)\n",
            "The loss function for the iteration 4300----->13.231033339915024 :)\n",
            "The loss function for the iteration 4310----->13.217566843594813 :)\n",
            "The loss function for the iteration 4320----->13.204334952729052 :)\n",
            "The loss function for the iteration 4330----->13.19128018291004 :)\n",
            "The loss function for the iteration 4340----->13.178509825598514 :)\n",
            "The loss function for the iteration 4350----->13.165908590610506 :)\n",
            "The loss function for the iteration 4360----->13.153500129883142 :)\n",
            "The loss function for the iteration 4370----->13.141318222086223 :)\n",
            "The loss function for the iteration 4380----->13.129316112175497 :)\n",
            "The loss function for the iteration 4390----->13.117500167733583 :)\n",
            "The loss function for the iteration 4400----->13.105921971719908 :)\n",
            "The loss function for the iteration 4410----->13.094513642058262 :)\n",
            "The loss function for the iteration 4420----->13.083246898744996 :)\n",
            "The loss function for the iteration 4430----->13.07221791246309 :)\n",
            "The loss function for the iteration 4440----->13.061307369695031 :)\n",
            "The loss function for the iteration 4450----->13.050630276305409 :)\n",
            "The loss function for the iteration 4460----->13.040087946135705 :)\n",
            "The loss function for the iteration 4470----->13.029696870856014 :)\n",
            "The loss function for the iteration 4480----->13.01950456611252 :)\n",
            "The loss function for the iteration 4490----->13.00949322702634 :)\n",
            "The loss function for the iteration 4500----->12.99961274703875 :)\n",
            "The loss function for the iteration 4510----->12.989879264694473 :)\n",
            "The loss function for the iteration 4520----->12.980307802987294 :)\n",
            "The loss function for the iteration 4530----->12.970918643741845 :)\n",
            "The loss function for the iteration 4540----->12.961633669518214 :)\n",
            "The loss function for the iteration 4550----->12.95250736428586 :)\n",
            "The loss function for the iteration 4560----->12.943552687386344 :)\n",
            "The loss function for the iteration 4570----->12.934693967573127 :)\n",
            "The loss function for the iteration 4580----->12.925965485450702 :)\n",
            "The loss function for the iteration 4590----->12.917435013481645 :)\n",
            "The loss function for the iteration 4600----->12.909005257623834 :)\n",
            "The loss function for the iteration 4610----->12.900696263884708 :)\n",
            "The loss function for the iteration 4620----->12.892564078816623 :)\n",
            "The loss function for the iteration 4630----->12.88449251138458 :)\n",
            "The loss function for the iteration 4640----->12.87658023449185 :)\n",
            "The loss function for the iteration 4650----->12.868774741430686 :)\n",
            "The loss function for the iteration 4660----->12.861100972246172 :)\n",
            "The loss function for the iteration 4670----->12.853518495071254 :)\n",
            "The loss function for the iteration 4680----->12.846102079929913 :)\n",
            "The loss function for the iteration 4690----->12.838797948226205 :)\n",
            "The loss function for the iteration 4700----->12.831559998917212 :)\n",
            "The loss function for the iteration 4710----->12.82445210654093 :)\n",
            "The loss function for the iteration 4720----->12.817510428595044 :)\n",
            "The loss function for the iteration 4730----->12.810693218315723 :)\n",
            "The loss function for the iteration 4740----->12.803966262871644 :)\n",
            "The loss function for the iteration 4750----->12.797352693391051 :)\n",
            "The loss function for the iteration 4760----->12.790876058489404 :)\n",
            "The loss function for the iteration 4770----->12.784463790222718 :)\n",
            "The loss function for the iteration 4780----->12.778143079580879 :)\n",
            "The loss function for the iteration 4790----->12.771938001394751 :)\n",
            "The loss function for the iteration 4800----->12.765809677918673 :)\n",
            "The loss function for the iteration 4810----->12.759798298577028 :)\n",
            "The loss function for the iteration 4820----->12.753868455118067 :)\n",
            "The loss function for the iteration 4830----->12.748033856451535 :)\n",
            "The loss function for the iteration 4840----->12.74227287402813 :)\n",
            "The loss function for the iteration 4850----->12.736603594646493 :)\n",
            "The loss function for the iteration 4860----->12.731042795265669 :)\n",
            "The loss function for the iteration 4870----->12.72554511971223 :)\n",
            "The loss function for the iteration 4880----->12.720125927363533 :)\n",
            "The loss function for the iteration 4890----->12.714832025423458 :)\n",
            "The loss function for the iteration 4900----->12.709559052381167 :)\n",
            "The loss function for the iteration 4910----->12.70436975270736 :)\n",
            "The loss function for the iteration 4920----->12.699274729888703 :)\n",
            "The loss function for the iteration 4930----->12.694276093733283 :)\n",
            "The loss function for the iteration 4940----->12.689345037938434 :)\n",
            "The loss function for the iteration 4950----->12.684463434240403 :)\n",
            "The loss function for the iteration 4960----->12.679664052560652 :)\n",
            "The loss function for the iteration 4970----->12.674940558293578 :)\n",
            "The loss function for the iteration 4980----->12.670303792221704 :)\n",
            "The loss function for the iteration 4990----->12.665692104536259 :)\n",
            "The loss function for the iteration 5000----->12.661178238695152 :)\n",
            "The loss function for the iteration 5010----->12.656726419178815 :)\n",
            "The loss function for the iteration 5020----->12.652326004797427 :)\n",
            "The loss function for the iteration 5030----->12.648008609098635 :)\n",
            "The loss function for the iteration 5040----->12.643747663210187 :)\n",
            "The loss function for the iteration 5050----->12.639556673878602 :)\n",
            "The loss function for the iteration 5060----->12.635410326870115 :)\n",
            "The loss function for the iteration 5070----->12.631351348837919 :)\n",
            "The loss function for the iteration 5080----->12.627338858886015 :)\n",
            "The loss function for the iteration 5090----->12.623362774058844 :)\n",
            "The loss function for the iteration 5100----->12.619466310006928 :)\n",
            "The loss function for the iteration 5110----->12.615620179045392 :)\n",
            "The loss function for the iteration 5120----->12.611822741370126 :)\n",
            "The loss function for the iteration 5130----->12.608076340978108 :)\n",
            "The loss function for the iteration 5140----->12.604382369750747 :)\n",
            "The loss function for the iteration 5150----->12.600789891254554 :)\n",
            "The loss function for the iteration 5160----->12.597167205705283 :)\n",
            "The loss function for the iteration 5170----->12.59366134339084 :)\n",
            "The loss function for the iteration 5180----->12.590195200613131 :)\n",
            "The loss function for the iteration 5190----->12.586749558256434 :)\n",
            "The loss function for the iteration 5200----->12.583374066972283 :)\n",
            "The loss function for the iteration 5210----->12.580006187335785 :)\n",
            "The loss function for the iteration 5220----->12.576743900998947 :)\n",
            "The loss function for the iteration 5230----->12.573475637667467 :)\n",
            "The loss function for the iteration 5240----->12.57028719597617 :)\n",
            "The loss function for the iteration 5250----->12.567148803467294 :)\n",
            "The loss function for the iteration 5260----->12.564029673899551 :)\n",
            "The loss function for the iteration 5270----->12.56094753775109 :)\n",
            "The loss function for the iteration 5280----->12.557931972869223 :)\n",
            "The loss function for the iteration 5290----->12.554895852307016 :)\n",
            "The loss function for the iteration 5300----->12.551984525197083 :)\n",
            "The loss function for the iteration 5310----->12.549086706400757 :)\n",
            "The loss function for the iteration 5320----->12.546247770169977 :)\n",
            "The loss function for the iteration 5330----->12.54340201977876 :)\n",
            "The loss function for the iteration 5340----->12.540634854177176 :)\n",
            "The loss function for the iteration 5350----->12.53786558003712 :)\n",
            "The loss function for the iteration 5360----->12.535176791614933 :)\n",
            "The loss function for the iteration 5370----->12.532477433061324 :)\n",
            "The loss function for the iteration 5380----->12.529871081437108 :)\n",
            "The loss function for the iteration 5390----->12.527210948568857 :)\n",
            "The loss function for the iteration 5400----->12.524682913616004 :)\n",
            "The loss function for the iteration 5410----->12.522149670189956 :)\n",
            "The loss function for the iteration 5420----->12.519649132023607 :)\n",
            "The loss function for the iteration 5430----->12.517192190931802 :)\n",
            "The loss function for the iteration 5440----->12.514749556874504 :)\n",
            "The loss function for the iteration 5450----->12.512359144794711 :)\n",
            "The loss function for the iteration 5460----->12.509979452687011 :)\n",
            "The loss function for the iteration 5470----->12.507630808013825 :)\n",
            "The loss function for the iteration 5480----->12.505322018033914 :)\n",
            "The loss function for the iteration 5490----->12.503061697143426 :)\n",
            "The loss function for the iteration 5500----->12.500782999924386 :)\n",
            "The loss function for the iteration 5510----->12.498577292548571 :)\n",
            "The loss function for the iteration 5520----->12.496378404690814 :)\n",
            "The loss function for the iteration 5530----->12.494215952021463 :)\n",
            "The loss function for the iteration 5540----->12.492072160073317 :)\n",
            "The loss function for the iteration 5550----->12.489948028942312 :)\n",
            "The loss function for the iteration 5560----->12.487857232994955 :)\n",
            "The loss function for the iteration 5570----->12.485830946304494 :)\n",
            "The loss function for the iteration 5580----->12.48378209246581 :)\n",
            "The loss function for the iteration 5590----->12.481771838909978 :)\n",
            "The loss function for the iteration 5600----->12.479768787286186 :)\n",
            "The loss function for the iteration 5610----->12.477853773212015 :)\n",
            "The loss function for the iteration 5620----->12.475924948235466 :)\n",
            "The loss function for the iteration 5630----->12.47399557526408 :)\n",
            "The loss function for the iteration 5640----->12.472088927814571 :)\n",
            "The loss function for the iteration 5650----->12.470255286873035 :)\n",
            "The loss function for the iteration 5660----->12.46839164836654 :)\n",
            "The loss function for the iteration 5670----->12.466585862919473 :)\n",
            "The loss function for the iteration 5680----->12.464795742909242 :)\n",
            "The loss function for the iteration 5690----->12.463009470381234 :)\n",
            "The loss function for the iteration 5700----->12.461263972665073 :)\n",
            "The loss function for the iteration 5710----->12.459530054005228 :)\n",
            "The loss function for the iteration 5720----->12.457818230437287 :)\n",
            "The loss function for the iteration 5730----->12.45613687094505 :)\n",
            "The loss function for the iteration 5740----->12.454461650972503 :)\n",
            "The loss function for the iteration 5750----->12.452808555198153 :)\n",
            "The loss function for the iteration 5760----->12.451194081830677 :)\n",
            "The loss function for the iteration 5770----->12.449554557410101 :)\n",
            "The loss function for the iteration 5780----->12.447946874561037 :)\n",
            "The loss function for the iteration 5790----->12.446392849420283 :)\n",
            "The loss function for the iteration 5800----->12.444829769073241 :)\n",
            "The loss function for the iteration 5810----->12.443287606731635 :)\n",
            "The loss function for the iteration 5820----->12.44177351207156 :)\n",
            "The loss function for the iteration 5830----->12.440283426018663 :)\n",
            "The loss function for the iteration 5840----->12.43878705258214 :)\n",
            "The loss function for the iteration 5850----->12.437330561787894 :)\n",
            "The loss function for the iteration 5860----->12.435885712832762 :)\n",
            "The loss function for the iteration 5870----->12.434439577498212 :)\n",
            "The loss function for the iteration 5880----->12.433032620203903 :)\n",
            "The loss function for the iteration 5890----->12.431613946969136 :)\n",
            "The loss function for the iteration 5900----->12.43023520765403 :)\n",
            "The loss function for the iteration 5910----->12.428865898502341 :)\n",
            "The loss function for the iteration 5920----->12.427487672879142 :)\n",
            "The loss function for the iteration 5930----->12.426173749706518 :)\n",
            "The loss function for the iteration 5940----->12.424854095707962 :)\n",
            "The loss function for the iteration 5950----->12.423505988321468 :)\n",
            "The loss function for the iteration 5960----->12.422221728919732 :)\n",
            "The loss function for the iteration 5970----->12.420950581942058 :)\n",
            "The loss function for the iteration 5980----->12.419674128177475 :)\n",
            "The loss function for the iteration 5990----->12.41842703062363 :)\n",
            "The loss function for the iteration 6000----->12.417163017462187 :)\n",
            "The loss function for the iteration 6010----->12.415930776213482 :)\n",
            "The loss function for the iteration 6020----->12.414715237625076 :)\n",
            "The loss function for the iteration 6030----->12.413521146163003 :)\n",
            "The loss function for the iteration 6040----->12.412333053200769 :)\n",
            "The loss function for the iteration 6050----->12.411163244688485 :)\n",
            "The loss function for the iteration 6060----->12.409993840521343 :)\n",
            "The loss function for the iteration 6070----->12.408809009413936 :)\n",
            "The loss function for the iteration 6080----->12.40769202192362 :)\n",
            "The loss function for the iteration 6090----->12.406568519593174 :)\n",
            "The loss function for the iteration 6100----->12.40544052547369 :)\n",
            "The loss function for the iteration 6110----->12.404338471664945 :)\n",
            "The loss function for the iteration 6120----->12.403217746502916 :)\n",
            "The loss function for the iteration 6130----->12.402124054463863 :)\n",
            "The loss function for the iteration 6140----->12.401069006918268 :)\n",
            "The loss function for the iteration 6150----->12.40000381794377 :)\n",
            "The loss function for the iteration 6160----->12.39894319578287 :)\n",
            "The loss function for the iteration 6170----->12.397891599786881 :)\n",
            "The loss function for the iteration 6180----->12.396890019630796 :)\n",
            "The loss function for the iteration 6190----->12.39583961183012 :)\n",
            "The loss function for the iteration 6200----->12.394814701816543 :)\n",
            "The loss function for the iteration 6210----->12.393826560132398 :)\n",
            "The loss function for the iteration 6220----->12.392838948842103 :)\n",
            "The loss function for the iteration 6230----->12.391862142056201 :)\n",
            "The loss function for the iteration 6240----->12.390878315203814 :)\n",
            "The loss function for the iteration 6250----->12.389912947231778 :)\n",
            "The loss function for the iteration 6260----->12.388939636014246 :)\n",
            "The loss function for the iteration 6270----->12.388006662698555 :)\n",
            "The loss function for the iteration 6280----->12.387068330129475 :)\n",
            "The loss function for the iteration 6290----->12.38612563384457 :)\n",
            "The loss function for the iteration 6300----->12.385223644177824 :)\n",
            "The loss function for the iteration 6310----->12.384279572141601 :)\n",
            "The loss function for the iteration 6320----->12.383401685745495 :)\n",
            "The loss function for the iteration 6330----->12.382493061257165 :)\n",
            "The loss function for the iteration 6340----->12.381578136529244 :)\n",
            "The loss function for the iteration 6350----->12.380720112360274 :)\n",
            "The loss function for the iteration 6360----->12.37985261439368 :)\n",
            "The loss function for the iteration 6370----->12.378997939292072 :)\n",
            "The loss function for the iteration 6380----->12.378102748109951 :)\n",
            "The loss function for the iteration 6390----->12.377291512391938 :)\n",
            "The loss function for the iteration 6400----->12.376444949483325 :)\n",
            "The loss function for the iteration 6410----->12.375607264585499 :)\n",
            "The loss function for the iteration 6420----->12.374792167273592 :)\n",
            "The loss function for the iteration 6430----->12.37394261309623 :)\n",
            "The loss function for the iteration 6440----->12.373134922987157 :)\n",
            "The loss function for the iteration 6450----->12.372323541287326 :)\n",
            "The loss function for the iteration 6460----->12.371535513088903 :)\n",
            "The loss function for the iteration 6470----->12.370728153691408 :)\n",
            "The loss function for the iteration 6480----->12.369959441225257 :)\n",
            "The loss function for the iteration 6490----->12.369165908717164 :)\n",
            "The loss function for the iteration 6500----->12.368414362140577 :)\n",
            "The loss function for the iteration 6510----->12.367650103567835 :)\n",
            "The loss function for the iteration 6520----->12.36687678391376 :)\n",
            "The loss function for the iteration 6530----->12.366147795709296 :)\n",
            "The loss function for the iteration 6540----->12.365386944603149 :)\n",
            "The loss function for the iteration 6550----->12.364644518150673 :)\n",
            "The loss function for the iteration 6560----->12.363948731152238 :)\n",
            "The loss function for the iteration 6570----->12.36322067712619 :)\n",
            "The loss function for the iteration 6580----->12.362488619589097 :)\n",
            "The loss function for the iteration 6590----->12.36180353455 :)\n",
            "The loss function for the iteration 6600----->12.361111125870904 :)\n",
            "The loss function for the iteration 6610----->12.360424635484213 :)\n",
            "The loss function for the iteration 6620----->12.35974689012416 :)\n",
            "The loss function for the iteration 6630----->12.359053137298275 :)\n",
            "The loss function for the iteration 6640----->12.358382670455136 :)\n",
            "The loss function for the iteration 6650----->12.357695538980808 :)\n",
            "The loss function for the iteration 6660----->12.357022227610754 :)\n",
            "The loss function for the iteration 6670----->12.356366930423238 :)\n",
            "The loss function for the iteration 6680----->12.35571581384633 :)\n",
            "The loss function for the iteration 6690----->12.355086503081589 :)\n",
            "The loss function for the iteration 6700----->12.354411094090533 :)\n",
            "The loss function for the iteration 6710----->12.35382082277872 :)\n",
            "The loss function for the iteration 6720----->12.353130957500987 :)\n",
            "The loss function for the iteration 6730----->12.352514141759485 :)\n",
            "The loss function for the iteration 6740----->12.351880605566286 :)\n",
            "The loss function for the iteration 6750----->12.351235187187562 :)\n",
            "The loss function for the iteration 6760----->12.350640115443841 :)\n",
            "The loss function for the iteration 6770----->12.350003281698 :)\n",
            "The loss function for the iteration 6780----->12.349381952360266 :)\n",
            "The loss function for the iteration 6790----->12.348798698734843 :)\n",
            "The loss function for the iteration 6800----->12.34821737190647 :)\n",
            "The loss function for the iteration 6810----->12.34761247895942 :)\n",
            "The loss function for the iteration 6820----->12.347017779969704 :)\n",
            "The loss function for the iteration 6830----->12.34642315715215 :)\n",
            "The loss function for the iteration 6840----->12.34586676834133 :)\n",
            "The loss function for the iteration 6850----->12.345259733770988 :)\n",
            "The loss function for the iteration 6860----->12.344689382946962 :)\n",
            "The loss function for the iteration 6870----->12.344112061289554 :)\n",
            "The loss function for the iteration 6880----->12.34356945165844 :)\n",
            "The loss function for the iteration 6890----->12.342976961000021 :)\n",
            "The loss function for the iteration 6900----->12.342421446053915 :)\n",
            "The loss function for the iteration 6910----->12.341871366087233 :)\n",
            "The loss function for the iteration 6920----->12.341316905066755 :)\n",
            "The loss function for the iteration 6930----->12.34076391350682 :)\n",
            "The loss function for the iteration 6940----->12.34022104599146 :)\n",
            "The loss function for the iteration 6950----->12.339711262379973 :)\n",
            "The loss function for the iteration 6960----->12.33914806227967 :)\n",
            "The loss function for the iteration 6970----->12.338625837859514 :)\n",
            "The loss function for the iteration 6980----->12.338099916182593 :)\n",
            "The loss function for the iteration 6990----->12.337612588727605 :)\n",
            "The loss function for the iteration 7000----->12.337094447654684 :)\n",
            "The loss function for the iteration 7010----->12.336580925948109 :)\n",
            "The loss function for the iteration 7020----->12.336085217637704 :)\n",
            "The loss function for the iteration 7030----->12.335594295825192 :)\n",
            "The loss function for the iteration 7040----->12.335092626942574 :)\n",
            "The loss function for the iteration 7050----->12.334601402505013 :)\n",
            "The loss function for the iteration 7060----->12.334109243236705 :)\n",
            "The loss function for the iteration 7070----->12.333614288447556 :)\n",
            "The loss function for the iteration 7080----->12.333169720606191 :)\n",
            "The loss function for the iteration 7090----->12.332682239558082 :)\n",
            "The loss function for the iteration 7100----->12.332227775516362 :)\n",
            "The loss function for the iteration 7110----->12.331776626916225 :)\n",
            "The loss function for the iteration 7120----->12.331333418262243 :)\n",
            "The loss function for the iteration 7130----->12.330892138260973 :)\n",
            "The loss function for the iteration 7140----->12.330424010531846 :)\n",
            "The loss function for the iteration 7150----->12.33001156067589 :)\n",
            "The loss function for the iteration 7160----->12.329558359306127 :)\n",
            "The loss function for the iteration 7170----->12.32913801082029 :)\n",
            "The loss function for the iteration 7180----->12.32869757295618 :)\n",
            "The loss function for the iteration 7190----->12.328265198360768 :)\n",
            "The loss function for the iteration 7200----->12.327847691987651 :)\n",
            "The loss function for the iteration 7210----->12.327388651803155 :)\n",
            "The loss function for the iteration 7220----->12.326980827899106 :)\n",
            "The loss function for the iteration 7230----->12.32656438801573 :)\n",
            "The loss function for the iteration 7240----->12.326143651583278 :)\n",
            "The loss function for the iteration 7250----->12.32574136696447 :)\n",
            "The loss function for the iteration 7260----->12.325328611119584 :)\n",
            "The loss function for the iteration 7270----->12.324904191552815 :)\n",
            "The loss function for the iteration 7280----->12.324521441216433 :)\n",
            "The loss function for the iteration 7290----->12.324107951032406 :)\n",
            "The loss function for the iteration 7300----->12.323691573674036 :)\n",
            "The loss function for the iteration 7310----->12.323321061958344 :)\n",
            "The loss function for the iteration 7320----->12.322889380547444 :)\n",
            "The loss function for the iteration 7330----->12.322507352300175 :)\n",
            "The loss function for the iteration 7340----->12.322128141417231 :)\n",
            "The loss function for the iteration 7350----->12.321702254133868 :)\n",
            "The loss function for the iteration 7360----->12.3213251037481 :)\n",
            "The loss function for the iteration 7370----->12.320935584237603 :)\n",
            "The loss function for the iteration 7380----->12.320551084673614 :)\n",
            "The loss function for the iteration 7390----->12.32021015852567 :)\n",
            "The loss function for the iteration 7400----->12.319804243900908 :)\n",
            "The loss function for the iteration 7410----->12.319425325020308 :)\n",
            "The loss function for the iteration 7420----->12.319046941378032 :)\n",
            "The loss function for the iteration 7430----->12.318676307278972 :)\n",
            "The loss function for the iteration 7440----->12.318324124358544 :)\n",
            "The loss function for the iteration 7450----->12.3179674363865 :)\n",
            "The loss function for the iteration 7460----->12.317603456073368 :)\n",
            "The loss function for the iteration 7470----->12.317211178140866 :)\n",
            "The loss function for the iteration 7480----->12.316855695845444 :)\n",
            "The loss function for the iteration 7490----->12.316509666521082 :)\n",
            "The loss function for the iteration 7500----->12.316120865598057 :)\n",
            "The loss function for the iteration 7510----->12.315811741157367 :)\n",
            "The loss function for the iteration 7520----->12.315430761963896 :)\n",
            "The loss function for the iteration 7530----->12.315095196125213 :)\n",
            "The loss function for the iteration 7540----->12.314728542790094 :)\n",
            "The loss function for the iteration 7550----->12.314379400703661 :)\n",
            "The loss function for the iteration 7560----->12.31403820358166 :)\n",
            "The loss function for the iteration 7570----->12.313709953152761 :)\n",
            "The loss function for the iteration 7580----->12.313363450703793 :)\n",
            "The loss function for the iteration 7590----->12.313005832060448 :)\n",
            "The loss function for the iteration 7600----->12.312670325852995 :)\n",
            "The loss function for the iteration 7610----->12.3123275231322 :)\n",
            "The loss function for the iteration 7620----->12.311986535744897 :)\n",
            "The loss function for the iteration 7630----->12.311666792511982 :)\n",
            "The loss function for the iteration 7640----->12.311338094016136 :)\n",
            "The loss function for the iteration 7650----->12.311007229079717 :)\n",
            "The loss function for the iteration 7660----->12.310687189954582 :)\n",
            "The loss function for the iteration 7670----->12.310356312997799 :)\n",
            "The loss function for the iteration 7680----->12.310047424592035 :)\n",
            "The loss function for the iteration 7690----->12.309743098910577 :)\n",
            "The loss function for the iteration 7700----->12.309414689512172 :)\n",
            "The loss function for the iteration 7710----->12.30909025116807 :)\n",
            "The loss function for the iteration 7720----->12.308773960691052 :)\n",
            "The loss function for the iteration 7730----->12.308465772452303 :)\n",
            "The loss function for the iteration 7740----->12.308130728506756 :)\n",
            "The loss function for the iteration 7750----->12.307805756345788 :)\n",
            "The loss function for the iteration 7760----->12.307522859512282 :)\n",
            "The loss function for the iteration 7770----->12.307200479453368 :)\n",
            "The loss function for the iteration 7780----->12.306901931081446 :)\n",
            "The loss function for the iteration 7790----->12.306582192516451 :)\n",
            "The loss function for the iteration 7800----->12.306299913128768 :)\n",
            "The loss function for the iteration 7810----->12.305980908462544 :)\n",
            "The loss function for the iteration 7820----->12.30569574103016 :)\n",
            "The loss function for the iteration 7830----->12.305391652113705 :)\n",
            "The loss function for the iteration 7840----->12.305095381583566 :)\n",
            "The loss function for the iteration 7850----->12.304801012699599 :)\n",
            "The loss function for the iteration 7860----->12.304512054491356 :)\n",
            "The loss function for the iteration 7870----->12.304212620131775 :)\n",
            "The loss function for the iteration 7880----->12.303926268171432 :)\n",
            "The loss function for the iteration 7890----->12.303598126140658 :)\n",
            "The loss function for the iteration 7900----->12.303332642760207 :)\n",
            "The loss function for the iteration 7910----->12.303041753181574 :)\n",
            "The loss function for the iteration 7920----->12.302752234751345 :)\n",
            "The loss function for the iteration 7930----->12.302490615683203 :)\n",
            "The loss function for the iteration 7940----->12.302203170572852 :)\n",
            "The loss function for the iteration 7950----->12.301869600654022 :)\n",
            "The loss function for the iteration 7960----->12.301599843375275 :)\n",
            "The loss function for the iteration 7970----->12.301313771984116 :)\n",
            "The loss function for the iteration 7980----->12.301041676945678 :)\n",
            "The loss function for the iteration 7990----->12.300770488659008 :)\n",
            "The loss function for the iteration 8000----->12.300507319148402 :)\n",
            "The loss function for the iteration 8010----->12.300213412799653 :)\n",
            "The loss function for the iteration 8020----->12.299947007241425 :)\n",
            "The loss function for the iteration 8030----->12.299677004226446 :)\n",
            "The loss function for the iteration 8040----->12.29941124332412 :)\n",
            "The loss function for the iteration 8050----->12.29913206616584 :)\n",
            "The loss function for the iteration 8060----->12.298874571211353 :)\n",
            "The loss function for the iteration 8070----->12.298617605648023 :)\n",
            "The loss function for the iteration 8080----->12.298304263553936 :)\n",
            "The loss function for the iteration 8090----->12.298069219759583 :)\n",
            "The loss function for the iteration 8100----->12.29782124911505 :)\n",
            "The loss function for the iteration 8110----->12.297582012626334 :)\n",
            "The loss function for the iteration 8120----->12.29729050627503 :)\n",
            "The loss function for the iteration 8130----->12.297028176207839 :)\n",
            "The loss function for the iteration 8140----->12.296776258245865 :)\n",
            "The loss function for the iteration 8150----->12.296493632181416 :)\n",
            "The loss function for the iteration 8160----->12.296243256918997 :)\n",
            "The loss function for the iteration 8170----->12.295993324548416 :)\n",
            "The loss function for the iteration 8180----->12.29574913527024 :)\n",
            "The loss function for the iteration 8190----->12.295490617105543 :)\n",
            "The loss function for the iteration 8200----->12.295273248681369 :)\n",
            "The loss function for the iteration 8210----->12.294998714223764 :)\n",
            "The loss function for the iteration 8220----->12.29474672993872 :)\n",
            "The loss function for the iteration 8230----->12.294481370609454 :)\n",
            "The loss function for the iteration 8240----->12.294219233904835 :)\n",
            "The loss function for the iteration 8250----->12.293978828829436 :)\n",
            "The loss function for the iteration 8260----->12.293751574244993 :)\n",
            "The loss function for the iteration 8270----->12.293497885542376 :)\n",
            "The loss function for the iteration 8280----->12.293253803751195 :)\n",
            "The loss function for the iteration 8290----->12.293014815280094 :)\n",
            "The loss function for the iteration 8300----->12.29278688019303 :)\n",
            "The loss function for the iteration 8310----->12.292530986688107 :)\n",
            "The loss function for the iteration 8320----->12.292299437581562 :)\n",
            "The loss function for the iteration 8330----->12.292074754090203 :)\n",
            "The loss function for the iteration 8340----->12.291819904041668 :)\n",
            "The loss function for the iteration 8350----->12.291568525826506 :)\n",
            "The loss function for the iteration 8360----->12.291322410535827 :)\n",
            "The loss function for the iteration 8370----->12.291108556510212 :)\n",
            "The loss function for the iteration 8380----->12.290886963477671 :)\n",
            "The loss function for the iteration 8390----->12.29062501923745 :)\n",
            "The loss function for the iteration 8400----->12.290399048252965 :)\n",
            "The loss function for the iteration 8410----->12.290169736743561 :)\n",
            "The loss function for the iteration 8420----->12.289953543038463 :)\n",
            "The loss function for the iteration 8430----->12.28972973953817 :)\n",
            "The loss function for the iteration 8440----->12.289486616607622 :)\n",
            "The loss function for the iteration 8450----->12.289298916459757 :)\n",
            "The loss function for the iteration 8460----->12.289018822012649 :)\n",
            "The loss function for the iteration 8470----->12.288817486598882 :)\n",
            "The loss function for the iteration 8480----->12.28859455880245 :)\n",
            "The loss function for the iteration 8490----->12.28833723214745 :)\n",
            "The loss function for the iteration 8500----->12.288141302178651 :)\n",
            "The loss function for the iteration 8510----->12.287913543448118 :)\n",
            "The loss function for the iteration 8520----->12.287692582906946 :)\n",
            "The loss function for the iteration 8530----->12.28749814709903 :)\n",
            "The loss function for the iteration 8540----->12.287226572946544 :)\n",
            "The loss function for the iteration 8550----->12.287041161611961 :)\n",
            "The loss function for the iteration 8560----->12.286832224670363 :)\n",
            "The loss function for the iteration 8570----->12.28659919356234 :)\n",
            "The loss function for the iteration 8580----->12.286398841966612 :)\n",
            "The loss function for the iteration 8590----->12.286150608028866 :)\n",
            "The loss function for the iteration 8600----->12.285976820024366 :)\n",
            "The loss function for the iteration 8610----->12.285756404853577 :)\n",
            "The loss function for the iteration 8620----->12.28553658720015 :)\n",
            "The loss function for the iteration 8630----->12.285297889208485 :)\n",
            "The loss function for the iteration 8640----->12.28509634053919 :)\n",
            "The loss function for the iteration 8650----->12.284908151668686 :)\n",
            "The loss function for the iteration 8660----->12.284693793381056 :)\n",
            "The loss function for the iteration 8670----->12.284469664159266 :)\n",
            "The loss function for the iteration 8680----->12.284252651236159 :)\n",
            "The loss function for the iteration 8690----->12.28407824083254 :)\n",
            "The loss function for the iteration 8700----->12.283872623015167 :)\n",
            "The loss function for the iteration 8710----->12.283648480920064 :)\n",
            "The loss function for the iteration 8720----->12.28345385664949 :)\n",
            "The loss function for the iteration 8730----->12.2832668126668 :)\n",
            "The loss function for the iteration 8740----->12.28304399584298 :)\n",
            "The loss function for the iteration 8750----->12.282845061454031 :)\n",
            "The loss function for the iteration 8760----->12.282637792071156 :)\n",
            "The loss function for the iteration 8770----->12.282437558355884 :)\n",
            "The loss function for the iteration 8780----->12.282235729734152 :)\n",
            "The loss function for the iteration 8790----->12.282055488285357 :)\n",
            "The loss function for the iteration 8800----->12.281851712079057 :)\n",
            "The loss function for the iteration 8810----->12.281644251261163 :)\n",
            "The loss function for the iteration 8820----->12.281428420748933 :)\n",
            "The loss function for the iteration 8830----->12.281242876330603 :)\n",
            "The loss function for the iteration 8840----->12.281075132615761 :)\n",
            "The loss function for the iteration 8850----->12.280874194723218 :)\n",
            "The loss function for the iteration 8860----->12.280677908514361 :)\n",
            "The loss function for the iteration 8870----->12.280461712131927 :)\n",
            "The loss function for the iteration 8880----->12.280271578677347 :)\n",
            "The loss function for the iteration 8890----->12.28009608598012 :)\n",
            "The loss function for the iteration 8900----->12.27994778354951 :)\n",
            "The loss function for the iteration 8910----->12.27969638087598 :)\n",
            "The loss function for the iteration 8920----->12.279528012128685 :)\n",
            "The loss function for the iteration 8930----->12.279319788098153 :)\n",
            "The loss function for the iteration 8940----->12.279158921126232 :)\n",
            "The loss function for the iteration 8950----->12.27895654098312 :)\n",
            "The loss function for the iteration 8960----->12.278790512558093 :)\n",
            "The loss function for the iteration 8970----->12.278566252368869 :)\n",
            "The loss function for the iteration 8980----->12.278361890159943 :)\n",
            "The loss function for the iteration 8990----->12.278217829433316 :)\n",
            "The loss function for the iteration 9000----->12.27802714368617 :)\n",
            "The loss function for the iteration 9010----->12.277846099846595 :)\n",
            "The loss function for the iteration 9020----->12.277660456445517 :)\n",
            "The loss function for the iteration 9030----->12.277474688916193 :)\n",
            "The loss function for the iteration 9040----->12.27727619771156 :)\n",
            "The loss function for the iteration 9050----->12.277092249500805 :)\n",
            "The loss function for the iteration 9060----->12.276932221862626 :)\n",
            "The loss function for the iteration 9070----->12.276759186773702 :)\n",
            "The loss function for the iteration 9080----->12.276567241722532 :)\n",
            "The loss function for the iteration 9090----->12.276391292160849 :)\n",
            "The loss function for the iteration 9100----->12.276207669070141 :)\n",
            "The loss function for the iteration 9110----->12.27605521688838 :)\n",
            "The loss function for the iteration 9120----->12.275875830273154 :)\n",
            "The loss function for the iteration 9130----->12.275686850925013 :)\n",
            "The loss function for the iteration 9140----->12.27546974430184 :)\n",
            "The loss function for the iteration 9150----->12.275342920335813 :)\n",
            "The loss function for the iteration 9160----->12.275155259446047 :)\n",
            "The loss function for the iteration 9170----->12.274971259041575 :)\n",
            "The loss function for the iteration 9180----->12.274814867991871 :)\n",
            "The loss function for the iteration 9190----->12.27463406350933 :)\n",
            "The loss function for the iteration 9200----->12.274459524228941 :)\n",
            "The loss function for the iteration 9210----->12.274296412158176 :)\n",
            "The loss function for the iteration 9220----->12.274123743306163 :)\n",
            "The loss function for the iteration 9230----->12.27393393974792 :)\n",
            "The loss function for the iteration 9240----->12.273799532647981 :)\n",
            "The loss function for the iteration 9250----->12.273608437235865 :)\n",
            "The loss function for the iteration 9260----->12.273425355111538 :)\n",
            "The loss function for the iteration 9270----->12.273274839859633 :)\n",
            "The loss function for the iteration 9280----->12.27313287984721 :)\n",
            "The loss function for the iteration 9290----->12.27295200838595 :)\n",
            "The loss function for the iteration 9300----->12.272767696076363 :)\n",
            "The loss function for the iteration 9310----->12.272596559679856 :)\n",
            "The loss function for the iteration 9320----->12.272446195640027 :)\n",
            "The loss function for the iteration 9330----->12.272288167717985 :)\n",
            "The loss function for the iteration 9340----->12.272141307664867 :)\n",
            "The loss function for the iteration 9350----->12.271991961365975 :)\n",
            "The loss function for the iteration 9360----->12.27184094074472 :)\n",
            "The loss function for the iteration 9370----->12.271688190970266 :)\n",
            "The loss function for the iteration 9380----->12.271566448801725 :)\n",
            "The loss function for the iteration 9390----->12.271426249604218 :)\n",
            "The loss function for the iteration 9400----->12.271282107404044 :)\n",
            "The loss function for the iteration 9410----->12.271132696051222 :)\n",
            "The loss function for the iteration 9420----->12.270982085516135 :)\n",
            "The loss function for the iteration 9430----->12.270831047295118 :)\n",
            "The loss function for the iteration 9440----->12.270725290437293 :)\n",
            "The loss function for the iteration 9450----->12.270574258264713 :)\n",
            "The loss function for the iteration 9460----->12.270428728582571 :)\n",
            "The loss function for the iteration 9470----->12.270300730120146 :)\n",
            "The loss function for the iteration 9480----->12.270136607602828 :)\n",
            "The loss function for the iteration 9490----->12.270023729365887 :)\n",
            "The loss function for the iteration 9500----->12.269908004874997 :)\n",
            "The loss function for the iteration 9510----->12.269738029770771 :)\n",
            "The loss function for the iteration 9520----->12.269629888853546 :)\n",
            "The loss function for the iteration 9530----->12.269514942297775 :)\n",
            "The loss function for the iteration 9540----->12.26937334677914 :)\n",
            "The loss function for the iteration 9550----->12.269242619011134 :)\n",
            "The loss function for the iteration 9560----->12.269083413698068 :)\n",
            "The loss function for the iteration 9570----->12.268961239645268 :)\n",
            "The loss function for the iteration 9580----->12.268859046047314 :)\n",
            "The loss function for the iteration 9590----->12.268701324588944 :)\n",
            "The loss function for the iteration 9600----->12.268585778224057 :)\n",
            "The loss function for the iteration 9610----->12.268480077554297 :)\n",
            "The loss function for the iteration 9620----->12.268332913767768 :)\n",
            "The loss function for the iteration 9630----->12.268201565732774 :)\n",
            "The loss function for the iteration 9640----->12.268075602166448 :)\n",
            "The loss function for the iteration 9650----->12.267953843372338 :)\n",
            "The loss function for the iteration 9660----->12.267844173791095 :)\n",
            "The loss function for the iteration 9670----->12.267708871870955 :)\n",
            "The loss function for the iteration 9680----->12.267583710325939 :)\n",
            "The loss function for the iteration 9690----->12.267466947457411 :)\n",
            "The loss function for the iteration 9700----->12.267336129983068 :)\n",
            "The loss function for the iteration 9710----->12.267204068511724 :)\n",
            "The loss function for the iteration 9720----->12.2671073026983 :)\n",
            "The loss function for the iteration 9730----->12.267000777075161 :)\n",
            "The loss function for the iteration 9740----->12.266876467480337 :)\n",
            "The loss function for the iteration 9750----->12.266739504660404 :)\n",
            "The loss function for the iteration 9760----->12.266604831404209 :)\n",
            "The loss function for the iteration 9770----->12.266502971855976 :)\n",
            "The loss function for the iteration 9780----->12.266399528806755 :)\n",
            "The loss function for the iteration 9790----->12.266240734339345 :)\n",
            "The loss function for the iteration 9800----->12.26615255259428 :)\n",
            "The loss function for the iteration 9810----->12.266019273040072 :)\n",
            "The loss function for the iteration 9820----->12.26588558499428 :)\n",
            "The loss function for the iteration 9830----->12.265808752712834 :)\n",
            "The loss function for the iteration 9840----->12.265697431965245 :)\n",
            "The loss function for the iteration 9850----->12.265545711468974 :)\n",
            "The loss function for the iteration 9860----->12.265460430423348 :)\n",
            "The loss function for the iteration 9870----->12.265332236647918 :)\n",
            "The loss function for the iteration 9880----->12.265249535030865 :)\n",
            "The loss function for the iteration 9890----->12.265108300884403 :)\n",
            "The loss function for the iteration 9900----->12.265000860838683 :)\n",
            "The loss function for the iteration 9910----->12.264869248325027 :)\n",
            "The loss function for the iteration 9920----->12.264780690675288 :)\n",
            "The loss function for the iteration 9930----->12.264691358547635 :)\n",
            "The loss function for the iteration 9940----->12.264539859592666 :)\n",
            "The loss function for the iteration 9950----->12.264447197175636 :)\n",
            "The loss function for the iteration 9960----->12.264352251247562 :)\n",
            "The loss function for the iteration 9970----->12.264218978658027 :)\n",
            "The loss function for the iteration 9980----->12.264141793292923 :)\n",
            "The loss function for the iteration 9990----->12.264018882197115 :)\n",
            "The loss function for the iteration 10000----->12.26391729392775 :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso Regression using skicit-learn (5 points)"
      ],
      "metadata": {
        "id": "gxeSokECU_PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `sklearn` to train a Lasso Regression Model. To determine the best regularization coefficients, use grid-search (or other techniques you've learned till now)."
      ],
      "metadata": {
        "id": "LjkaQEKlU_PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sk_lasso = Lasso()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:35:17.374562Z",
          "iopub.execute_input": "2023-04-30T08:35:17.375634Z",
          "iopub.status.idle": "2023-04-30T08:35:17.403838Z",
          "shell.execute_reply.started": "2023-04-30T08:35:17.375589Z",
          "shell.execute_reply": "2023-04-30T08:35:17.401840Z"
        },
        "trusted": true,
        "id": "Q1fX_s6aU_PW"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'alpha': [0.001, 0.01, .1, 1, 10, 100]}"
      ],
      "metadata": {
        "id": "9ucwJmW11cja"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(sk_lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "PG-WIErl2I0a",
        "outputId": "cf88b191-5d15-486c-b9ad-1247879feab2"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+02, tolerance: 6.610e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.602e+01, tolerance: 6.919e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.405e+02, tolerance: 6.751e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.606e+02, tolerance: 6.403e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.991e+01, tolerance: 6.652e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.497e+02, tolerance: 8.336e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=Lasso(),\n",
              "             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring='neg_mean_squared_error')"
            ],
            "text/html": [
              "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Lasso(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Lasso(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best mean squared error:\", -grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNNtV8GB2Lmz",
        "outputId": "6c7c2407-57c1-4682-8636-82e4ab5c55dd"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'alpha': 0.001}\n",
            "Best mean squared error: 3.459417088839287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_lasso = Lasso(alpha = 0.001)"
      ],
      "metadata": {
        "id": "dGrWV0Ad-knU"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "AIaAsOwk-y9L",
        "outputId": "08dc1e72-23e0-4be3-ec8b-5c286ed1860c"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.497e+02, tolerance: 8.336e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso(alpha=0.001)"
            ],
            "text/html": [
              "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" checked><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(alpha=0.001)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression From scratch (5 points)"
      ],
      "metadata": {
        "id": "6vHBMK9pU_PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a ridge regression model using your own code and the following class."
      ],
      "metadata": {
        "id": "qkmTdCqlU_PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeRegression(Regression):\n",
        "    def __init__(self, lamda, learning_rate, iteration):\n",
        "        self.regularization = l2_regularization(lamda)\n",
        "        super(RidgeRegression, self).__init__(learning_rate, iteration, self.regularization)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:35:21.762634Z",
          "iopub.execute_input": "2023-04-30T08:35:21.763364Z",
          "iopub.status.idle": "2023-04-30T08:35:21.769134Z",
          "shell.execute_reply.started": "2023-04-30T08:35:21.763321Z",
          "shell.execute_reply": "2023-04-30T08:35:21.768198Z"
        },
        "trusted": true,
        "id": "ueIbohzQU_PW"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "VLh-EV-HOBRB",
        "outputId": "41c65f8f-f0ba-4900-85e3-4bc58c402c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.21493149,  0.56446727, -0.26470005, ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321],\n",
              "       [ 0.27958568,  0.05760833,  2.68457981, ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321],\n",
              "       [ 0.03232709, -1.18138018, -0.27267108, ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321],\n",
              "       ...,\n",
              "       [ 1.51587862,  0.00933605, -0.27267108, ..., -0.08851417,\n",
              "         2.49387004, -2.49387004],\n",
              "       [-1.20396584, -1.22965246, -0.27267108, ..., -0.08851417,\n",
              "         2.49387004, -2.49387004],\n",
              "       [ 1.02136145,  0.76560177,  0.1976195 , ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_ridge = RidgeRegression(lamda=0.001, learning_rate=0.001, iteration=10000)\n",
        "my_ridge.train(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:36:39.759220Z",
          "iopub.execute_input": "2023-04-30T08:36:39.759679Z",
          "iopub.status.idle": "2023-04-30T08:36:39.803215Z",
          "shell.execute_reply.started": "2023-04-30T08:36:39.759636Z",
          "shell.execute_reply": "2023-04-30T08:36:39.801482Z"
        },
        "trusted": true,
        "id": "Ows-DOCFU_PW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc778d39-ad69-4bc6-f24b-f624034c4f7c"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss function for the iteration 10----->2379.5317467521177 :)\n",
            "The loss function for the iteration 20----->2329.5180478884045 :)\n",
            "The loss function for the iteration 30----->2280.741573009738 :)\n",
            "The loss function for the iteration 40----->2233.1520965924865 :)\n",
            "The loss function for the iteration 50----->2186.703540136961 :)\n",
            "The loss function for the iteration 60----->2141.353477952617 :)\n",
            "The loss function for the iteration 70----->2097.0627080117783 :)\n",
            "The loss function for the iteration 80----->2053.7948790758674 :)\n",
            "The loss function for the iteration 90----->2011.5161664987277 :)\n",
            "The loss function for the iteration 100----->1970.1949901475411 :)\n",
            "The loss function for the iteration 110----->1929.8017687757565 :)\n",
            "The loss function for the iteration 120----->1890.3087059538923 :)\n",
            "The loss function for the iteration 130----->1851.6896033298253 :)\n",
            "The loss function for the iteration 140----->1813.9196975648408 :)\n",
            "The loss function for the iteration 150----->1776.9755177877407 :)\n",
            "The loss function for the iteration 160----->1740.8347608375184 :)\n",
            "The loss function for the iteration 170----->1705.476181934784 :)\n",
            "The loss function for the iteration 180----->1670.8794987413967 :)\n",
            "The loss function for the iteration 190----->1637.0253070433298 :)\n",
            "The loss function for the iteration 200----->1603.8950065299662 :)\n",
            "The loss function for the iteration 210----->1571.4707353485808 :)\n",
            "The loss function for the iteration 220----->1539.7353122904585 :)\n",
            "The loss function for the iteration 230----->1508.6721856185575 :)\n",
            "The loss function for the iteration 240----->1478.265387679285 :)\n",
            "The loss function for the iteration 250----->1448.4994945555457 :)\n",
            "The loss function for the iteration 260----->1419.3595901173542 :)\n",
            "The loss function for the iteration 270----->1390.8312339119375 :)\n",
            "The loss function for the iteration 280----->1362.900432409345 :)\n",
            "The loss function for the iteration 290----->1335.5536131836764 :)\n",
            "The loss function for the iteration 300----->1308.7776016654334 :)\n",
            "The loss function for the iteration 310----->1282.5596001484992 :)\n",
            "The loss function for the iteration 320----->1256.8871687767457 :)\n",
            "The loss function for the iteration 330----->1231.7482082712093 :)\n",
            "The loss function for the iteration 340----->1207.1309441899107 :)\n",
            "The loss function for the iteration 350----->1183.0239125393412 :)\n",
            "The loss function for the iteration 360----->1159.4159465800115 :)\n",
            "The loss function for the iteration 370----->1136.2961646886974 :)\n",
            "The loss function for the iteration 380----->1113.6539591575824 :)\n",
            "The loss function for the iteration 390----->1091.478985825726 :)\n",
            "The loss function for the iteration 400----->1069.7611544515 :)\n",
            "The loss function for the iteration 410----->1048.4906197461041 :)\n",
            "The loss function for the iteration 420----->1027.6577729982623 :)\n",
            "The loss function for the iteration 430----->1007.253234228806 :)\n",
            "The loss function for the iteration 440----->987.2678448214496 :)\n",
            "The loss function for the iteration 450----->967.6926605825388 :)\n",
            "The loss function for the iteration 460----->948.5189451883183 :)\n",
            "The loss function for the iteration 470----->929.7381639831493 :)\n",
            "The loss function for the iteration 480----->911.3419780965315 :)\n",
            "The loss function for the iteration 490----->893.3222388504466 :)\n",
            "The loss function for the iteration 500----->875.6709824319149 :)\n",
            "The loss function for the iteration 510----->858.380424808486 :)\n",
            "The loss function for the iteration 520----->841.4429568669091 :)\n",
            "The loss function for the iteration 530----->824.8511397574138 :)\n",
            "The loss function for the iteration 540----->808.59770042798 :)\n",
            "The loss function for the iteration 550----->792.6755273346262 :)\n",
            "The loss function for the iteration 560----->777.0776663152413 :)\n",
            "The loss function for the iteration 570----->761.7973166158029 :)\n",
            "The loss function for the iteration 580----->746.8278270589119 :)\n",
            "The loss function for the iteration 590----->732.1626923456341 :)\n",
            "The loss function for the iteration 600----->717.7955494824872 :)\n",
            "The loss function for the iteration 610----->703.7201743262093 :)\n",
            "The loss function for the iteration 620----->689.930478239622 :)\n",
            "The loss function for the iteration 630----->676.4205048525287 :)\n",
            "The loss function for the iteration 640----->663.1844269221031 :)\n",
            "The loss function for the iteration 650----->650.2165432877333 :)\n",
            "The loss function for the iteration 660----->637.5112759156798 :)\n",
            "The loss function for the iteration 670----->625.0631670293228 :)\n",
            "The loss function for the iteration 680----->612.8668763210634 :)\n",
            "The loss function for the iteration 690----->600.9171782423059 :)\n",
            "The loss function for the iteration 700----->589.2089593681607 :)\n",
            "The loss function for the iteration 710----->577.7372158337816 :)\n",
            "The loss function for the iteration 720----->566.4970508394778 :)\n",
            "The loss function for the iteration 730----->555.4836722218944 :)\n",
            "The loss function for the iteration 740----->544.6923900887897 :)\n",
            "The loss function for the iteration 750----->534.118614515036 :)\n",
            "The loss function for the iteration 760----->523.7578532976833 :)\n",
            "The loss function for the iteration 770----->513.6057097679784 :)\n",
            "The loss function for the iteration 780----->503.6578806584311 :)\n",
            "The loss function for the iteration 790----->493.91015402306726 :)\n",
            "The loss function for the iteration 800----->484.35840720913154 :)\n",
            "The loss function for the iteration 810----->474.99860487861173 :)\n",
            "The loss function for the iteration 820----->465.82679707799684 :)\n",
            "The loss function for the iteration 830----->456.83911735480865 :)\n",
            "The loss function for the iteration 840----->448.0317809194669 :)\n",
            "The loss function for the iteration 850----->439.40108285115485 :)\n",
            "The loss function for the iteration 860----->430.9433963464045 :)\n",
            "The loss function for the iteration 870----->422.6551710091405 :)\n",
            "The loss function for the iteration 880----->414.53293118103477 :)\n",
            "The loss function for the iteration 890----->406.57327431102 :)\n",
            "The loss function for the iteration 900----->398.77286936288783 :)\n",
            "The loss function for the iteration 910----->391.1284552599265 :)\n",
            "The loss function for the iteration 920----->383.63683936559215 :)\n",
            "The loss function for the iteration 930----->376.2948959992509 :)\n",
            "The loss function for the iteration 940----->369.09956498606823 :)\n",
            "The loss function for the iteration 950----->362.0478502401363 :)\n",
            "The loss function for the iteration 960----->355.1368183799877 :)\n",
            "The loss function for the iteration 970----->348.36359737564914 :)\n",
            "The loss function for the iteration 980----->341.72537522643927 :)\n",
            "The loss function for the iteration 990----->335.21939866872117 :)\n",
            "The loss function for the iteration 1000----->328.8429719128559 :)\n",
            "The loss function for the iteration 1010----->322.5934554086314 :)\n",
            "The loss function for the iteration 1020----->316.46826463845304 :)\n",
            "The loss function for the iteration 1030----->310.46486893761113 :)\n",
            "The loss function for the iteration 1040----->304.58079034096363 :)\n",
            "The loss function for the iteration 1050----->298.81360245538656 :)\n",
            "The loss function for the iteration 1060----->293.1609293573568 :)\n",
            "The loss function for the iteration 1070----->287.62044451508365 :)\n",
            "The loss function for the iteration 1080----->282.18986973456714 :)\n",
            "The loss function for the iteration 1090----->276.86697412903754 :)\n",
            "The loss function for the iteration 1100----->271.64957311120133 :)\n",
            "The loss function for the iteration 1110----->266.53552740775876 :)\n",
            "The loss function for the iteration 1120----->261.522742095669 :)\n",
            "The loss function for the iteration 1130----->256.6091656596371 :)\n",
            "The loss function for the iteration 1140----->251.79278907034703 :)\n",
            "The loss function for the iteration 1150----->247.07164488293117 :)\n",
            "The loss function for the iteration 1160----->242.44380635522123 :)\n",
            "The loss function for the iteration 1170----->237.90738658531544 :)\n",
            "The loss function for the iteration 1180----->233.4605376680139 :)\n",
            "The loss function for the iteration 1190----->229.10144986967885 :)\n",
            "The loss function for the iteration 1200----->224.82835082111754 :)\n",
            "The loss function for the iteration 1210----->220.63950472804555 :)\n",
            "The loss function for the iteration 1220----->216.53321159874778 :)\n",
            "The loss function for the iteration 1230----->212.50780648853322 :)\n",
            "The loss function for the iteration 1240----->208.56165876060643 :)\n",
            "The loss function for the iteration 1250----->204.69317136297983 :)\n",
            "The loss function for the iteration 1260----->200.9007801210617 :)\n",
            "The loss function for the iteration 1270----->197.18295304556358 :)\n",
            "The loss function for the iteration 1280----->193.5381896553883 :)\n",
            "The loss function for the iteration 1290----->189.9650203151472 :)\n",
            "The loss function for the iteration 1300----->186.4620055869912 :)\n",
            "The loss function for the iteration 1310----->183.0277355964213 :)\n",
            "The loss function for the iteration 1320----->179.6608294117779 :)\n",
            "The loss function for the iteration 1330----->176.35993443708742 :)\n",
            "The loss function for the iteration 1340----->173.12372581798044 :)\n",
            "The loss function for the iteration 1350----->169.95090586038262 :)\n",
            "The loss function for the iteration 1360----->166.84020346169623 :)\n",
            "The loss function for the iteration 1370----->163.79037355419015 :)\n",
            "The loss function for the iteration 1380----->160.8001965603308 :)\n",
            "The loss function for the iteration 1390----->157.86847785979492 :)\n",
            "The loss function for the iteration 1400----->154.99404726788634 :)\n",
            "The loss function for the iteration 1410----->152.17575852512522 :)\n",
            "The loss function for the iteration 1420----->149.41248879774975 :)\n",
            "The loss function for the iteration 1430----->146.70313818889474 :)\n",
            "The loss function for the iteration 1440----->144.0466292602127 :)\n",
            "The loss function for the iteration 1450----->141.44190656369634 :)\n",
            "The loss function for the iteration 1460----->138.8879361834955 :)\n",
            "The loss function for the iteration 1470----->136.38370528749752 :)\n",
            "The loss function for the iteration 1480----->133.9282216884555 :)\n",
            "The loss function for the iteration 1490----->131.52051341446196 :)\n",
            "The loss function for the iteration 1500----->129.15962828855945 :)\n",
            "The loss function for the iteration 1510----->126.84463351728776 :)\n",
            "The loss function for the iteration 1520----->124.57461528797106 :)\n",
            "The loss function for the iteration 1530----->122.34867837456275 :)\n",
            "The loss function for the iteration 1540----->120.16594575184627 :)\n",
            "The loss function for the iteration 1550----->118.02555821782337 :)\n",
            "The loss function for the iteration 1560----->115.92667402410332 :)\n",
            "The loss function for the iteration 1570----->113.86846851412709 :)\n",
            "The loss function for the iteration 1580----->111.85013376904335 :)\n",
            "The loss function for the iteration 1590----->109.87087826108284 :)\n",
            "The loss function for the iteration 1600----->107.92992651426175 :)\n",
            "The loss function for the iteration 1610----->106.02651877225745 :)\n",
            "The loss function for the iteration 1620----->104.15991067329814 :)\n",
            "The loss function for the iteration 1630----->102.32937293191881 :)\n",
            "The loss function for the iteration 1640----->100.53419102743267 :)\n",
            "The loss function for the iteration 1650----->98.7736648989714 :)\n",
            "The loss function for the iteration 1660----->97.0471086469567 :)\n",
            "The loss function for the iteration 1670----->95.35385024085896 :)\n",
            "The loss function for the iteration 1680----->93.69323123311156 :)\n",
            "The loss function for the iteration 1690----->92.0646064790471 :)\n",
            "The loss function for the iteration 1700----->90.46734386272504 :)\n",
            "The loss function for the iteration 1710----->88.90082402852208 :)\n",
            "The loss function for the iteration 1720----->87.36444011836872 :)\n",
            "The loss function for the iteration 1730----->85.85759751449969 :)\n",
            "The loss function for the iteration 1740----->84.37971358761082 :)\n",
            "The loss function for the iteration 1750----->82.93021745029718 :)\n",
            "The loss function for the iteration 1760----->81.50854971566481 :)\n",
            "The loss function for the iteration 1770----->80.11416226100438 :)\n",
            "The loss function for the iteration 1780----->78.74651799641418 :)\n",
            "The loss function for the iteration 1790----->77.4050906382756 :)\n",
            "The loss function for the iteration 1800----->76.08936448746326 :)\n",
            "The loss function for the iteration 1810----->74.7988342122049 :)\n",
            "The loss function for the iteration 1820----->73.53300463547572 :)\n",
            "The loss function for the iteration 1830----->72.29139052684506 :)\n",
            "The loss function for the iteration 1840----->71.07351639866773 :)\n",
            "The loss function for the iteration 1850----->69.87891630653625 :)\n",
            "The loss function for the iteration 1860----->68.70713365389669 :)\n",
            "The loss function for the iteration 1870----->67.55772100074648 :)\n",
            "The loss function for the iteration 1880----->66.43023987631928 :)\n",
            "The loss function for the iteration 1890----->65.32426059567779 :)\n",
            "The loss function for the iteration 1900----->64.23936208012887 :)\n",
            "The loss function for the iteration 1910----->63.17513168138149 :)\n",
            "The loss function for the iteration 1920----->62.1311650093649 :)\n",
            "The loss function for the iteration 1930----->61.10706576363235 :)\n",
            "The loss function for the iteration 1940----->60.10244556827212 :)\n",
            "The loss function for the iteration 1950----->59.11692381025379 :)\n",
            "The loss function for the iteration 1960----->58.15012748113319 :)\n",
            "The loss function for the iteration 1970----->57.201691022049474 :)\n",
            "The loss function for the iteration 1980----->56.2712561719392 :)\n",
            "The loss function for the iteration 1990----->55.358471818905734 :)\n",
            "The loss function for the iteration 2000----->54.46299385466954 :)\n",
            "The loss function for the iteration 2010----->53.58448503204271 :)\n",
            "The loss function for the iteration 2020----->52.72261482535458 :)\n",
            "The loss function for the iteration 2030----->51.877059293775055 :)\n",
            "The loss function for the iteration 2040----->51.047500947467235 :)\n",
            "The loss function for the iteration 2050----->50.233628616513776 :)\n",
            "The loss function for the iteration 2060----->49.435137322556756 :)\n",
            "The loss function for the iteration 2070----->48.65172815309424 :)\n",
            "The loss function for the iteration 2080----->47.88310813837845 :)\n",
            "The loss function for the iteration 2090----->47.12899013085677 :)\n",
            "The loss function for the iteration 2100----->46.389092687109375 :)\n",
            "The loss function for the iteration 2110----->45.66313995222189 :)\n",
            "The loss function for the iteration 2120----->44.95086154654911 :)\n",
            "The loss function for the iteration 2130----->44.25199245481701 :)\n",
            "The loss function for the iteration 2140----->43.56627291751222 :)\n",
            "The loss function for the iteration 2150----->42.893448324511354 :)\n",
            "The loss function for the iteration 2160----->42.23326911090787 :)\n",
            "The loss function for the iteration 2170----->41.58549065498219 :)\n",
            "The loss function for the iteration 2180----->40.94987317827601 :)\n",
            "The loss function for the iteration 2190----->40.32618164772691 :)\n",
            "The loss function for the iteration 2200----->39.71418567981375 :)\n",
            "The loss function for the iteration 2210----->39.113659446678476 :)\n",
            "The loss function for the iteration 2220----->38.5243815841791 :)\n",
            "The loss function for the iteration 2230----->37.946135101832425 :)\n",
            "The loss function for the iteration 2240----->37.37870729460863 :)\n",
            "The loss function for the iteration 2250----->36.82188965653841 :)\n",
            "The loss function for the iteration 2260----->36.27547779609718 :)\n",
            "The loss function for the iteration 2270----->35.73927135332254 :)\n",
            "The loss function for the iteration 2280----->35.2130739186369 :)\n",
            "The loss function for the iteration 2290----->34.696692953334214 :)\n",
            "The loss function for the iteration 2300----->34.189939711697264 :)\n",
            "The loss function for the iteration 2310----->33.69262916471282 :)\n",
            "The loss function for the iteration 2320----->33.20457992534995 :)\n",
            "The loss function for the iteration 2330----->32.72561417536927 :)\n",
            "The loss function for the iteration 2340----->32.25555759363018 :)\n",
            "The loss function for the iteration 2350----->31.794239285868017 :)\n",
            "The loss function for the iteration 2360----->31.341491715905665 :)\n",
            "The loss function for the iteration 2370----->30.897150638274404 :)\n",
            "The loss function for the iteration 2380----->30.461055032212304 :)\n",
            "The loss function for the iteration 2390----->30.03304703700986 :)\n",
            "The loss function for the iteration 2400----->29.612971888679866 :)\n",
            "The loss function for the iteration 2410----->29.20067785791833 :)\n",
            "The loss function for the iteration 2420----->28.796016189332573 :)\n",
            "The loss function for the iteration 2430----->28.398841041912075 :)\n",
            "The loss function for the iteration 2440----->28.009009430709572 :)\n",
            "The loss function for the iteration 2450----->27.626381169715966 :)\n",
            "The loss function for the iteration 2460----->27.25081881589683 :)\n",
            "The loss function for the iteration 2470----->26.882187614370547 :)\n",
            "The loss function for the iteration 2480----->26.520355444704066 :)\n",
            "The loss function for the iteration 2490----->26.165192768300365 :)\n",
            "The loss function for the iteration 2500----->25.816572576858935 :)\n",
            "The loss function for the iteration 2510----->25.474370341883862 :)\n",
            "The loss function for the iteration 2520----->25.138463965219394 :)\n",
            "The loss function for the iteration 2530----->24.808733730591413 :)\n",
            "The loss function for the iteration 2540----->24.48506225613335 :)\n",
            "The loss function for the iteration 2550----->24.167334447877664 :)\n",
            "The loss function for the iteration 2560----->23.85543745419062 :)\n",
            "The loss function for the iteration 2570----->23.54926062113375 :)\n",
            "The loss function for the iteration 2580----->23.24869544873028 :)\n",
            "The loss function for the iteration 2590----->22.953635548118882 :)\n",
            "The loss function for the iteration 2600----->22.663976599576664 :)\n",
            "The loss function for the iteration 2610----->22.379616311393036 :)\n",
            "The loss function for the iteration 2620----->22.100454379577336 :)\n",
            "The loss function for the iteration 2630----->21.82639244838218 :)\n",
            "The loss function for the iteration 2640----->21.55733407162633 :)\n",
            "The loss function for the iteration 2650----->21.29318467480101 :)\n",
            "The loss function for the iteration 2660----->21.033851517942296 :)\n",
            "The loss function for the iteration 2670----->20.779243659254647 :)\n",
            "The loss function for the iteration 2680----->20.529271919471086 :)\n",
            "The loss function for the iteration 2690----->20.283848846931917 :)\n",
            "The loss function for the iteration 2700----->20.042888683369913 :)\n",
            "The loss function for the iteration 2710----->19.80630733038728 :)\n",
            "The loss function for the iteration 2720----->19.574022316607802 :)\n",
            "The loss function for the iteration 2730----->19.34595276549392 :)\n",
            "The loss function for the iteration 2740----->19.122019363810836 :)\n",
            "The loss function for the iteration 2750----->18.902144330729165 :)\n",
            "The loss function for the iteration 2760----->18.68625138754747 :)\n",
            "The loss function for the iteration 2770----->18.474265728027614 :)\n",
            "The loss function for the iteration 2780----->18.26611398932635 :)\n",
            "The loss function for the iteration 2790----->18.06172422351298 :)\n",
            "The loss function for the iteration 2800----->17.861025869658775 :)\n",
            "The loss function for the iteration 2810----->17.663949726490472 :)\n",
            "The loss function for the iteration 2820----->17.47042792559118 :)\n",
            "The loss function for the iteration 2830----->17.280393905142446 :)\n",
            "The loss function for the iteration 2840----->17.093782384191805 :)\n",
            "The loss function for the iteration 2850----->16.910529337440344 :)\n",
            "The loss function for the iteration 2860----->16.73057197053309 :)\n",
            "The loss function for the iteration 2870----->16.553848695848245 :)\n",
            "The loss function for the iteration 2880----->16.38029910877066 :)\n",
            "The loss function for the iteration 2890----->16.20986396444246 :)\n",
            "The loss function for the iteration 2900----->16.04248515497873 :)\n",
            "The loss function for the iteration 2910----->15.878105687141604 :)\n",
            "The loss function for the iteration 2920----->15.716669660458917 :)\n",
            "The loss function for the iteration 2930----->15.55812224578414 :)\n",
            "The loss function for the iteration 2940----->15.402409664284672 :)\n",
            "The loss function for the iteration 2950----->15.249479166849127 :)\n",
            "The loss function for the iteration 2960----->15.099279013909017 :)\n",
            "The loss function for the iteration 2970----->14.951758455662157 :)\n",
            "The loss function for the iteration 2980----->14.806867712692831 :)\n",
            "The loss function for the iteration 2990----->14.66455795697794 :)\n",
            "The loss function for the iteration 3000----->14.524781293273502 :)\n",
            "The loss function for the iteration 3010----->14.387490740873396 :)\n",
            "The loss function for the iteration 3020----->14.252640215730047 :)\n",
            "The loss function for the iteration 3030----->14.120184512935037 :)\n",
            "The loss function for the iteration 3040----->13.990079289545335 :)\n",
            "The loss function for the iteration 3050----->13.862281047756639 :)\n",
            "The loss function for the iteration 3060----->13.736747118407411 :)\n",
            "The loss function for the iteration 3070----->13.613435644815205 :)\n",
            "The loss function for the iteration 3080----->13.492305566931769 :)\n",
            "The loss function for the iteration 3090----->13.373316605817504 :)\n",
            "The loss function for the iteration 3100----->13.256429248420895 :)\n",
            "The loss function for the iteration 3110----->13.14160473266354 :)\n",
            "The loss function for the iteration 3120----->13.02880503281979 :)\n",
            "The loss function for the iteration 3130----->12.91799284518913 :)\n",
            "The loss function for the iteration 3140----->12.809131574051166 :)\n",
            "The loss function for the iteration 3150----->12.702185317901296 :)\n",
            "The loss function for the iteration 3160----->12.597118855958302 :)\n",
            "The loss function for the iteration 3170----->12.493897634940769 :)\n",
            "The loss function for the iteration 3180----->12.392487756105265 :)\n",
            "The loss function for the iteration 3190----->12.292855962541754 :)\n",
            "The loss function for the iteration 3200----->12.194969626721171 :)\n",
            "The loss function for the iteration 3210----->12.098796738289392 :)\n",
            "The loss function for the iteration 3220----->12.00430589210244 :)\n",
            "The loss function for the iteration 3230----->11.91146627650069 :)\n",
            "The loss function for the iteration 3240----->11.82024766181223 :)\n",
            "The loss function for the iteration 3250----->11.730620389086372 :)\n",
            "The loss function for the iteration 3260----->11.642555359048224 :)\n",
            "The loss function for the iteration 3270----->11.556024021272677 :)\n",
            "The loss function for the iteration 3280----->11.470998363571447 :)\n",
            "The loss function for the iteration 3290----->11.387450901590693 :)\n",
            "The loss function for the iteration 3300----->11.305354668613798 :)\n",
            "The loss function for the iteration 3310----->11.224683205564878 :)\n",
            "The loss function for the iteration 3320----->11.145410551210041 :)\n",
            "The loss function for the iteration 3330----->11.067511232552064 :)\n",
            "The loss function for the iteration 3340----->10.990960255414397 :)\n",
            "The loss function for the iteration 3350----->10.915733095209614 :)\n",
            "The loss function for the iteration 3360----->10.841805687892354 :)\n",
            "The loss function for the iteration 3370----->10.769154421087084 :)\n",
            "The loss function for the iteration 3380----->10.697756125393738 :)\n",
            "The loss function for the iteration 3390----->10.62758806586163 :)\n",
            "The loss function for the iteration 3400----->10.55862793363318 :)\n",
            "The loss function for the iteration 3410----->10.49085383775142 :)\n",
            "The loss function for the iteration 3420----->10.424244297127302 :)\n",
            "The loss function for the iteration 3430----->10.358778232666323 :)\n",
            "The loss function for the iteration 3440----->10.294434959549616 :)\n",
            "The loss function for the iteration 3450----->10.231194179665394 :)\n",
            "The loss function for the iteration 3460----->10.169035974190617 :)\n",
            "The loss function for the iteration 3470----->10.107940796316093 :)\n",
            "The loss function for the iteration 3480----->10.047889464116661 :)\n",
            "The loss function for the iteration 3490----->9.988863153559407 :)\n",
            "The loss function for the iteration 3500----->9.930843391648398 :)\n",
            "The loss function for the iteration 3510----->9.87381204970682 :)\n",
            "The loss function for the iteration 3520----->9.817751336785873 :)\n",
            "The loss function for the iteration 3530----->9.762643793206962 :)\n",
            "The loss function for the iteration 3540----->9.708472284227737 :)\n",
            "The loss function for the iteration 3550----->9.65521999383295 :)\n",
            "The loss function for the iteration 3560----->9.6028704186458 :)\n",
            "The loss function for the iteration 3570----->9.551407361959896 :)\n",
            "The loss function for the iteration 3580----->9.50081492788561 :)\n",
            "The loss function for the iteration 3590----->9.451077515612933 :)\n",
            "The loss function for the iteration 3600----->9.402179813783992 :)\n",
            "The loss function for the iteration 3610----->9.354106794977564 :)\n",
            "The loss function for the iteration 3620----->9.30684371029935 :)\n",
            "The loss function for the iteration 3630----->9.260376084078699 :)\n",
            "The loss function for the iteration 3640----->9.214689708668384 :)\n",
            "The loss function for the iteration 3650----->9.169770639345641 :)\n",
            "The loss function for the iteration 3660----->9.125605189312422 :)\n",
            "The loss function for the iteration 3670----->9.08217992479306 :)\n",
            "The loss function for the iteration 3680----->9.039481660227054 :)\n",
            "The loss function for the iteration 3690----->8.997497453555294 :)\n",
            "The loss function for the iteration 3700----->8.956214601598944 :)\n",
            "The loss function for the iteration 3710----->8.915620635525755 :)\n",
            "The loss function for the iteration 3720----->8.875703316407577 :)\n",
            "The loss function for the iteration 3730----->8.836450630860806 :)\n",
            "The loss function for the iteration 3740----->8.79785078677454 :)\n",
            "The loss function for the iteration 3750----->8.759892209118938 :)\n",
            "The loss function for the iteration 3760----->8.722563535836201 :)\n",
            "The loss function for the iteration 3770----->8.685853613811597 :)\n",
            "The loss function for the iteration 3780----->8.649751494921066 :)\n",
            "The loss function for the iteration 3790----->8.614246432156623 :)\n",
            "The loss function for the iteration 3800----->8.579327875826582 :)\n",
            "The loss function for the iteration 3810----->8.544985469828191 :)\n",
            "The loss function for the iteration 3820----->8.51120904799413 :)\n",
            "The loss function for the iteration 3830----->8.477988630507891 :)\n",
            "The loss function for the iteration 3840----->8.445314420390064 :)\n",
            "The loss function for the iteration 3850----->8.413176800050739 :)\n",
            "The loss function for the iteration 3860----->8.381566327909978 :)\n",
            "The loss function for the iteration 3870----->8.350473735082973 :)\n",
            "The loss function for the iteration 3880----->8.319889922128727 :)\n",
            "The loss function for the iteration 3890----->8.289805955861683 :)\n",
            "The loss function for the iteration 3900----->8.260213066225504 :)\n",
            "The loss function for the iteration 3910----->8.231102643226166 :)\n",
            "The loss function for the iteration 3920----->8.20246623392456 :)\n",
            "The loss function for the iteration 3930----->8.174295539486282 :)\n",
            "The loss function for the iteration 3940----->8.146582412289726 :)\n",
            "The loss function for the iteration 3950----->8.1193188530882 :)\n",
            "The loss function for the iteration 3960----->8.092497008227705 :)\n",
            "The loss function for the iteration 3970----->8.066109166917217 :)\n",
            "The loss function for the iteration 3980----->8.040147758552244 :)\n",
            "The loss function for the iteration 3990----->8.014605350089454 :)\n",
            "The loss function for the iteration 4000----->7.989474643471774 :)\n",
            "The loss function for the iteration 4010----->7.964748473102737 :)\n",
            "The loss function for the iteration 4020----->7.940419803369014 :)\n",
            "The loss function for the iteration 4030----->7.916481726211327 :)\n",
            "The loss function for the iteration 4040----->7.892927458741029 :)\n",
            "The loss function for the iteration 4050----->7.869750340902591 :)\n",
            "The loss function for the iteration 4060----->7.846943833181142 :)\n",
            "The loss function for the iteration 4070----->7.824501514353316 :)\n",
            "The loss function for the iteration 4080----->7.802417079281703 :)\n",
            "The loss function for the iteration 4090----->7.780684336751065 :)\n",
            "The loss function for the iteration 4100----->7.759297207346119 :)\n",
            "The loss function for the iteration 4110----->7.73824972136963 :)\n",
            "The loss function for the iteration 4120----->7.71753601680077 :)\n",
            "The loss function for the iteration 4130----->7.697150337291441 :)\n",
            "The loss function for the iteration 4140----->7.677087030202395 :)\n",
            "The loss function for the iteration 4150----->7.657340544675039 :)\n",
            "The loss function for the iteration 4160----->7.637905429741112 :)\n",
            "The loss function for the iteration 4170----->7.618776332468496 :)\n",
            "The loss function for the iteration 4180----->7.599947996141319 :)\n",
            "The loss function for the iteration 4190----->7.5814152584758805 :)\n",
            "The loss function for the iteration 4200----->7.563173049869716 :)\n",
            "The loss function for the iteration 4210----->7.545216391684247 :)\n",
            "The loss function for the iteration 4220----->7.5275403945600665 :)\n",
            "The loss function for the iteration 4230----->7.510140256764398 :)\n",
            "The loss function for the iteration 4240----->7.4930112625696506 :)\n",
            "The loss function for the iteration 4250----->7.4761487806630775 :)\n",
            "The loss function for the iteration 4260----->7.459548262586079 :)\n",
            "The loss function for the iteration 4270----->7.443205241203941 :)\n",
            "The loss function for the iteration 4280----->7.427115329203712 :)\n",
            "The loss function for the iteration 4290----->7.411274217621535 :)\n",
            "The loss function for the iteration 4300----->7.395677674396534 :)\n",
            "The loss function for the iteration 4310----->7.380321542953514 :)\n",
            "The loss function for the iteration 4320----->7.365201740811701 :)\n",
            "The loss function for the iteration 4330----->7.350314258219704 :)\n",
            "The loss function for the iteration 4340----->7.335655156816914 :)\n",
            "The loss function for the iteration 4350----->7.321220568319314 :)\n",
            "The loss function for the iteration 4360----->7.307006693231399 :)\n",
            "The loss function for the iteration 4370----->7.293009799580807 :)\n",
            "The loss function for the iteration 4380----->7.279226221678336 :)\n",
            "The loss function for the iteration 4390----->7.265652358900567 :)\n",
            "The loss function for the iteration 4400----->7.25228467449541 :)\n",
            "The loss function for the iteration 4410----->7.2391196944104514 :)\n",
            "The loss function for the iteration 4420----->7.226154006143501 :)\n",
            "The loss function for the iteration 4430----->7.213384257614258 :)\n",
            "The loss function for the iteration 4440----->7.200807156057794 :)\n",
            "The loss function for the iteration 4450----->7.188419466938518 :)\n",
            "The loss function for the iteration 4460----->7.17621801288476 :)\n",
            "The loss function for the iteration 4470----->7.164199672642955 :)\n",
            "The loss function for the iteration 4480----->7.152361380052397 :)\n",
            "The loss function for the iteration 4490----->7.140700123038078 :)\n",
            "The loss function for the iteration 4500----->7.129212942623494 :)\n",
            "The loss function for the iteration 4510----->7.117896931961155 :)\n",
            "The loss function for the iteration 4520----->7.106749235382022 :)\n",
            "The loss function for the iteration 4530----->7.095767047462161 :)\n",
            "The loss function for the iteration 4540----->7.084947612107027 :)\n",
            "The loss function for the iteration 4550----->7.07428822165328 :)\n",
            "The loss function for the iteration 4560----->7.063786215987076 :)\n",
            "The loss function for the iteration 4570----->7.053438981678698 :)\n",
            "The loss function for the iteration 4580----->7.043243951133922 :)\n",
            "The loss function for the iteration 4590----->7.033198601760667 :)\n",
            "The loss function for the iteration 4600----->7.023300455151599 :)\n",
            "The loss function for the iteration 4610----->7.013547076282052 :)\n",
            "The loss function for the iteration 4620----->7.003936072722597 :)\n",
            "The loss function for the iteration 4630----->6.994465093866422 :)\n",
            "The loss function for the iteration 4640----->6.985131830171389 :)\n",
            "The loss function for the iteration 4650----->6.975934012415886 :)\n",
            "The loss function for the iteration 4660----->6.966869410968878 :)\n",
            "The loss function for the iteration 4670----->6.9579358350730285 :)\n",
            "The loss function for the iteration 4680----->6.949131132141739 :)\n",
            "The loss function for the iteration 4690----->6.9404531870692505 :)\n",
            "The loss function for the iteration 4700----->6.9318999215530015 :)\n",
            "The loss function for the iteration 4710----->6.923469293429148 :)\n",
            "The loss function for the iteration 4720----->6.915159296020208 :)\n",
            "The loss function for the iteration 4730----->6.906967957494869 :)\n",
            "The loss function for the iteration 4740----->6.898893340239846 :)\n",
            "The loss function for the iteration 4750----->6.890933540243001 :)\n",
            "The loss function for the iteration 4760----->6.883086686488259 :)\n",
            "The loss function for the iteration 4770----->6.875350940361431 :)\n",
            "The loss function for the iteration 4780----->6.8677244950678356 :)\n",
            "The loss function for the iteration 4790----->6.860205575059423 :)\n",
            "The loss function for the iteration 4800----->6.852792435473571 :)\n",
            "The loss function for the iteration 4810----->6.8454833615818735 :)\n",
            "The loss function for the iteration 4820----->6.838276668249106 :)\n",
            "The loss function for the iteration 4830----->6.831170699402298 :)\n",
            "The loss function for the iteration 4840----->6.824163827509582 :)\n",
            "The loss function for the iteration 4850----->6.817254453068604 :)\n",
            "The loss function for the iteration 4860----->6.810441004104257 :)\n",
            "The loss function for the iteration 4870----->6.803721935676473 :)\n",
            "The loss function for the iteration 4880----->6.79709572939564 :)\n",
            "The loss function for the iteration 4890----->6.790560892948495 :)\n",
            "The loss function for the iteration 4900----->6.784115959631763 :)\n",
            "The loss function for the iteration 4910----->6.777759487894675 :)\n",
            "The loss function for the iteration 4920----->6.771490060889867 :)\n",
            "The loss function for the iteration 4930----->6.765306286032839 :)\n",
            "The loss function for the iteration 4940----->6.759206794568986 :)\n",
            "The loss function for the iteration 4950----->6.75319024114913 :)\n",
            "The loss function for the iteration 4960----->6.7472553034123175 :)\n",
            "The loss function for the iteration 4970----->6.741400681576789 :)\n",
            "The loss function for the iteration 4980----->6.735625098038172 :)\n",
            "The loss function for the iteration 4990----->6.729927296975092 :)\n",
            "The loss function for the iteration 5000----->6.724306043961988 :)\n",
            "The loss function for the iteration 5010----->6.71876012558908 :)\n",
            "The loss function for the iteration 5020----->6.713288349089382 :)\n",
            "The loss function for the iteration 5030----->6.707889541972262 :)\n",
            "The loss function for the iteration 5040----->6.702562551664171 :)\n",
            "The loss function for the iteration 5050----->6.697306245155375 :)\n",
            "The loss function for the iteration 5060----->6.692119508653572 :)\n",
            "The loss function for the iteration 5070----->6.6870012472439235 :)\n",
            "The loss function for the iteration 5080----->6.681950384554698 :)\n",
            "The loss function for the iteration 5090----->6.676965862429634 :)\n",
            "The loss function for the iteration 5100----->6.672046640606073 :)\n",
            "The loss function for the iteration 5110----->6.667191696398845 :)\n",
            "The loss function for the iteration 5120----->6.662400024390153 :)\n",
            "The loss function for the iteration 5130----->6.657670636124808 :)\n",
            "The loss function for the iteration 5140----->6.653002559811352 :)\n",
            "The loss function for the iteration 5150----->6.648394840028332 :)\n",
            "The loss function for the iteration 5160----->6.643846537436122 :)\n",
            "The loss function for the iteration 5170----->6.639356728493829 :)\n",
            "The loss function for the iteration 5180----->6.634924505181526 :)\n",
            "The loss function for the iteration 5190----->6.630548974727311 :)\n",
            "The loss function for the iteration 5200----->6.626229259339527 :)\n",
            "The loss function for the iteration 5210----->6.621964495943744 :)\n",
            "The loss function for the iteration 5220----->6.6177538359244785 :)\n",
            "The loss function for the iteration 5230----->6.613596444871659 :)\n",
            "The loss function for the iteration 5240----->6.609491502331709 :)\n",
            "The loss function for the iteration 5250----->6.605438201563039 :)\n",
            "The loss function for the iteration 5260----->6.601435749296135 :)\n",
            "The loss function for the iteration 5270----->6.597483365497654 :)\n",
            "The loss function for the iteration 5280----->6.593580283139359 :)\n",
            "The loss function for the iteration 5290----->6.5897257479705225 :)\n",
            "The loss function for the iteration 5300----->6.585919018295021 :)\n",
            "The loss function for the iteration 5310----->6.582159364752181 :)\n",
            "The loss function for the iteration 5320----->6.5784460701016805 :)\n",
            "The loss function for the iteration 5330----->6.5747784290122055 :)\n",
            "The loss function for the iteration 5340----->6.571155747854015 :)\n",
            "The loss function for the iteration 5350----->6.5675773444953665 :)\n",
            "The loss function for the iteration 5360----->6.56404254810225 :)\n",
            "The loss function for the iteration 5370----->6.5605506989420945 :)\n",
            "The loss function for the iteration 5380----->6.557101148190862 :)\n",
            "The loss function for the iteration 5390----->6.553693257743543 :)\n",
            "The loss function for the iteration 5400----->6.55032640002821 :)\n",
            "The loss function for the iteration 5410----->6.546999957823219 :)\n",
            "The loss function for the iteration 5420----->6.543713324077896 :)\n",
            "The loss function for the iteration 5430----->6.540465901736301 :)\n",
            "The loss function for the iteration 5440----->6.537257103564223 :)\n",
            "The loss function for the iteration 5450----->6.534086351979235 :)\n",
            "The loss function for the iteration 5460----->6.530953078883858 :)\n",
            "The loss function for the iteration 5470----->6.52785672550159 :)\n",
            "The loss function for the iteration 5480----->6.524796742216007 :)\n",
            "The loss function for the iteration 5490----->6.521772588412695 :)\n",
            "The loss function for the iteration 5500----->6.518783732324007 :)\n",
            "The loss function for the iteration 5510----->6.515829650876452 :)\n",
            "The loss function for the iteration 5520----->6.51290982954114 :)\n",
            "The loss function for the iteration 5530----->6.5100237621866 :)\n",
            "The loss function for the iteration 5540----->6.50717095093426 :)\n",
            "The loss function for the iteration 5550----->6.504350906016738 :)\n",
            "The loss function for the iteration 5560----->6.501563145638369 :)\n",
            "The loss function for the iteration 5570----->6.498807195838358 :)\n",
            "The loss function for the iteration 5580----->6.496082590356364 :)\n",
            "The loss function for the iteration 5590----->6.493388870500473 :)\n",
            "The loss function for the iteration 5600----->6.490725585017506 :)\n",
            "The loss function for the iteration 5610----->6.488092289965632 :)\n",
            "The loss function for the iteration 5620----->6.485488548589188 :)\n",
            "The loss function for the iteration 5630----->6.482913931195937 :)\n",
            "The loss function for the iteration 5640----->6.480368015036194 :)\n",
            "The loss function for the iteration 5650----->6.477850384184365 :)\n",
            "The loss function for the iteration 5660----->6.4753606294222905 :)\n",
            "The loss function for the iteration 5670----->6.472898348125151 :)\n",
            "The loss function for the iteration 5680----->6.470463144148736 :)\n",
            "The loss function for the iteration 5690----->6.468054627719419 :)\n",
            "The loss function for the iteration 5700----->6.465672415325286 :)\n",
            "The loss function for the iteration 5710----->6.463316129610061 :)\n",
            "The loss function for the iteration 5720----->6.460985399268088 :)\n",
            "The loss function for the iteration 5730----->6.458679858941824 :)\n",
            "The loss function for the iteration 5740----->6.456399149120651 :)\n",
            "The loss function for the iteration 5750----->6.454142916041821 :)\n",
            "The loss function for the iteration 5760----->6.4519108115929695 :)\n",
            "The loss function for the iteration 5770----->6.449702493216279 :)\n",
            "The loss function for the iteration 5780----->6.447517623814731 :)\n",
            "The loss function for the iteration 5790----->6.445355871659332 :)\n",
            "The loss function for the iteration 5800----->6.443216910298604 :)\n",
            "The loss function for the iteration 5810----->6.441100418469436 :)\n",
            "The loss function for the iteration 5820----->6.439006080009257 :)\n",
            "The loss function for the iteration 5830----->6.436933583770298 :)\n",
            "The loss function for the iteration 5840----->6.434882623534765 :)\n",
            "The loss function for the iteration 5850----->6.432852897931864 :)\n",
            "The loss function for the iteration 5860----->6.430844110356286 :)\n",
            "The loss function for the iteration 5870----->6.4288559688879054 :)\n",
            "The loss function for the iteration 5880----->6.4268881862130325 :)\n",
            "The loss function for the iteration 5890----->6.424940479547097 :)\n",
            "The loss function for the iteration 5900----->6.423012570558525 :)\n",
            "The loss function for the iteration 5910----->6.4211041852939985 :)\n",
            "The loss function for the iteration 5920----->6.419215054105177 :)\n",
            "The loss function for the iteration 5930----->6.417344911576487 :)\n",
            "The loss function for the iteration 5940----->6.415493496454193 :)\n",
            "The loss function for the iteration 5950----->6.413660551576959 :)\n",
            "The loss function for the iteration 5960----->6.411845823807248 :)\n",
            "The loss function for the iteration 5970----->6.410049063964214 :)\n",
            "The loss function for the iteration 5980----->6.408270026757514 :)\n",
            "The loss function for the iteration 5990----->6.406508470722658 :)\n",
            "The loss function for the iteration 6000----->6.404764158156976 :)\n",
            "The loss function for the iteration 6010----->6.403036855057062 :)\n",
            "The loss function for the iteration 6020----->6.401326331057261 :)\n",
            "The loss function for the iteration 6030----->6.3996323593690665 :)\n",
            "The loss function for the iteration 6040----->6.397954716721684 :)\n",
            "The loss function for the iteration 6050----->6.396293183303631 :)\n",
            "The loss function for the iteration 6060----->6.394647542705372 :)\n",
            "The loss function for the iteration 6070----->6.393017581862804 :)\n",
            "The loss function for the iteration 6080----->6.391403091001825 :)\n",
            "The loss function for the iteration 6090----->6.389803863583975 :)\n",
            "The loss function for the iteration 6100----->6.388219696252775 :)\n",
            "The loss function for the iteration 6110----->6.38665038878115 :)\n",
            "The loss function for the iteration 6120----->6.385095744019764 :)\n",
            "The loss function for the iteration 6130----->6.383555567846149 :)\n",
            "The loss function for the iteration 6140----->6.382029669114812 :)\n",
            "The loss function for the iteration 6150----->6.380517859608142 :)\n",
            "The loss function for the iteration 6160----->6.379019953988225 :)\n",
            "The loss function for the iteration 6170----->6.377535769749365 :)\n",
            "The loss function for the iteration 6180----->6.376065127171621 :)\n",
            "The loss function for the iteration 6190----->6.374607849274904 :)\n",
            "The loss function for the iteration 6200----->6.37316376177407 :)\n",
            "The loss function for the iteration 6210----->6.371732693034675 :)\n",
            "The loss function for the iteration 6220----->6.370314474029495 :)\n",
            "The loss function for the iteration 6230----->6.368908938295901 :)\n",
            "The loss function for the iteration 6240----->6.367515921893721 :)\n",
            "The loss function for the iteration 6250----->6.366135263364122 :)\n",
            "The loss function for the iteration 6260----->6.364766803688991 :)\n",
            "The loss function for the iteration 6270----->6.363410386251102 :)\n",
            "The loss function for the iteration 6280----->6.362065856794885 :)\n",
            "The loss function for the iteration 6290----->6.3607330633880474 :)\n",
            "The loss function for the iteration 6300----->6.35941185638351 :)\n",
            "The loss function for the iteration 6310----->6.358102088382441 :)\n",
            "The loss function for the iteration 6320----->6.356803614197468 :)\n",
            "The loss function for the iteration 6330----->6.355516290816979 :)\n",
            "The loss function for the iteration 6340----->6.354239977369545 :)\n",
            "The loss function for the iteration 6350----->6.3529745350893005 :)\n",
            "The loss function for the iteration 6360----->6.351719827281869 :)\n",
            "The loss function for the iteration 6370----->6.350475719290705 :)\n",
            "The loss function for the iteration 6380----->6.349242078464124 :)\n",
            "The loss function for the iteration 6390----->6.348018774122945 :)\n",
            "The loss function for the iteration 6400----->6.346805677528503 :)\n",
            "The loss function for the iteration 6410----->6.345602661851462 :)\n",
            "The loss function for the iteration 6420----->6.344409602140852 :)\n",
            "The loss function for the iteration 6430----->6.343226375293895 :)\n",
            "The loss function for the iteration 6440----->6.342052860026215 :)\n",
            "The loss function for the iteration 6450----->6.340888936842545 :)\n",
            "The loss function for the iteration 6460----->6.33973448800803 :)\n",
            "The loss function for the iteration 6470----->6.338589397519843 :)\n",
            "The loss function for the iteration 6480----->6.337453551079488 :)\n",
            "The loss function for the iteration 6490----->6.336326836065366 :)\n",
            "The loss function for the iteration 6500----->6.33520914150596 :)\n",
            "The loss function for the iteration 6510----->6.334100358053436 :)\n",
            "The loss function for the iteration 6520----->6.333000377957638 :)\n",
            "The loss function for the iteration 6530----->6.331909095040496 :)\n",
            "The loss function for the iteration 6540----->6.3308264046710585 :)\n",
            "The loss function for the iteration 6550----->6.329752203740667 :)\n",
            "The loss function for the iteration 6560----->6.3286863906387545 :)\n",
            "The loss function for the iteration 6570----->6.327628865228972 :)\n",
            "The loss function for the iteration 6580----->6.326579528825781 :)\n",
            "The loss function for the iteration 6590----->6.325538284171292 :)\n",
            "The loss function for the iteration 6600----->6.324505035412635 :)\n",
            "The loss function for the iteration 6610----->6.32347968807967 :)\n",
            "The loss function for the iteration 6620----->6.322462149062991 :)\n",
            "The loss function for the iteration 6630----->6.321452326592516 :)\n",
            "The loss function for the iteration 6640----->6.320450130216102 :)\n",
            "The loss function for the iteration 6650----->6.319455470778849 :)\n",
            "The loss function for the iteration 6660----->6.318468260402515 :)\n",
            "The loss function for the iteration 6670----->6.31748841246543 :)\n",
            "The loss function for the iteration 6680----->6.316515841582563 :)\n",
            "The loss function for the iteration 6690----->6.315550463586194 :)\n",
            "The loss function for the iteration 6700----->6.314592195506647 :)\n",
            "The loss function for the iteration 6710----->6.3136409555534225 :)\n",
            "The loss function for the iteration 6720----->6.312696663096757 :)\n",
            "The loss function for the iteration 6730----->6.3117592386493975 :)\n",
            "The loss function for the iteration 6740----->6.310828603848528 :)\n",
            "The loss function for the iteration 6750----->6.3099046814383515 :)\n",
            "The loss function for the iteration 6760----->6.308987395252632 :)\n",
            "The loss function for the iteration 6770----->6.308076670197697 :)\n",
            "The loss function for the iteration 6780----->6.30717243223571 :)\n",
            "The loss function for the iteration 6790----->6.306274608368064 :)\n",
            "The loss function for the iteration 6800----->6.3053831266193185 :)\n",
            "The loss function for the iteration 6810----->6.3044979160212 :)\n",
            "The loss function for the iteration 6820----->6.303618906596876 :)\n",
            "The loss function for the iteration 6830----->6.302746029345621 :)\n",
            "The loss function for the iteration 6840----->6.301879216227604 :)\n",
            "The loss function for the iteration 6850----->6.301018400148976 :)\n",
            "The loss function for the iteration 6860----->6.300163514947173 :)\n",
            "The loss function for the iteration 6870----->6.299314495376586 :)\n",
            "The loss function for the iteration 6880----->6.298471277094256 :)\n",
            "The loss function for the iteration 6890----->6.297633796645995 :)\n",
            "The loss function for the iteration 6900----->6.296801991452624 :)\n",
            "The loss function for the iteration 6910----->6.295975799796469 :)\n",
            "The loss function for the iteration 6920----->6.295155160808124 :)\n",
            "The loss function for the iteration 6930----->6.2943400144533275 :)\n",
            "The loss function for the iteration 6940----->6.293530301520142 :)\n",
            "The loss function for the iteration 6950----->6.292725963606346 :)\n",
            "The loss function for the iteration 6960----->6.291926943106936 :)\n",
            "The loss function for the iteration 6970----->6.291133183201981 :)\n",
            "The loss function for the iteration 6980----->6.290344627844528 :)\n",
            "The loss function for the iteration 6990----->6.289561221748764 :)\n",
            "The loss function for the iteration 7000----->6.28878291037848 :)\n",
            "The loss function for the iteration 7010----->6.288009639935477 :)\n",
            "The loss function for the iteration 7020----->6.287241357348411 :)\n",
            "The loss function for the iteration 7030----->6.2864780102617015 :)\n",
            "The loss function for the iteration 7040----->6.285719547024568 :)\n",
            "The loss function for the iteration 7050----->6.28496591668037 :)\n",
            "The loss function for the iteration 7060----->6.28421706895603 :)\n",
            "The loss function for the iteration 7070----->6.28347295425171 :)\n",
            "The loss function for the iteration 7080----->6.282733523630516 :)\n",
            "The loss function for the iteration 7090----->6.281998728808544 :)\n",
            "The loss function for the iteration 7100----->6.281268522144931 :)\n",
            "The loss function for the iteration 7110----->6.280542856632163 :)\n",
            "The loss function for the iteration 7120----->6.279821685886544 :)\n",
            "The loss function for the iteration 7130----->6.279104964138728 :)\n",
            "The loss function for the iteration 7140----->6.278392646224524 :)\n",
            "The loss function for the iteration 7150----->6.277684687575739 :)\n",
            "The loss function for the iteration 7160----->6.27698104421126 :)\n",
            "The loss function for the iteration 7170----->6.276281672728186 :)\n",
            "The loss function for the iteration 7180----->6.275586530293225 :)\n",
            "The loss function for the iteration 7190----->6.274895574634096 :)\n",
            "The loss function for the iteration 7200----->6.274208764031168 :)\n",
            "The loss function for the iteration 7210----->6.273526057309209 :)\n",
            "The loss function for the iteration 7220----->6.272847413829204 :)\n",
            "The loss function for the iteration 7230----->6.272172793480417 :)\n",
            "The loss function for the iteration 7240----->6.271502156672497 :)\n",
            "The loss function for the iteration 7250----->6.270835464327707 :)\n",
            "The loss function for the iteration 7260----->6.270172677873358 :)\n",
            "The loss function for the iteration 7270----->6.269513759234272 :)\n",
            "The loss function for the iteration 7280----->6.268858670825402 :)\n",
            "The loss function for the iteration 7290----->6.2682073755446 :)\n",
            "The loss function for the iteration 7300----->6.267559836765443 :)\n",
            "The loss function for the iteration 7310----->6.26691601833021 :)\n",
            "The loss function for the iteration 7320----->6.266275884542968 :)\n",
            "The loss function for the iteration 7330----->6.265639400162809 :)\n",
            "The loss function for the iteration 7340----->6.265006530397039 :)\n",
            "The loss function for the iteration 7350----->6.264377240894647 :)\n",
            "The loss function for the iteration 7360----->6.263751497739861 :)\n",
            "The loss function for the iteration 7370----->6.263129267445718 :)\n",
            "The loss function for the iteration 7380----->6.262510516947733 :)\n",
            "The loss function for the iteration 7390----->6.261895213597815 :)\n",
            "The loss function for the iteration 7400----->6.261283325158075 :)\n",
            "The loss function for the iteration 7410----->6.260674819794901 :)\n",
            "The loss function for the iteration 7420----->6.260069666073051 :)\n",
            "The loss function for the iteration 7430----->6.259467832949833 :)\n",
            "The loss function for the iteration 7440----->6.258869289769411 :)\n",
            "The loss function for the iteration 7450----->6.2582740062571895 :)\n",
            "The loss function for the iteration 7460----->6.257681952514221 :)\n",
            "The loss function for the iteration 7470----->6.257093099011856 :)\n",
            "The loss function for the iteration 7480----->6.256507416586286 :)\n",
            "The loss function for the iteration 7490----->6.255924876433344 :)\n",
            "The loss function for the iteration 7500----->6.255345450103287 :)\n",
            "The loss function for the iteration 7510----->6.254769109495624 :)\n",
            "The loss function for the iteration 7520----->6.254195826854214 :)\n",
            "The loss function for the iteration 7530----->6.25362557476215 :)\n",
            "The loss function for the iteration 7540----->6.253058326137062 :)\n",
            "The loss function for the iteration 7550----->6.252494054226145 :)\n",
            "The loss function for the iteration 7560----->6.251932732601565 :)\n",
            "The loss function for the iteration 7570----->6.251374335155727 :)\n",
            "The loss function for the iteration 7580----->6.25081883609677 :)\n",
            "The loss function for the iteration 7590----->6.250266209943963 :)\n",
            "The loss function for the iteration 7600----->6.2497164315233835 :)\n",
            "The loss function for the iteration 7610----->6.249169475963452 :)\n",
            "The loss function for the iteration 7620----->6.248625318690685 :)\n",
            "The loss function for the iteration 7630----->6.248083935425448 :)\n",
            "The loss function for the iteration 7640----->6.247545302177793 :)\n",
            "The loss function for the iteration 7650----->6.247009395243343 :)\n",
            "The loss function for the iteration 7660----->6.24647619119927 :)\n",
            "The loss function for the iteration 7670----->6.245945666900317 :)\n",
            "The loss function for the iteration 7680----->6.245417799474879 :)\n",
            "The loss function for the iteration 7690----->6.244892566321119 :)\n",
            "The loss function for the iteration 7700----->6.244369945103226 :)\n",
            "The loss function for the iteration 7710----->6.243849913747658 :)\n",
            "The loss function for the iteration 7720----->6.243332450439455 :)\n",
            "The loss function for the iteration 7730----->6.242817533618597 :)\n",
            "The loss function for the iteration 7740----->6.242305141976493 :)\n",
            "The loss function for the iteration 7750----->6.24179525445242 :)\n",
            "The loss function for the iteration 7760----->6.2412878502300835 :)\n",
            "The loss function for the iteration 7770----->6.240782908734243 :)\n",
            "The loss function for the iteration 7780----->6.240280409627272 :)\n",
            "The loss function for the iteration 7790----->6.239780332805939 :)\n",
            "The loss function for the iteration 7800----->6.239282658398159 :)\n",
            "The loss function for the iteration 7810----->6.238787366759725 :)\n",
            "The loss function for the iteration 7820----->6.238294438471218 :)\n",
            "The loss function for the iteration 7830----->6.237803854334873 :)\n",
            "The loss function for the iteration 7840----->6.237315595371527 :)\n",
            "The loss function for the iteration 7850----->6.2368296428176055 :)\n",
            "The loss function for the iteration 7860----->6.2363459781221975 :)\n",
            "The loss function for the iteration 7870----->6.235864582944068 :)\n",
            "The loss function for the iteration 7880----->6.2353854391488275 :)\n",
            "The loss function for the iteration 7890----->6.234908528806091 :)\n",
            "The loss function for the iteration 7900----->6.234433834186702 :)\n",
            "The loss function for the iteration 7910----->6.233961337759959 :)\n",
            "The loss function for the iteration 7920----->6.233491022190924 :)\n",
            "The loss function for the iteration 7930----->6.233022870337786 :)\n",
            "The loss function for the iteration 7940----->6.232556865249167 :)\n",
            "The loss function for the iteration 7950----->6.232092990161637 :)\n",
            "The loss function for the iteration 7960----->6.23163122849706 :)\n",
            "The loss function for the iteration 7970----->6.231171563860186 :)\n",
            "The loss function for the iteration 7980----->6.230713980036136 :)\n",
            "The loss function for the iteration 7990----->6.230258460987936 :)\n",
            "The loss function for the iteration 8000----->6.229804990854181 :)\n",
            "The loss function for the iteration 8010----->6.229353553946666 :)\n",
            "The loss function for the iteration 8020----->6.228904134748023 :)\n",
            "The loss function for the iteration 8030----->6.228456717909468 :)\n",
            "The loss function for the iteration 8040----->6.228011288248548 :)\n",
            "The loss function for the iteration 8050----->6.227567830746883 :)\n",
            "The loss function for the iteration 8060----->6.227126330548017 :)\n",
            "The loss function for the iteration 8070----->6.226686772955242 :)\n",
            "The loss function for the iteration 8080----->6.226249143429479 :)\n",
            "The loss function for the iteration 8090----->6.225813427587195 :)\n",
            "The loss function for the iteration 8100----->6.225379611198303 :)\n",
            "The loss function for the iteration 8110----->6.224947680184214 :)\n",
            "The loss function for the iteration 8120----->6.224517620615737 :)\n",
            "The loss function for the iteration 8130----->6.2240894187111895 :)\n",
            "The loss function for the iteration 8140----->6.223663060834401 :)\n",
            "The loss function for the iteration 8150----->6.223238533492843 :)\n",
            "The loss function for the iteration 8160----->6.2228158233356945 :)\n",
            "The loss function for the iteration 8170----->6.222394917152058 :)\n",
            "The loss function for the iteration 8180----->6.221975801869044 :)\n",
            "The loss function for the iteration 8190----->6.221558464550073 :)\n",
            "The loss function for the iteration 8200----->6.221142892392949 :)\n",
            "The loss function for the iteration 8210----->6.220729072728298 :)\n",
            "The loss function for the iteration 8220----->6.22031699301769 :)\n",
            "The loss function for the iteration 8230----->6.2199066408520025 :)\n",
            "The loss function for the iteration 8240----->6.219498003949742 :)\n",
            "The loss function for the iteration 8250----->6.2190910701554305 :)\n",
            "The loss function for the iteration 8260----->6.21868582743788 :)\n",
            "The loss function for the iteration 8270----->6.218282263888721 :)\n",
            "The loss function for the iteration 8280----->6.217880367720719 :)\n",
            "The loss function for the iteration 8290----->6.217480127266262 :)\n",
            "The loss function for the iteration 8300----->6.21708153097584 :)\n",
            "The loss function for the iteration 8310----->6.216684567416528 :)\n",
            "The loss function for the iteration 8320----->6.21628922527044 :)\n",
            "The loss function for the iteration 8330----->6.215895493333382 :)\n",
            "The loss function for the iteration 8340----->6.215503360513301 :)\n",
            "The loss function for the iteration 8350----->6.215112815828885 :)\n",
            "The loss function for the iteration 8360----->6.2147238484082 :)\n",
            "The loss function for the iteration 8370----->6.214336447487271 :)\n",
            "The loss function for the iteration 8380----->6.21395060240872 :)\n",
            "The loss function for the iteration 8390----->6.213566302620421 :)\n",
            "The loss function for the iteration 8400----->6.213183537674178 :)\n",
            "The loss function for the iteration 8410----->6.2128022972244645 :)\n",
            "The loss function for the iteration 8420----->6.212422571027058 :)\n",
            "The loss function for the iteration 8430----->6.2120443489378 :)\n",
            "The loss function for the iteration 8440----->6.211667620911402 :)\n",
            "The loss function for the iteration 8450----->6.211292377000135 :)\n",
            "The loss function for the iteration 8460----->6.210918607352637 :)\n",
            "The loss function for the iteration 8470----->6.210546302212744 :)\n",
            "The loss function for the iteration 8480----->6.210175451918296 :)\n",
            "The loss function for the iteration 8490----->6.209806046899969 :)\n",
            "The loss function for the iteration 8500----->6.209438077680116 :)\n",
            "The loss function for the iteration 8510----->6.209071534871633 :)\n",
            "The loss function for the iteration 8520----->6.208706409176916 :)\n",
            "The loss function for the iteration 8530----->6.208342691386671 :)\n",
            "The loss function for the iteration 8540----->6.2079803723788505 :)\n",
            "The loss function for the iteration 8550----->6.2076194431176415 :)\n",
            "The loss function for the iteration 8560----->6.207259894652339 :)\n",
            "The loss function for the iteration 8570----->6.206901718116373 :)\n",
            "The loss function for the iteration 8580----->6.206544904726239 :)\n",
            "The loss function for the iteration 8590----->6.206189445780526 :)\n",
            "The loss function for the iteration 8600----->6.205835332658852 :)\n",
            "The loss function for the iteration 8610----->6.205482556820994 :)\n",
            "The loss function for the iteration 8620----->6.205131109805833 :)\n",
            "The loss function for the iteration 8630----->6.204780983230409 :)\n",
            "The loss function for the iteration 8640----->6.204432168789051 :)\n",
            "The loss function for the iteration 8650----->6.204084658252345 :)\n",
            "The loss function for the iteration 8660----->6.20373844346632 :)\n",
            "The loss function for the iteration 8670----->6.203393516351467 :)\n",
            "The loss function for the iteration 8680----->6.2030498689019105 :)\n",
            "The loss function for the iteration 8690----->6.202707493184496 :)\n",
            "The loss function for the iteration 8700----->6.202366381337936 :)\n",
            "The loss function for the iteration 8710----->6.2020265255719815 :)\n",
            "The loss function for the iteration 8720----->6.201687918166513 :)\n",
            "The loss function for the iteration 8730----->6.201350551470813 :)\n",
            "The loss function for the iteration 8740----->6.201014417902641 :)\n",
            "The loss function for the iteration 8750----->6.200679509947539 :)\n",
            "The loss function for the iteration 8760----->6.200345820157935 :)\n",
            "The loss function for the iteration 8770----->6.200013341152416 :)\n",
            "The loss function for the iteration 8780----->6.199682065614958 :)\n",
            "The loss function for the iteration 8790----->6.199351986294115 :)\n",
            "The loss function for the iteration 8800----->6.199023096002333 :)\n",
            "The loss function for the iteration 8810----->6.198695387615161 :)\n",
            "The loss function for the iteration 8820----->6.198368854070546 :)\n",
            "The loss function for the iteration 8830----->6.198043488368064 :)\n",
            "The loss function for the iteration 8840----->6.1977192835682935 :)\n",
            "The loss function for the iteration 8850----->6.197396232792022 :)\n",
            "The loss function for the iteration 8860----->6.197074329219637 :)\n",
            "The loss function for the iteration 8870----->6.196753566090355 :)\n",
            "The loss function for the iteration 8880----->6.19643393670163 :)\n",
            "The loss function for the iteration 8890----->6.196115434408444 :)\n",
            "The loss function for the iteration 8900----->6.195798052622665 :)\n",
            "The loss function for the iteration 8910----->6.195481784812378 :)\n",
            "The loss function for the iteration 8920----->6.195166624501294 :)\n",
            "The loss function for the iteration 8930----->6.194852565268053 :)\n",
            "The loss function for the iteration 8940----->6.194539600745696 :)\n",
            "The loss function for the iteration 8950----->6.19422772462093 :)\n",
            "The loss function for the iteration 8960----->6.193916930633675 :)\n",
            "The loss function for the iteration 8970----->6.193607212576303 :)\n",
            "The loss function for the iteration 8980----->6.193298564293199 :)\n",
            "The loss function for the iteration 8990----->6.192990979680074 :)\n",
            "The loss function for the iteration 9000----->6.192684452683428 :)\n",
            "The loss function for the iteration 9010----->6.192378977300025 :)\n",
            "The loss function for the iteration 9020----->6.192074547576259 :)\n",
            "The loss function for the iteration 9030----->6.191771157607684 :)\n",
            "The loss function for the iteration 9040----->6.191468801538398 :)\n",
            "The loss function for the iteration 9050----->6.191167473560534 :)\n",
            "The loss function for the iteration 9060----->6.1908671679138 :)\n",
            "The loss function for the iteration 9070----->6.190567878884827 :)\n",
            "The loss function for the iteration 9080----->6.190269600806757 :)\n",
            "The loss function for the iteration 9090----->6.189972328058701 :)\n",
            "The loss function for the iteration 9100----->6.189676055065233 :)\n",
            "The loss function for the iteration 9110----->6.189380776295889 :)\n",
            "The loss function for the iteration 9120----->6.189086486264705 :)\n",
            "The loss function for the iteration 9130----->6.1887931795297435 :)\n",
            "The loss function for the iteration 9140----->6.188500850692537 :)\n",
            "The loss function for the iteration 9150----->6.188209494397741 :)\n",
            "The loss function for the iteration 9160----->6.187919105332579 :)\n",
            "The loss function for the iteration 9170----->6.187629678226399 :)\n",
            "The loss function for the iteration 9180----->6.18734120785026 :)\n",
            "The loss function for the iteration 9190----->6.187053689016466 :)\n",
            "The loss function for the iteration 9200----->6.1867671165781175 :)\n",
            "The loss function for the iteration 9210----->6.186481485428663 :)\n",
            "The loss function for the iteration 9220----->6.186196790501547 :)\n",
            "The loss function for the iteration 9230----->6.185913026769691 :)\n",
            "The loss function for the iteration 9240----->6.185630189245115 :)\n",
            "The loss function for the iteration 9250----->6.185348272978561 :)\n",
            "The loss function for the iteration 9260----->6.18506727305904 :)\n",
            "The loss function for the iteration 9270----->6.184787184613441 :)\n",
            "The loss function for the iteration 9280----->6.184508002806149 :)\n",
            "The loss function for the iteration 9290----->6.184229722838628 :)\n",
            "The loss function for the iteration 9300----->6.183952339949087 :)\n",
            "The loss function for the iteration 9310----->6.183675849412001 :)\n",
            "The loss function for the iteration 9320----->6.183400246537879 :)\n",
            "The loss function for the iteration 9330----->6.183125526672748 :)\n",
            "The loss function for the iteration 9340----->6.182851685197862 :)\n",
            "The loss function for the iteration 9350----->6.18257871752935 :)\n",
            "The loss function for the iteration 9360----->6.182306619117808 :)\n",
            "The loss function for the iteration 9370----->6.182035385447985 :)\n",
            "The loss function for the iteration 9380----->6.181765012038425 :)\n",
            "The loss function for the iteration 9390----->6.181495494441101 :)\n",
            "The loss function for the iteration 9400----->6.1812268282411456 :)\n",
            "The loss function for the iteration 9410----->6.180959009056393 :)\n",
            "The loss function for the iteration 9420----->6.18069203253715 :)\n",
            "The loss function for the iteration 9430----->6.180425894365872 :)\n",
            "The loss function for the iteration 9440----->6.180160590256769 :)\n",
            "The loss function for the iteration 9450----->6.179896115955506 :)\n",
            "The loss function for the iteration 9460----->6.1796324672389344 :)\n",
            "The loss function for the iteration 9470----->6.17936963991475 :)\n",
            "The loss function for the iteration 9480----->6.1791076298211856 :)\n",
            "The loss function for the iteration 9490----->6.1788464328266794 :)\n",
            "The loss function for the iteration 9500----->6.178586044829653 :)\n",
            "The loss function for the iteration 9510----->6.1783264617581075 :)\n",
            "The loss function for the iteration 9520----->6.178067679569442 :)\n",
            "The loss function for the iteration 9530----->6.177809694250074 :)\n",
            "The loss function for the iteration 9540----->6.177552501815228 :)\n",
            "The loss function for the iteration 9550----->6.177296098308594 :)\n",
            "The loss function for the iteration 9560----->6.177040479802075 :)\n",
            "The loss function for the iteration 9570----->6.176785642395513 :)\n",
            "The loss function for the iteration 9580----->6.176531582216407 :)\n",
            "The loss function for the iteration 9590----->6.176278295419662 :)\n",
            "The loss function for the iteration 9600----->6.176025778187321 :)\n",
            "The loss function for the iteration 9610----->6.175774026728276 :)\n",
            "The loss function for the iteration 9620----->6.175523037278033 :)\n",
            "The loss function for the iteration 9630----->6.17527280609847 :)\n",
            "The loss function for the iteration 9640----->6.175023329477547 :)\n",
            "The loss function for the iteration 9650----->6.174774603729088 :)\n",
            "The loss function for the iteration 9660----->6.174526625192514 :)\n",
            "The loss function for the iteration 9670----->6.174279390232632 :)\n",
            "The loss function for the iteration 9680----->6.174032895239328 :)\n",
            "The loss function for the iteration 9690----->6.17378713662742 :)\n",
            "The loss function for the iteration 9700----->6.1735421108363395 :)\n",
            "The loss function for the iteration 9710----->6.173297814329947 :)\n",
            "The loss function for the iteration 9720----->6.173054243596295 :)\n",
            "The loss function for the iteration 9730----->6.1728113951474235 :)\n",
            "The loss function for the iteration 9740----->6.172569265519037 :)\n",
            "The loss function for the iteration 9750----->6.172327851270425 :)\n",
            "The loss function for the iteration 9760----->6.172087148984147 :)\n",
            "The loss function for the iteration 9770----->6.171847155265853 :)\n",
            "The loss function for the iteration 9780----->6.1716078667440515 :)\n",
            "The loss function for the iteration 9790----->6.171369280069919 :)\n",
            "The loss function for the iteration 9800----->6.171131391917067 :)\n",
            "The loss function for the iteration 9810----->6.17089419898135 :)\n",
            "The loss function for the iteration 9820----->6.170657697980674 :)\n",
            "The loss function for the iteration 9830----->6.1704218856547906 :)\n",
            "The loss function for the iteration 9840----->6.170186758765073 :)\n",
            "The loss function for the iteration 9850----->6.1699523140943136 :)\n",
            "The loss function for the iteration 9860----->6.169718548446589 :)\n",
            "The loss function for the iteration 9870----->6.1694854586470385 :)\n",
            "The loss function for the iteration 9880----->6.169253041541628 :)\n",
            "The loss function for the iteration 9890----->6.169021293997028 :)\n",
            "The loss function for the iteration 9900----->6.168790212900396 :)\n",
            "The loss function for the iteration 9910----->6.1685597951591795 :)\n",
            "The loss function for the iteration 9920----->6.168330037700997 :)\n",
            "The loss function for the iteration 9930----->6.168100937473369 :)\n",
            "The loss function for the iteration 9940----->6.167872491443595 :)\n",
            "The loss function for the iteration 9950----->6.167644696598619 :)\n",
            "The loss function for the iteration 9960----->6.16741754994473 :)\n",
            "The loss function for the iteration 9970----->6.1671910485075285 :)\n",
            "The loss function for the iteration 9980----->6.166965189331677 :)\n",
            "The loss function for the iteration 9990----->6.166739969480721 :)\n",
            "The loss function for the iteration 10000----->6.166515386037023 :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression using scikit-learn (5 points)"
      ],
      "metadata": {
        "id": "etHAUbz3U_PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `sklearn` to train a Ridge Regression Model. To determine the best regularization coefficients, use grid-search (or other techniques you've learned till now)."
      ],
      "metadata": {
        "id": "-CHPali0U_PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge = Ridge()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:37:26.807262Z",
          "iopub.execute_input": "2023-04-30T08:37:26.807683Z",
          "iopub.status.idle": "2023-04-30T08:37:26.832283Z",
          "shell.execute_reply.started": "2023-04-30T08:37:26.807621Z",
          "shell.execute_reply": "2023-04-30T08:37:26.830639Z"
        },
        "trusted": true,
        "id": "IJY3TMVAU_PW"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'alpha': [0.001, 0.01, .1, 1, 10, 100]}"
      ],
      "metadata": {
        "id": "Pxjbs4gF51GM"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(sk_ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "j0QcBnN957Mc",
        "outputId": "9aeb78e3-ed99-443e-fdd0-7f851871f6e1"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=Ridge(),\n",
              "             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring='neg_mean_squared_error')"
            ],
            "text/html": [
              "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Ridge(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Ridge(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best mean squared error:\", -grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2J_N22P6TaH",
        "outputId": "f4271a31-a507-44ef-83e3-1f9cfd916026"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'alpha': 0.1}\n",
            "Best mean squared error: 3.429153818917494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge = Ridge(0.1)"
      ],
      "metadata": {
        "id": "Ezu8IOin-W4Y"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "tVWy6t1y-byb",
        "outputId": "25b7dcee-3a3b-4626-cada-83b644c23ec2"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge(alpha=0.1)"
            ],
            "text/html": [
              "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" checked><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=0.1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation (15 points)"
      ],
      "metadata": {
        "id": "bRGCZENmU_PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each model (the 2 models trained using `sklearn` and the ones based on your code), predict the output for the testing samples."
      ],
      "metadata": {
        "id": "1KGVuWJUU_PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gUrgTm8QAEK",
        "outputId": "7bc4450f-ca73-4209-93f7-e5b049be9bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.77410286, -1.23769784, -0.27267108, ..., -0.08851417,\n",
              "         2.49387004, -2.49387004],\n",
              "       [-0.95670726, -0.02284547, -0.23281594, ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321],\n",
              "       [-0.95670726,  0.40355967,  0.25341668, ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321],\n",
              "       ...,\n",
              "       [ 0.77410286, -0.32052453, -0.26470005, ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321],\n",
              "       [-0.70944867, -0.66647586, -0.27267108, ..., -0.08851417,\n",
              "         2.49387004, -2.49387004],\n",
              "       [-0.21493149, -0.05502699, -0.27267108, ..., -0.08851417,\n",
              "        -0.40098321,  0.40098321]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_lasso_pred = sk_lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "I8udNewt8h4h"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_lasso_pred = my_lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "HxGfnUKP8h72"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_ridge_pred = my_ridge.predict(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:37:34.817713Z",
          "iopub.execute_input": "2023-04-30T08:37:34.818097Z",
          "iopub.status.idle": "2023-04-30T08:37:34.828388Z",
          "shell.execute_reply.started": "2023-04-30T08:37:34.818063Z",
          "shell.execute_reply": "2023-04-30T08:37:34.826441Z"
        },
        "trusted": true,
        "id": "JWkNVRW7U_PW"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge_pred = sk_ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "x2XbdCmF8RHH"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measure the performance of the models based on \"mean squared error\" and the \"coefficient of determination\" of the prediction."
      ],
      "metadata": {
        "id": "jm-OLTTvU_PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "2qk594NIMEcY"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_lasso = mean_squared_error(y_test, sk_lasso_pred)\n",
        "r2_lasso = r2_score(y_test, sk_lasso_pred)\n",
        "\n",
        "\n",
        "print(\"sklearn Lasso Regression:\")\n",
        "print(\"Mean squared error:\", mse_lasso)\n",
        "print(\"Coefficient of determination (R^2):\", r2_lasso)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-30T08:37:38.137503Z",
          "iopub.execute_input": "2023-04-30T08:37:38.137975Z",
          "iopub.status.idle": "2023-04-30T08:37:38.147255Z",
          "shell.execute_reply.started": "2023-04-30T08:37:38.137931Z",
          "shell.execute_reply": "2023-04-30T08:37:38.145677Z"
        },
        "trusted": true,
        "id": "-MqKWn_WU_PX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eedfc69-5811-46c5-89cb-3d36a4b21a7c"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn Lasso Regression:\n",
            "Mean squared error: 3.8409334138484263\n",
            "Coefficient of determination (R^2): 0.95074962914001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#my_lasso_pred = my_lasso_pred.reshape(-1)\n",
        "\n",
        "mse_lasso = mean_squared_error(y_test, my_lasso_pred)\n",
        "r2_lasso = r2_score(y_test, my_lasso_pred)\n",
        "\n",
        "print(\"my Lasso Regression:\")\n",
        "print(\"Mean squared error:\", mse_lasso)\n",
        "print(\"Coefficient of determination (R^2):\", r2_lasso)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGk-9fckLYwK",
        "outputId": "695cd3ee-6f19-414f-eff4-ca64ea01ba0a"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my Lasso Regression:\n",
            "Mean squared error: 6.499092459534797\n",
            "Coefficient of determination (R^2): 0.9166653832812098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_ridge = mean_squared_error(y_test, my_ridge_pred)\n",
        "r2_ridge = r2_score(y_test, my_ridge_pred)\n",
        "\n",
        "print(\"my Ridge Regression:\")\n",
        "print(\"Mean squared error:\", mse_ridge)\n",
        "print(\"Coefficient of determination (R^2):\", r2_ridge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsliypwRLY0-",
        "outputId": "0c510b6c-9332-47be-d2ec-defbebcc2ca4"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my Ridge Regression:\n",
            "Mean squared error: 4.262067536908737\n",
            "Coefficient of determination (R^2): 0.9453496366101382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_ridge = mean_squared_error(y_test, sk_ridge_pred)\n",
        "r2_ridge = r2_score(y_test, sk_ridge_pred)\n",
        "\n",
        "print(\"sklearn Ridge Regression:\")\n",
        "print(\"Mean squared error:\", mse_ridge)\n",
        "print(\"Coefficient of determination (R^2):\", r2_ridge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLEY_YcOLZJj",
        "outputId": "060865fb-a6ab-4c1b-f1d1-ec31b2e4554f"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn Ridge Regression:\n",
            "Mean squared error: 3.816836542199142\n",
            "Coefficient of determination (R^2): 0.9510586112902899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle"
      ],
      "metadata": {
        "id": "RKvQNxCBd4bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ktrain = pd.read_csv(\"/content/assignment5-training-data.csv\")"
      ],
      "metadata": {
        "id": "vbuWH0rBRHse"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ktest = pd.read_csv(\"/content/assignment5-test-data.csv\")"
      ],
      "metadata": {
        "id": "BsbKbjYUGiWF"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ktrain = df_ktrain.dropna()\n",
        "#df_ktest = df_ktest.dropna()"
      ],
      "metadata": {
        "id": "bCbDxXpiQ9ZD"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ktest = df_ktest.drop('ID', axis=1)"
      ],
      "metadata": {
        "id": "NLyfOEfrH2-J"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename specific columns\n",
        "#df_ktest.rename(columns={'ID': 'Life expectancy '}, inplace=True)\n"
      ],
      "metadata": {
        "id": "eeih4jOka75z"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ktrain_con = df_ktrain.drop(columns=[\"Life expectancy \"])\n",
        "\n",
        "life_exp = df_ktrain[\"Life expectancy \"]"
      ],
      "metadata": {
        "id": "H1XQ8Zu2iVd0"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_ktrain_con))\n",
        "print(len(df_ktest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdDO6oYfICzm",
        "outputId": "5cbc1f66-3490-421c-823e-4910d4ff5f4a"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1415\n",
            "441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_concat = pd.concat([df_ktrain_con, df_ktest], ignore_index=False, axis=1)"
      ],
      "metadata": {
        "id": "jCeFu0dFQYKy"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_concat)"
      ],
      "metadata": {
        "id": "gPY9echJRgQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689b14fc-ab8d-40e1-e572-a73d0db38c2f"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1603"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = df_concat.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "df_concat = pd.get_dummies(df_concat, columns=cat_cols)"
      ],
      "metadata": {
        "id": "MLCtLYvCQ-3w"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ktest = df_concat.tail(441)\n",
        "\n",
        "# Get the rest of the dataframe\n",
        "df_ktrain = df_concat.drop(df_ktest.index)"
      ],
      "metadata": {
        "id": "J1GDmPTeY05k"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the dataframes\n",
        "df1 = df.iloc[:500] # First 500 rows\n",
        "df2 = df.iloc[500:] # Last 500 rows\n"
      ],
      "metadata": {
        "id": "K9bJgVUzaOf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_ktrain))\n",
        "print(len(df_ktest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlDxPFItZRBY",
        "outputId": "f55ea236-34d1-4ab5-e1ef-356bacfd00a2"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1162\n",
            "441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df_ktrain.drop(columns=[\"Life expectancy \"]).to_numpy()\n",
        "\n",
        "y = df_ktrain[\"Life expectancy \"].to_numpy()"
      ],
      "metadata": {
        "id": "zCMSKSLBRAJL"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_t = df_concat.to_numpy()"
      ],
      "metadata": {
        "id": "psDlzyAES8Fi"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)"
      ],
      "metadata": {
        "id": "gSvnlJiLRB8R"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whWlZeiMUnmI",
        "outputId": "99902397-042a-48ba-dde1-69a1c2fe6740"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1162"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "nD5qmKjDRrhs"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_lasso = Lasso()"
      ],
      "metadata": {
        "id": "2f_jWSwaRtp4"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'alpha': [0.001, 0.01, .1, 1, 10, 100]}"
      ],
      "metadata": {
        "id": "3j2D6fWDRvvR"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(sk_lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "M6Mmnj5uRyFS",
        "outputId": "d0354182-4ea3-42cb-937a-b125dece1fe9"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+02, tolerance: 6.610e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.602e+01, tolerance: 6.919e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.405e+02, tolerance: 6.751e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.606e+02, tolerance: 6.403e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.991e+01, tolerance: 6.652e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.497e+02, tolerance: 8.336e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=Lasso(),\n",
              "             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring='neg_mean_squared_error')"
            ],
            "text/html": [
              "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Lasso(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Lasso(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best mean squared error:\", -grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcYRxjglR04u",
        "outputId": "3851413a-407d-46b8-fa55-b60569f1610e"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'alpha': 0.001}\n",
            "Best mean squared error: 3.459417088839287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_lasso = Lasso(alpha = 0.001)"
      ],
      "metadata": {
        "id": "BXmw4BidR3SD"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "QQAgjVDPSKtM",
        "outputId": "e7bf82da-e05e-4f2d-e2a0-eaa75e7cdf8b"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.497e+02, tolerance: 8.336e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso(alpha=0.001)"
            ],
            "text/html": [
              "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(alpha=0.001)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge = Ridge()"
      ],
      "metadata": {
        "id": "GI2oSN_2R6Zi"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'alpha': [0.001, 0.01, .1, 1, 10, 100]}"
      ],
      "metadata": {
        "id": "FQypnttDR_Jf"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(sk_ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "HdUjL0hsSAvD",
        "outputId": "909db654-78ad-43af-c2c3-ce09f8c690ab"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=Ridge(),\n",
              "             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring='neg_mean_squared_error')"
            ],
            "text/html": [
              "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Ridge(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Ridge(),\n",
              "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best mean squared error:\", -grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bndBXiwrSC4V",
        "outputId": "303a5047-80a1-44de-ccdc-fa545841a4bd"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'alpha': 0.1}\n",
            "Best mean squared error: 3.429153818917494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge = Ridge(0.1)"
      ],
      "metadata": {
        "id": "OJVYBn5rSEUm"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "JrV8fw8mSH7W",
        "outputId": "7888cc9e-0cd2-484e-e11a-13a1cd4f306b"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge(alpha=0.1)"
            ],
            "text/html": [
              "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=0.1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "wJdEdDJH88lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "df = pd.get_dummies(df, columns=cat_cols)"
      ],
      "metadata": {
        "id": "W7FT6QkT8-HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYjMMY-kcS1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_t = df_ktest.drop(columns=[\"Life expectancy \"]).to_numpy()"
      ],
      "metadata": {
        "id": "b25LrrXT9GOv"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_t = scaler.fit_transform(x_t)"
      ],
      "metadata": {
        "id": "xrntJLuL9He8"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_t = x_t.fillna(x_t.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "McQDnmgIcx39",
        "outputId": "49e9f267-6b59-4ff6-8b9a-25814c933b35"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-203-e1b34dc24661>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fillna'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_t = np.nan_to_num(x_t)"
      ],
      "metadata": {
        "id": "g_vnGRjRdARv"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_pred = sk_lasso.predict(x_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "GQLtHfmoTLsO",
        "outputId": "42ae0eb2-2e86-4458-acd6-276ba01a0abd"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-205-98da4466146f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlasso_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 648 features, but Lasso is expecting 154 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_ridge_pred = sk_ridge.predict(x_t)"
      ],
      "metadata": {
        "id": "ayaOF7P0TMeE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "6b6a9fd4-4ecf-47c8-fe89-97d48e97293b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-e49071346cd7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msk_ridge_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk_ridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 135 features, but Ridge is expecting 154 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Competition Link: https://www.kaggle.com/t/adbf95666e7c4f41a6be1129a9e4415c"
      ],
      "metadata": {
        "id": "SWVaPC-Yd6yb"
      }
    }
  ]
}